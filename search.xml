<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Lua中的值及其类型]]></title>
    <url>%2F2019%2F06%2F03%2Fvalue-and-its-type-in-lua%2F</url>
    <content type="text"><![CDATA[基本数据类型及其子类型 Lua是一门动态类型的语言，这意味着Lua中的变量没有类型，而值才有类型。一个变量可以在不同时刻指向不同类型的值。 在lua-5.3.5版本中，有9中基本的数据类型，其定义如下： 1234567891011#define LUA_TNIL 0 // 空类型#define LUA_TBOOLEAN 1 // 布尔类型#define LUA_TLIGHTUSERDATA 2 // 指针类型(void *)#define LUA_TNUMBER 3 // 数字类型#define LUA_TSTRING 4 // 字符串类型#define LUA_TTABLE 5 // 表类型#define LUA_TFUNCTION 6 // 函数类型#define LUA_TUSERDATA 7 // 指针类型(void *)#define LUA_TTHREAD 8 // Lua虚拟机、协程#define LUA_NUMTAGS 9 其中需要说明的是LUA_TLIGHTUSERDATA和LUA_TUSERDATA的区别是：前者的所有对象共享一个元表，且其内部所指向的内存的申请及释放不需要由Lua来完成；后者的每个对象都有自己的元表，需要进行垃圾回收，并且其内部所指向的内存的申请和释放需要由Lua来完成。除此之外，上述的一些基本类型有子类型（变种类型），一些基本类型需要GC，例如LUA_TNUMBER类型可以进一步分为整数类型和实数类型，现将这些子类型及GC标记的定义一并罗列如下： 123456789101112131415#define LUA_TLCL (LUA_TFUNCTION | (0 &lt;&lt; 4)) /* Lua closure */#define LUA_TLCF (LUA_TFUNCTION | (1 &lt;&lt; 4)) /* light C function */#define LUA_TCCL (LUA_TFUNCTION | (2 &lt;&lt; 4)) /* C closure */#define LUA_TSHRSTR (LUA_TSTRING | (0 &lt;&lt; 4)) /* short strings */#define LUA_TLNGSTR (LUA_TSTRING | (1 &lt;&lt; 4)) /* long strings */#define LUA_TNUMFLT (LUA_TNUMBER | (0 &lt;&lt; 4)) /* float numbers */#define LUA_TNUMINT (LUA_TNUMBER | (1 &lt;&lt; 4)) /* integer numbers */* Bit mark for collectable types */#define BIT_ISCOLLECTABLE (1 &lt;&lt; 6)/* mark a tag as collectable */#define ctb(t) ((t) | BIT_ISCOLLECTABLE) Lua中是如何实现对基本类型、子类型、是否需要GC等的表示的呢？其实从子类型的定义我们也可以大概了解到，具体如下： 基本类型有9种，因此可以用低四位，即bits 0-3来表示； 每种基本类型的子类型不超过4种，因此可以用中间两位，即bits 4-5来表示； 用于标记某种类型是否需要进行GC，只需要一个位，因此可以用bit 6来表示； 值表示的统一数据结构 Lua中所有的值都是第一类值，即所有的值都可以存储在变量中，也可以作为参数传递给函数，也可以作为函数的返回值。为了更好地管理Lua中的值，Lua使用了一个通用的数据结构TValue来存储Lua中可能出现的任何值及其类型，其定义如下： 1234567891011121314typedef union Value &#123; GCObject *gc; /* collectable objects */ void *p; /* light userdata */ int b; /* booleans */ lua_CFunction f; /* light C functions */ lua_Integer i; /* integer numbers */ lua_Number n; /* float numbers */&#125; Value;#define TValuefields Value value_; int tt_typedef struct lua_TValue &#123; TValuefields;&#125; TValue; 联合体Value将Lua中所有可能的值都囊括了进来，比如通过包含GCObject *类型的成员gc，可以将所有需要GC的值都包含进来。结构体TValue包含了两个成员，一个是联合体类型Value的对象value_，用于表示Lua中所有可能出现的值，另一个是int类型的tt_，用来表示Value具体表示哪种类型的值。这样通过TValue就是可以表示Lua中所有的值及其类型了。 那如何将TValue与某种具体类型的值之间做转换呢？其主要的逻辑是将TValue中的value_及tt_与具体的数据及其类型对应起来做转换。以TValue和LUA_TNUMFLT之间的转换为例： 123456789101112/* Macro to test type */#define ttisfloat(o) checktag((o), LUA_TNUMFLT)/* Macro to access values */#define fltvalue(o) check_exp(ttisfloat(o), val_(o).n)#define val_(o) ((o)-&gt;value_)#define settt_(o,t) ((o)-&gt;tt_=(t))/* Macro to set value */#define setfltvalue(obj,x) \ &#123; TValue *io=(obj); val_(io).n=(x); settt_(io, LUA_TNUMFLT); &#125; 我们可以通过宏fltvalue()从TValue对象中取出其中存放的实数，通过宏setfltvalue()将实数存放到TValue对象中，并将其内部类型设置为LUA_TNUMFLT。 垃圾回收类型及其“继承” 上面说到，Value将Lua中所有可能的值都包含了进来，从它的定义中我们不难看出，它确实将布尔类型、整数类型、实数类型、轻量级函数类型等类型的值包含了进来，只要给Value类型的对象赋其中某一个类型的值就行，但是为什么说通过包含GCObject *类型的gc成员就将所有需要GC的类型的值也包含进来了呢？我们先来看下GCObject的定义： 12345678910111213141516171819202122/* Common type for all collectable objects */typedef struct GCObject GCObject;/* Common Header for all collectable objects (in macro form, to be included in other objects) */#define CommonHeader GCObject *next; lu_byte tt; lu_byte marked/* Common type has only the common header */struct GCObject &#123; CommonHeader;&#125;;typedef struct TString &#123; CommonHeader; lu_byte extra; /* reserved words for short strings; &quot;has hash&quot; for longs */ lu_byte shrlen; /* length for short strings */ unsigned int hash; union &#123; size_t lnglen; /* length for long strings */ struct TString *hnext; /* linked list for hash table */ &#125; u;&#125; TString; 为了更好地说明上面提出来的问题，我们通过一个需要进行GC的类型TString（即lua中的字符串类型）来进行辅助说明。从上面GCObject类型和TString类型的定义中可以看出，两者均在开头包含了一个宏CommonHeader。这就使得这两个类型所对应的对象的内存布局的头部是相同的，两者的内存布局如图1所示，而TString类型的对象中多出来的内容则是其私有数据，因而所有需要GCObject类型的对象地方就都可以将TString类型的对象传进去，当然需要做一个强制类型转换(毕竟是伪继承)，将TString类型强转伪GCObject类型，在实际使用的时候，我们再将其强转回TString类型即可。所有其他需要进行GC的类型都和TString一样，会在定义的开头加上CommonHeader，从而实现类似的功能。从另外一方面看，我们可以将GCObject看成TString、Table等的父类，这也很好地体现了在C中如何实现继承等面向对象编程的方法。 从上面的分析我们可以知道，GCObject类型可以说是所有需要进行垃圾回收的类型的代表。在一些接口方面，都是以GCObject类型进行呈现的。例如在Lua中，进程要创建一个需要进行垃圾回收类型的对象时，都是申请一个GCObject对象，同时将具体的类型和所需要的内存大小通过参数传递进去。申请GCObject对象成功后，在函数外层再根据上下文环境将GCObject对象转换为具体类型的对象，再进行类型私有数据的初始化等操作。还是以TString为例，假设要创建一个TString类型的对象，lua源码中有如下例程： 1234567891011121314151617181920212223/* 创建一个新的字符串对象，未填入具体的字符串内容，只是申请了内存空间 */static TString *createstrobj (lua_State *L, size_t l, int tag, unsigned int h) &#123; TString *ts; GCObject *o; size_t totalsize; /* total size of TString object */ /* 计算字符串对应需要的内存大小，包括头部和内容，内容紧跟在头部之后 */ totalsize = sizelstring(l); /* 根据存放字符串所需的内存大小和类型标记创建一个新的GCObject对象 */ o = luaC_newobj(L, tag, totalsize); /* 将GCObject类型转换为具体的TString类型 */ ts = gco2ts(o); /* 保存字符串对应的hash值 */ ts-&gt;hash = h; ts-&gt;extra = 0; /* 字符串以&apos;\0&apos;结尾 */ getstr(ts)[l] = &apos;\0&apos;; /* ending 0 */ return ts;&#125; 注意到上面的例程中有一条将GCObject类型对象强转为TString类型对象的语句。为了方便做类似的转换，Lua专门定义了一个联合体及一系列的宏来辅助进行这样的操作： 12345678910111213union GCUnion &#123; GCObject gc; /* common header */ struct TString ts; struct Udata u; union Closure cl; struct Table h; struct Proto p; struct lua_State th; /* thread */&#125;;#define cast_u(o) cast(union GCUnion *, (o))#define gco2ts(o) check_exp(novariant((o)-&gt;tt) == LUA_TSTRING, &amp;((cast_u(o))-&gt;ts)) 为了节省篇幅，我们还是以GCObject类型强转TString为例。在createstrobj()这个创建TString类型的对象的例程中，我们先计算了我们要创建的TString对象的大小，包括TString结构体本身（字符串管理结构）以及实际内容的总大小，然后以字符串类型（可以是长字符串，也可以是短字符串）及对象大小来调用luaC_newobj()结构创建一个GCObject类型的对象(在当前上下文中，这是一个伪装成GCObject的TString)，然后调用gco2ts()这个宏将其转换为TString类型的对象。这个宏的关键点是“&amp;((cast_u(o))-&gt;ts)”，由于GCUnion是一个联合体，而该联合体内囊括的所有类型又都是需要GC的类型，那么可以说联合体GCUnion的一个对象和所有需要进行GC的类型的对象的内存布局的开头是一样的，正如GCObject和TString的内存布局开头那样，那么关键点中的“cast_u(o)”是可行的，而且不会引起错误，然后我们再以TString的方式来访问这个联合体，即关键点的剩余部分“&amp;((cast_u(o))-&gt;ts)”，这样的效果就是将GCObject对象转换为了TString对象。其他需要GC的类型也可以用类似方式进行转换。 本文关于Lua中值及其类型的总结就到这里了。参考：1、 Lua 5.3 Reference Manual2、 《Lua设计与实现》]]></content>
      <categories>
        <category>Lua</category>
      </categories>
      <tags>
        <tag>Lua</tag>
        <tag>编程语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[epoll的原理和实现]]></title>
    <url>%2F2017%2F10%2F05%2Fimplementation-of-epoll%2F</url>
    <content type="text"><![CDATA[epoll是Linux内核为处理大批量文件描述符而设计的IO多路复用机制，它能显著提高程序在存在大量并发连接而只有少部分活跃连接情况下的系统CPU利用率。epoll之所以可以做到如此高的效率是因为它在获取就绪事件的时候，并不会遍历所有被监听的文件描述符集，而只会遍历那些被设备IO事件异步唤醒而加入就绪链表的文件描述符集。 epoll机制提供了三个系统调用给用户态应用程序，用以管理感兴趣的事件，三个系统调用分别为epoll_create()、epoll_ctl()和epoll_wait()。另外还暴露了一个struct和一个union，分别为struct epoll_event和union epoll_data。而在epoll自身内部，还涉及了多个重要的数据结构，分别为struct eventpoll、struct epitem、struct eppoll_entry、struct ep_pqueue、struct poll_table_struct等。核心数据结构之间的关系如下图所示： 下面将从epoll机制提供给用户态的接口作为切入点，围绕核心数据结构，详细描述epoll的原理及其实现。 1、epoll_create() epoll_create系统调用，其原型如下： 1int epoll_create(int size) epoll_create()系统调用会创建一个类型为struct eventpoll的对象，并返回一个与之对应文件描述符，之后应用程序在用户态使用epoll的时候都将依靠这个文件描述符，而在epoll内部则是通过该文件描述符进一步获取到struct eventpoll类型的对象，再进行对应的操作，这在其实现中可以很清晰看出。这个接口在内核中对应的实现为epoll_create1()，该函数位于eventpoll.c文件中，其实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546SYSCALL_DEFINE1(epoll_create1, int, flags)&#123; int error, fd; struct eventpoll *ep = NULL; struct file *file; …… /* 创建struct eventpoll类型的对象 */ error = ep_alloc(&amp;ep); if (error &lt; 0) return error; /* 获取一个空闲的文件描述符 */ fd = get_unused_fd_flags(O_RDWR | (flags &amp; O_CLOEXEC)); if (fd &lt; 0) &#123; error = fd; goto out_free_ep; &#125; /* * 创建一个名字为"[eventpoll]"的文件，并返回对应的struct file类型的对象, * 从anon_inode_getfile()函数的实现可以看出，struct eventpoll类型的对象 * 被挂载到了struct file类型对象的private_data成员上。后续就可以通过 * struct file类型的对象获取struct eventpoll类型的对象。 */ file = anon_inode_getfile("[eventpoll]", &amp;eventpoll_fops, ep, O_RDWR | (flags &amp; O_CLOEXEC)); if (IS_ERR(file)) &#123; error = PTR_ERR(file); goto out_free_fd; &#125; ep-&gt;file = file; /* * 将文件描述符和文件对象关联起来，后续就可以通过fd获取struct eventpoll类型 * 的对象了，换句话说fd和struct eventpoll类型的对象关联起来了。 */ fd_install(fd, file); return fd;out_free_fd: put_unused_fd(fd);out_free_ep: ep_free(ep); return error;&#125; 从实现中可以看到epoll_create()系统调用完成了fd、file、eventpoll三个对象之间的关联，并将fd返回给用户态应用程序，从而减小了用户态的使用难度。每一个epoll fd都会对应一个struct eventpoll类型的对象，该对象用来管理用户态应用程序添加的感兴趣事件以及就绪了的事件，对于struct eventpoll类型，其定义为： 123456789101112131415161718192021222324252627282930313233struct eventpoll &#123; /* Protect the access to this structure */ spinlock_t lock; struct mutex mtx; /* 用于收集调用了epoll_wait()系统调用的用户态应用程序 */ wait_queue_head_t wq; /* Wait queue used by file-&gt;poll() */ wait_queue_head_t poll_wait; /* 用于收集已经就绪了的item对象 */ struct list_head rdllist; /* 用来挂载struct epitem类型对象的红黑树的根 */ struct rb_root rbr; /* * 这个单向链表也是用来收集就绪了item对象的，那这个成员什么时候会被使用呢? * 这个成员是在对rellist成员进行扫描操作获取就绪事件返还给用户态时被用来存放 * 扫描期间就绪的事件的。为什么需要这样做呢?因为在对rellist扫描期间需要保证 * 数据的一致性，如果此时又有新的就绪事件发生，那么就需要提供临时的空间来存 * 储，所以ovflist就扮演了这个角色。 */ struct epitem *ovflist; struct wakeup_source *ws; struct user_struct *user; /* struct eventpoll类型对象对应的文件对象 */ struct file *file; int visited; struct list_head visited_list_link;&#125; struct eventpoll类型的成员很多，到目前为止，我们只需要关注两个成员，一个是类型为struct rb_root的rbr，一个是类型为struct list_head的rdllist。其中rbr成员是一棵红黑树的根节点，这棵树中存放着所有通过epoll_ctl()系统调用添加到epoll中的事件对应的类型为struct epitem的对象；而rdllist链表则存放了将要通过epoll_wait()系统调用返回给用户态应用程序的就绪事件对应的struct epitem对象。 2、epoll_ctl() epoll_ctl()系统调用，其原型如下： 1int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); epoll_ctl()系统调用提供给用户态应用程序向epoll中添加、删除和修改感兴趣的事件，其中epfd就是通过epoll_create()系统调用获取的epoll对象文件描述符，op用于指明epoll如何操作event事件，其取值包括EPOLL_CTL_ADD、EPOLL_CTL_MOD和EPOLL_CTL_DEL，分别用于指明添加新的事件到epoll中、修改epoll中的事件以及删除epoll中的事件；fd就是被监控事件对应的文件描述符，而event则是被监控的事件对象，其类型为struct epoll_event，定义如下： 1234struct epoll_event &#123; __u32 events; __u64 data;&#125; 其中events成员指明了用户态应用程序感兴趣的事件类型，比如EPOLLIN和EPOLLOUT等。而data成员则是提供给用户态应用程序使用，一般用于存储事件的上下文，比如在nginx中，该成员用于存放指向ngx_connection_t类型对象的指针。 epoll_ctl()系统调用在内核对应的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd, struct epoll_event __user *, event)&#123; int error; int full_check = 0; struct fd f, tf; struct eventpoll *ep; struct epitem *epi; struct epoll_event epds; struct eventpoll *tep = NULL; error = -EFAULT; /* * 判断epoll是否需要从用户态拷贝事件，如果需要则拷贝，不需要直接往下走，如果 * 用户态传进来的op是删除事件操作，那么就不需要从用户态拷贝事件。 */ if (ep_op_has_event(op) &amp;&amp; copy_from_user(&amp;epds, event, sizeof(struct epoll_event))) goto error_return; error = -EBADF; f = fdget(epfd); if (!f.file) goto error_return; /* 获取被监控的文件描述符对应的struct fd类型的对象 */ tf = fdget(fd); if (!tf.file) goto error_fput; error = -EPERM; /* 被监控的文件描述符需要支持poll回调 */ if (!tf.file-&gt;f_op-&gt;poll) goto error_tgt_fput; /* Check if EPOLLWAKEUP is allowed */ if (ep_op_has_event(op)) ep_take_care_of_epollwakeup(&amp;epds); error = -EINVAL; /* epoll对象对应的fd不能被自己监控 */ if (f.file == tf.file || !is_file_epoll(f.file)) goto error_tgt_fput; …… /* * 从文件对象的私有数据中获取struct eventpoll类型的对象，这个在epoll_create1() * 函数中可以知道其挂载到了文件对象的私有数据成员中 */ ep = f.file-&gt;private_data; …… /* 尝试用fd和file对象从epoll的红黑树中去查找对应的struct epitem类型的对象 */ epi = ep_find(ep, tf.file, fd); error = -EINVAL; switch (op) &#123; case EPOLL_CTL_ADD: /* * 如果操作类型是add，且原先这个fd不在epoll中，那么就将其加入到epoll * 中，如果fd已经在epoll中，则错误码设置为-EEXIST */ if (!epi) &#123; epds.events |= POLLERR | POLLHUP; error = ep_insert(ep, &amp;epds, tf.file, fd, full_check); &#125; else error = -EEXIST; if (full_check) clear_tfile_check_list(); break; case EPOLL_CTL_DEL: /* * 如果操作类型是del，并且fd对应的struct epitem类型的对象在epoll中， * 那么就从epoll中移除 */ if (epi) error = ep_remove(ep, epi); else error = -ENOENT; break; case EPOLL_CTL_MOD: /* * 如果操作类型是mod，并且fd对应的struct epitem类型的对象在epoll中， * 那么就修改fd对应的struct epitem类型的对象感兴趣的事件类型。 */ if (epi) &#123; if (!(epi-&gt;event.events &amp; EPOLLEXCLUSIVE)) &#123; epds.events |= POLLERR | POLLHUP; error = ep_modify(ep, epi, &amp;epds); &#125; &#125; else error = -ENOENT; break; &#125; ……error_tgt_fput: if (full_check) mutex_unlock(&amp;epmutex); fdput(tf);error_fput: fdput(f);error_return:return error;&#125; 从epoll_ctl()的实现我们可以看到，在该函数中首先是通过epfd获取到对应的类型为struct eventpoll的epoll对象，接着判断epoll_ctl()参数fd对应的事件已经在epoll的监控中了，即用用户态传递进来的事件fd及其对应的file对象调用ep_find()到epoll对象的rbr成员中去寻找是否有对应的类型为struct epitem的对象，有则返回，否则返回NULL。然后再根据参数fd指定的操作类型对事件做进一步处理或进行异常处理。我们以事件之前未加入到epoll中，及操作类型EPOLL_CTL_ADD情况做进一步解析，在这种情况下会调用ep_insert()做进一步的处理，ep_insert()函数的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126static int ep_insert(struct eventpoll *ep, struct epoll_event *event, struct file *tfile, int fd, int full_check)&#123; int error, revents, pwake = 0; unsigned long flags; long user_watches; struct epitem *epi; struct ep_pqueue epq; /* 获取当前用户已经加入到epoll中监控的文件描述符数量，如果超过了上限，那么本次不加入 */ user_watches = atomic_long_read(&amp;ep-&gt;user-&gt;epoll_watches); if (unlikely(user_watches &gt;= max_user_watches)) return -ENOSPC; /* 从slab cache中申请一个struct epitem类型的对象 */ if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL))) return -ENOMEM; /* Item initialization follow here ... */ INIT_LIST_HEAD(&amp;epi-&gt;rdllink); INIT_LIST_HEAD(&amp;epi-&gt;fllink); INIT_LIST_HEAD(&amp;epi-&gt;pwqlist); /* 将epoll对象挂载到item的ep成员中 */ epi-&gt;ep = ep; /* 设置被监控的文件描述符及其对应的文件对象到item的ffd成员中 */ ep_set_ffd(&amp;epi-&gt;ffd, tfile, fd); /* 保存fd感兴趣的事件对象 */ epi-&gt;event = *event; epi-&gt;nwait = 0; epi-&gt;next = EP_UNACTIVE_PTR; if (epi-&gt;event.events &amp; EPOLLWAKEUP) &#123; error = ep_create_wakeup_source(epi); if (error) goto error_create_wakeup_source; &#125; else &#123; RCU_INIT_POINTER(epi-&gt;ws, NULL); &#125; /* Initialize the poll table using the queue callback */ epq.epi = epi; /* 初始化struct poll_table类型的对象 */ init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc); /* 返回值表示该文件描述符感兴趣的事件。如果感兴趣的事件没有发生，则为0 */ revents = ep_item_poll(epi, &amp;epq.pt); error = -ENOMEM; if (epi-&gt;nwait &lt; 0) goto error_unregister; /* Add the current item to the list of active epoll hook for this file */ spin_lock(&amp;tfile-&gt;f_lock); /* 将item对象挂载到对应的文件对象的f_ep_links成员中 */ list_add_tail_rcu(&amp;epi-&gt;fllink, &amp;tfile-&gt;f_ep_links); spin_unlock(&amp;tfile-&gt;f_lock); /* 将item对象插入到epoll对象的红黑树中 */ ep_rbtree_insert(ep, epi); /* now check if we've created too many backpaths */ error = -EINVAL; if (full_check &amp;&amp; reverse_path_check()) goto error_remove_epi; /* We have to drop the new item inside our item list to keep track of it */ spin_lock_irqsave(&amp;ep-&gt;lock, flags); /* If the file is already "ready" we drop it inside the ready list */ /* * revents &amp; event-&gt;events不为0，说明当前fd有感兴趣的事件发生。如果fd对应的item * 对象又不在ready list中，那么就将item对象加入到epoll对象的ready list链表中， * 表示事件就绪。 */ if ((revents &amp; event-&gt;events) &amp;&amp; !ep_is_linked(&amp;epi-&gt;rdllink)) &#123; list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); ep_pm_stay_awake(epi); /* * 如果有应用程序在等待事件发生，那么就唤醒上层等待事件发生的应用程序， * 那ep-&gt;wq这个成员是在什么时候被添加了等待epoll事件发生的应用程序信息 * 的呢?这个就是通过epoll_wait()系统调用来感知的。当有应用程序调用了 * epoll_wait()的时候，在ep_poll()函数中就会把当前应用程序current对象 * 加入到ep-&gt;wq成员中。 */ if (waitqueue_active(&amp;ep-&gt;wq)) wake_up_locked(&amp;ep-&gt;wq); if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; &#125; spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); /* 增加当前用户加入epoll中的文件描述符个数 */ atomic_long_inc(&amp;ep-&gt;user-&gt;epoll_watches); /* We have to call this outside the lock */ if (pwake) ep_poll_safewake(&amp;ep-&gt;poll_wait); return 0;error_remove_epi: spin_lock(&amp;tfile-&gt;f_lock); list_del_rcu(&amp;epi-&gt;fllink); spin_unlock(&amp;tfile-&gt;f_lock); rb_erase(&amp;epi-&gt;rbn, &amp;ep-&gt;rbr);error_unregister: ep_unregister_pollwait(ep, epi); spin_lock_irqsave(&amp;ep-&gt;lock, flags); if (ep_is_linked(&amp;epi-&gt;rdllink)) list_del_init(&amp;epi-&gt;rdllink); spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); wakeup_source_unregister(ep_wakeup_source(epi));error_create_wakeup_source: kmem_cache_free(epi_cache, epi); return error;&#125; 从ep_insert()函数的实现我们可以看到，ep_insert()函数主要做了如下三件事： 1）、创建并初始化一个strut epitem类型的对象，完成该对象和被监控事件（包括fd、struct epoll_event类型的对象）以及epoll对象eventpoll的关联。 2）、将struct epitem类型的对象加入到epoll对象eventpoll的红黑树中管理起来。 3）、将struct epitem类型的对象加入到被监控事件对应的目标文件的等待列表中，并注册事件就绪时会调用的回调函数，在epoll中该回调函数就是ep_poll_callback()。 第一和第二点比较清晰，下面将着重分析第三点的流程。 在ep_insert()函数中，epoll会定义一个类型为struct ep_pqueue的对象，该对象包括了epitem成员，以及一个类型为poll_table的对象成员pt。在ep_insert()函数中我们会将 pt的_qproc这个回调函数成员设置为ep_ptable_queue_proc()，并在ep_item_poll()函数中将pt的_key成员设置为用户态应用程序感兴趣的事件类型，然后调用被监控的目标文件的poll回调函数。被监控的目标文件的poll回调函数一般会调用poll_wait()函数，而poll_wait()又会调用pt的_qproc()回调函数，而在ep_insert()函数中设置的pt的_qproc()回调函数ep_ptable_queue_proc()会将epitem对象对应的eppoll_entry对象加入到被监控的目标文件的等待队列中，并设置感兴趣事件发生后的回调函数为ep_poll_callback()。目标文件的poll回调函数调用完poll_wait()之后会获取对应的就绪事件掩码。如果pt的回调函数成员_qproc没有设置，那么目标文件的poll回调函数一般就只会返回对应的就绪事件掩码。所以如果目标文件对应的事件就绪的话，ep_item_poll()函数就会返回。ep_item_poll()在epoll_ctl()和ep_wait()的处理流程中都会调用，两个流程中的区别在于epoll_ctl()处理流程中调用ep_item_poll()函数的是时候会设置poll_table的_qproc成员；而在epoll_wait()处理流程中则不会设置该成员，而只会获取就绪事件的掩码。 以socket为例，因为socket有多种类型，如tcp、udp等，所以socket层会实现一个通用的poll回调函数，这个函数就是sock_poll()。在sock_poll()函数中通常会调用某一具体类型协议对应的poll回调函数，以tcp为例，那么这个poll()回调函数就是tcp_poll()。当socket有事件就绪时，比如读事件就绪，就会调用sock-&gt;sk_data_ready这个回调函数，即sock_def_readable()，在这个回调函数中则会遍历socket 文件中的等待队列，然后依次调用队列节点的回调函数，在epoll中对应的回调函数就是ep_poll_callback()，这个函数会将就绪事件对应的epitem对象加入到epoll对象eventpoll的就绪链表rdllist中，这样用户态程序调用epoll_wait()的时候就能获取到该事件，如果调用ep_poll_callback()函数的时候发现epoll对象eventpoll的ovflist成员不等于EP_UNACTIVE_PTR的话，说明此时正在扫描rdllist链表，这个时候会将就绪事件对应的epitem对象加入到ovflist链表暂存起来，等rdllist链表扫描完之后在将ovflist链表中的内容移动到rdllist链表中，此部分实现可以参考ep_scan_ready_list()函数。 3、epoll_wait() epoll_wait()系统调用，其原型如下： 1int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); epoll_wait()系统调用主要是用于收集在epoll中监控的就绪事件。epoll_wait()函数返回值表示的是获取到的就绪事件个数，epfd表示的epoll对象fd，第二个参数则是已经分配好内存的epoll_event结构体数组，用于给内核存放就绪事件的；第三个参数表示本次最多可以返回的就绪事件个数，这个通常和events数组的大小一样；第四个参数表示在没有检测到事件发生时epoll_wait()的阻塞时长。epoll_wait()系统调用在内核对应的实现如下： 1234567891011121314151617181920212223242526272829303132333435363738SYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events, int, maxevents, int, timeout)&#123; int error; struct fd f; struct eventpoll *ep; /* The maximum number of event must be greater than zero */ if (maxevents &lt;= 0 || maxevents &gt; EP_MAX_EVENTS) return -EINVAL; /* Verify that the area passed by the user is writeable */ if (!access_ok(VERIFY_WRITE, events, maxevents * sizeof(struct epoll_event))) return -EFAULT; /* 通过epfd获取到对应的struct file类型的文件对象 */ f = fdget(epfd); if (!f.file) return -EBADF; error = -EINVAL; /* 判断用户态传进来的fd是不是epfd，即通过判断文件对象的操作回调是不是eventpoll_fops */ if (!is_file_epoll(f.file)) goto error_fput; /* * 在epoll_create1()函数中我们知道epoll对象会存放在其对应的文件对象的私有 * 数据成员中，所以这里可以通过这个成员获取到epoll对象。 */ ep = f.file-&gt;private_data; /* 获取就绪的事件，返回给用户态应用程序 */ error = ep_poll(ep, events, maxevents, timeout);error_fput: fdput(f); return error;&#125; 从epoll_wait()对应的内核实现我们可以看到，epoll_wait()首先是根据epfd获取到epoll对象eventpoll然后再调用ep_poll()获取就绪事件，也就是说ep_poll()函数是真正完成就绪事件获取工作的，其实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout)&#123; int res = 0, eavail, timed_out = 0; unsigned long flags; u64 slack = 0; wait_queue_t wait; ktime_t expires, *to = NULL; /* * 如果用户态传进来的epoll_wait()的阻塞时间大于0，则换算超时时间。如果一直 * 没有就绪事件发生，那么epoll_wait()就会休眠，让出处理器，等超时时间到了才 * 返回;如果有就绪事件就先放到用户态内存中，然后会返回用户态。 */ if (timeout &gt; 0) &#123; struct timespec64 end_time = ep_set_mstimeout(timeout); slack = select_estimate_accuracy(&amp;end_time); to = &amp;expires; *to = timespec64_to_ktime(end_time); &#125; else if (timeout == 0) &#123; /* 如果用户态传进来的timeout等于1，那么如果没有就绪事件就立即返回 */ timed_out = 1; spin_lock_irqsave(&amp;ep-&gt;lock, flags); goto check_events; &#125;fetch_events: spin_lock_irqsave(&amp;ep-&gt;lock, flags); if (!ep_events_available(ep)) &#123; /* * current宏返回的是thread_info结构task字段,task正好指向与thread_info * 结构关联的那个进程描述符。这里把调用了epoll_wait()系统调用等待epoll * 事件发生的进程加入到ep-&gt;wq等待队列中。并设置了默认的回调函数用于唤 * 醒应用程序。 */ init_waitqueue_entry(&amp;wait, current); __add_wait_queue_exclusive(&amp;ep-&gt;wq, &amp;wait); for (;;) &#123; set_current_state(TASK_INTERRUPTIBLE); /* 有就绪事件，或者超时则退出循环 */ if (ep_events_available(ep) || timed_out) break; if (signal_pending(current)) &#123; res = -EINTR; break; &#125; spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); /* 休眠让出处理器 */ if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) timed_out = 1; spin_lock_irqsave(&amp;ep-&gt;lock, flags); &#125; /* 从ep-&gt;wq等待队列中将调用了epoll_wait()的进程对应的节点移除 */ __remove_wait_queue(&amp;ep-&gt;wq, &amp;wait); /* 设置当前进程的状态为RUNNING */ __set_current_state(TASK_RUNNING); &#125;check_events: /* * 判断epoll对象的rdllist链表和ovflist链表是否为空，如果不为空，说明有就绪 * 事件发生，那么该函数返回1，否则返回0 */ eavail = ep_events_available(ep); spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); /* * 如果有就绪的事件发生，那么就调用ep_send_events()将就绪的事件存放到用户态 * 内存中，然后返回到用户态，否则判断是否超时，如果没有超时就继续等待就绪事 * 件发生，如果超时就返回用户态。 */ if (!res &amp;&amp; eavail &amp;&amp; !(res = ep_send_events(ep, events, maxevents)) &amp;&amp; !timed_out) goto fetch_events; return res;&#125; 从ep_poll()函数的实现可以看到，如果有就绪事件发生，则调用ep_send_events()函数做进一步处理，在ep_send_events()函数中又会调用ep_scan_ready_list()函数获取epoll对象eventpoll中的rdllist链表。由于在我们扫描处理eventpoll中的rdllist链表的时候可能同时会有就绪事件发生，这个时候为了保证数据的一致性，在这个时间段内发生的就绪事件会临时存放在eventpoll对象的ovflist链表成员中，待rdllist处理完毕之后，再将ovflist中的内容移动到rdllist链表中，等待下次epoll_wait()的调用。 当ep_scan_ready_list()函数获取到rdllist链表中的内容之后，会调用ep_send_events_proc()进行扫描处理，即遍历rdllist链表中的epitem对象，针对每一个epitem对象调用ep_item_poll()函数去获取就绪事件的掩码，如果掩码不为0，说明该epitem对象对应的事件发生了，那么就将其对应的struct epoll_event类型的对象拷贝到用户态指定的内存中；如果掩码为0，则直接处理下一个epitem。注意此时在调用ep_item_poll()函数的时候没有设置poll_table的_qproc回调函数成员，所以只会尝试去获取就绪事件的掩码，该函数的实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465static int ep_send_events_proc(struct eventpoll *ep, struct list_head *head, void *priv)&#123; struct ep_send_events_data *esed = priv; int eventcnt; unsigned int revents; struct epitem *epi; struct epoll_event __user *uevent; struct wakeup_source *ws; poll_table pt; init_poll_funcptr(&amp;pt, NULL); /* * 这里会循环遍历就绪事件对应的item对象组成的链表，依次将链表中item对象 * 对应的就绪事件拷贝到用户态，最多拷贝用户态程序指定的就绪事件数目。 */ for (eventcnt = 0, uevent = esed-&gt;events; !list_empty(head) &amp;&amp; eventcnt &lt; esed-&gt;maxevents;) &#123; epi = list_first_entry(head, struct epitem, rdllink); ws = ep_wakeup_source(epi); if (ws) &#123; if (ws-&gt;active) __pm_stay_awake(ep-&gt;ws); __pm_relax(ws); &#125; list_del_init(&amp;epi-&gt;rdllink); /* * 虽然这里也会调用ep_item_poll()，但是pt-&gt;_qproc这个回调函数并没有设置， * 这种情况下文件对象的poll回调函数就只会去获取就绪事件对应的掩码值， * 因为当pt-&gt;_qproc会空时，poll回调函数中调用的poll_wait()什么事情都不做 * 就返回，所以poll回调函数就只会去获取事件掩码值。 */ revents = ep_item_poll(epi, &amp;pt); /* 如果revents不为0，说明确实有就绪事件发生，那么就将就绪事件拷贝到用户态内存中 */ if (revents) &#123; /* 将就绪事件及其data拷贝到用户态内存中 */ if (__put_user(revents, &amp;uevent-&gt;events) || __put_user(epi-&gt;event.data, &amp;uevent-&gt;data)) &#123; list_add(&amp;epi-&gt;rdllink, head); ep_pm_stay_awake(epi); return eventcnt ? eventcnt : -EFAULT; &#125; eventcnt++; uevent++; if (epi-&gt;event.events &amp; EPOLLONESHOT) epi-&gt;event.events &amp;= EP_PRIVATE_BITS; /* * 这个地方就是epoll中特有的EPOLLET边缘触发逻辑的实现，即当一个 * 就绪事件拷贝到用户态内存后判断这个事件类型是否包含了EPOLLET位， * 如果没有，则将该事件对应的epitem对象重新加入到epoll的rdllist链 * 表中，用户态程序下次调用epoll_wait()返回时就又能获取该epitem了 */ else if (!(epi-&gt;event.events &amp; EPOLLET)) &#123; /* 将水平触发类型的就绪事件重新加入到ep-&gt;rdllist链表中 */ list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); ep_pm_stay_awake(epi); &#125; &#125; &#125; return eventcnt;&#125; 我们知道，epoll还有另外一个重要特性就是其支持边缘触发（ET）和水平触发（LT）两种工作模式；以网络套接字为例，对于边缘触发的工作模式，当一个新的事件到来的时候，应用程序可以通过epoll_wait()系统调用获取到这个就绪事件，但是如果用户态应用程序没有一次性处理完这个事件对应的套接字缓冲区的话，那么在这个套接字没有新事件到来之前，epoll_wait()都不会在返回这个事件了；而在水平触发的工作模式下，只要某个事件对应的套接字缓冲区中还有数据没有处理完，那么在调用epoll_wait()的时候总能获取到这个就绪事件，那么在epoll中是如何实现水平触发和边缘触发这两种工作模式的呢？epoll中的实现方式十分简洁，就是在ep_send_events_proc()函数扫描rdllist链表的时候，对于每一个有就绪事件发生的epitem对象，epoll都会判断该epitem对象中存放的用户态传递进来的事件掩码是否包含了EPOLLET位，如果没有包含这个EPOLLET位，那么epoll就会将epitem对象再次加入到rdllist链表中。这样用户态应用程序再次调用epoll_wait()的时候，就又可以在rdllist链表中获取到这个epitem对象了，如果在两次调用epoll_wait()的时间内，用户态应用程序没有处理完这个事件对应的套接字缓冲区中的内容，那么后面那次调用epoll_wait()的时候，就又可以通过调用ep_item_poll()函数获取到epitem对象对应的就绪事件了，这就是水平触发工作模式的原理。参考：1、 EPOLL Linux内核源代码实现原理分析]]></content>
      <categories>
        <category>eBPF</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>eBPF</tag>
        <tag>协议栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中rps/rfs的原理及实现]]></title>
    <url>%2F2017%2F07%2F09%2Fimplementation-of-rps-and-rfs%2F</url>
    <content type="text"><![CDATA[rps的全称是Receive Package Steering，rfs的全称是Receive Flow Steering，rps和rfs是google的工程师提供的两个补丁，用以在软件层面实现报文在多个cpu之间的负载均衡以及提高报文处理的缓存命中率。rps和rfs出现的原因主要有以下两个： 1、 对于多队列网卡，网卡硬件接收队列与cpu核数在数量上不匹配导致报文在cpu之间分配不均。 2、 对于单队列网卡，rps和rfs可以在软件层面将报文平均分配到多个cpu上。 关于rps和rfs更为详细的介绍，可以参考LWN上面的文章，链接于文档末尾参考部分给出。下面分别介绍rps和rfs的原理及其具体实现。 rps rps，即Receive Package Steering，其原理是单纯地以软件方式实现接收的报文在cpu之间平均分配，即利用报文的hash值找到匹配的cpu，然后将报文送至该cpu对应的backlog队列中进行下一步的处理。上面提到的报文hash值，可以是由网卡计算得到，也可以是由软件计算得到，具体的计算也因报文协议不同而有所差异，以tcp报文为例，tcp报文的hash值是根据四元组信息，即源ip、源端口、目的ip和目的端口进行hash计算得到的。 rps中的cpu位图 上面提到，rps是利用报文的hash值找到对应的cpu，然后将报文送至该cpu的backlog队列来实现报文在多个cpu之间的负载均衡的。所以首先需要知道哪些cpu核会参与报文的分发处理。Linux是通过配置文件的方式指定哪些cpu核参与到报文的分发处理，配置文件存放的路径是：/sys/class/net/(dev)/queues/rx-(n)/rps_cpus，我们以单网卡单队列、8核cpu为例，cpu核号从0开始算起，假设我们用第7个核来处理网卡中断，我们执行如下命令设置第0、2、4、6个cpu核参与到网卡的分发处理中来： 1# echo 55 &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus 当我们设置好该配置文件之后，内核就会去获取该配置文件的内容，然后根据解析的结果生成一个用于参与报文分发处理的cpu列表（实际实现是一个柔性数组），这样当收到报文之后，就可以建立起hash-cpu的映射关系了。 在看解析函数的实现之前，需要先对相应的数据结构有所了解。从上面的配置文件中我们可以看到配置文件是放置在网卡的接口队列下的，其实对应的内核实现中也是将解析的结果放置在网卡的接收队列实例中的，其定义如下： 1234567struct netdev_rx_queue &#123;#ifdef CONFIG_RPS struct rps_map __rcu *rps_map; ……#endif ……&#125; ____cacheline_aligned_in_smp; 用来存放解析结果的就是网卡硬件接收队列实例的rps_map成员，其类型定义如下： 12345struct rps_map &#123; unsigned int len; // cpus柔性数组的长度 struct rcu_head rcu; u16 cpus[0]; // 存放cpu id的柔性数组&#125;; struct rps_map的cpus成员是用来记录配置文件中配置的参与报文分发处理的cpu核号的柔性数组，而len成员就是cpus柔性数组的长度。 了解存放配置文件解析结果的数据结构之后，再看对应的解析函数就比较容易理解了，其实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344static ssize_t store_rps_map(struct netdev_rx_queue *queue, struct rx_queue_attribute *attribute, const char *buf, size_t len)&#123; struct rps_map *old_map, *map; cpumask_var_t mask; int err, cpu, i; static DEFINE_MUTEX(rps_map_mutex); …… if (!alloc_cpumask_var(&amp;mask, GFP_KERNEL)) return -ENOMEM; /* 解析缓冲区buf中的cpu位图 */ err = bitmap_parse(buf, len, cpumask_bits(mask), nr_cpumask_bits); if (err) &#123; free_cpumask_var(mask); return err; &#125; /* 申请map对象 */ map = kzalloc(max_t(unsigned int, RPS_MAP_SIZE(cpumask_weight(mask)), L1_CACHE_BYTES), GFP_KERNEL); …… /* 根据mask对象中的cpu掩码获取对应的cpu id，并存放到map-&gt;cpus数组中 */ i = 0; for_each_cpu_and(cpu, mask, cpu_online_mask) map-&gt;cpus[i++] = cpu; /* 记录数组的长度 */ if (i) map-&gt;len = i; else &#123; kfree(map); map = NULL; &#125; …… if (map) static_key_slow_inc(&amp;rps_needed); …… free_cpumask_var(mask); return len;&#125; 根据上面设定的rps_cpus配置文件内容，以及这里的解析函数实现，我们可以知道在解析完成之后，struct rps_map类型的实例中len的值为4，而cpus数组的内容则是{0,2,4,6}。 rps中的负载均衡 从配置文件中获取到了参与报文分发处理的cpu列表信息之后，就可以根据报文的hash值，从cpu列表选取一个cpu进行后续的报文处理了。从cpu列表中获取核号的实现如下： 12345678910111213141516171819202122232425262728static int get_rps_cpu(struct net_device *dev, struct sk_buff *skb, struct rps_dev_flow **rflowp)&#123; const struct rps_sock_flow_table *sock_flow_table; struct netdev_rx_queue *rxqueue = dev-&gt;_rx; struct rps_dev_flow_table *flow_table; struct rps_map *map; int cpu = -1; u32 tcpu; u32 hash; …… /* * 如果程序进入这个流程，说明使用rps而不是rfs来获取处理该报文所在流的cpu。 * 与rfs不同，rps是单纯用报文的hash值来分发报文，而不关注处理该流中报文的 * 应用程序所在的cpu。map-&gt;cpus数组在函数store_rps_map()中已经初始化，所以 * 这里就直接用报文中存放的hash值去索引map-&gt;cpus数组即可。 */ if (map) &#123; tcpu = map-&gt;cpus[reciprocal_scale(hash, map-&gt;len)]; if (cpu_online(tcpu)) &#123; cpu = tcpu; goto done; &#125; &#125;done: return cpu;&#125; 对于函数get_rps_cpu()，这里只选取了和rps处理相关的部分，rfs的部分在介绍rfs实现的时候在详细给出。从该函数中rps处理部分可以看到获取用于分发处理报文的cpu时，是用报文对应的hash值与上cpu列表掩码之后的得到的值当作cpu列表的索引值，进而获取到对应的cpu核号。 rps的报文处理流程 在了解了rps如何根据报文的hash值来选取分发处理该报文的cpu之后，我们自然而然会想到应该在什么时候获取到这个cpu呢，也就是何时调用get_rps_cpu()函数呢？我们知道，Linux在处理网卡收包中断的时候分为了上半部处理和下半部处理，对于支持NAPI接口的驱动而言，在上半部的处理主要就是将设备加入到cpu的私有数据softnet_data的待轮询设备列表中，而下半部主要就是调用poll回调函数从网卡缓冲区中获取报文，然后上送给协议栈。而函数get_rps_cpu()就是在下半部获取到报文信息，构造好skb对象，调用netif_receive_skb()之后，准备调用__netif_receive_skb()之前被调用的，也就是在netif_receive_skb_internal()函数中被调用的，其实现如下： 1234567891011121314151617181920212223242526272829303132333435static int netif_receive_skb_internal(struct sk_buff *skb)&#123; int ret; …… rcu_read_lock();#ifdef CONFIG_RPS if (static_key_false(&amp;rps_needed)) &#123; struct rps_dev_flow voidflow, *rflow = &amp;voidflow; /* 获取处理该报文的cpu以及对应的rps_dev_flow流表 */ int cpu = get_rps_cpu(skb-&gt;dev, skb, &amp;rflow); /* * 将报文放入到该cpu私有数据对象softnet_data中的input_pkt_queue中，那 * 什么时候去input_pkt_queue队列中去取呢?在cpu的私有数据对象softnet_data * 中，有一个backlog的成员，该成员的类型为struct napi_struct，在函数 * enqueue_to_backlog()中，会将backlog加入到cpu的待轮询设备列表中，并触发 * 软中断，在软中断处理函数net_rx_action()中会依次遍历待轮询设备列表中的 * 设备，并调用设备注册的poll回调函数来进行报文处理。而对应于rps，其poll * 回调函数为process_backlog()，该函数会调用__netif_receive_skb()函数将 * 报文送至协议栈进行处理。 */ if (cpu &gt;= 0) &#123; ret = enqueue_to_backlog(skb, cpu, &amp;rflow-&gt;last_qtail); rcu_read_unlock(); return ret; &#125; &#125;#endif /* 如果没有rps/rfs的话，报文直接就送上协议栈了。 */ ret = __netif_receive_skb(skb); rcu_read_unlock(); return ret;&#125; 从函数netif_receive_skb_internal()的实现中可以看到，当rps调用函数get_rps_cpu()获取到用于分发处理报文的目标cpu之后，如果目标cpu有效，则会调用enqueue_to_backlog()函数做进一步处理，而不是直接调用函数__netif_receive_skb()将报文送上协议栈处理。那函数enqueue_to_backlog()做了什么工作呢，而进入了rps处理流程的报文又是什么时候会被上送到协议栈处理呢？ 先来看下函数enqueue_to_backlog()主要做了哪些工作，其实该函数做的主要事情是尝试将报文加入到cpu私有数据对象softnet_data的input_pkt_queue队列中，如果该队列满了就将设备加入到cpu私有数据对象softnet_data的待轮询设备列表中，并触发一个收包软中断(软中断处理函数会调用poll回调函数从该队列中取数据)，然后再尝试将报文加入到input_pkt_queue队列中。该函数的实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243static int enqueue_to_backlog(struct sk_buff *skb, int cpu, unsigned int *qtail)&#123; struct softnet_data *sd; unsigned long flags; unsigned int qlen; sd = &amp;per_cpu(softnet_data, cpu); local_irq_save(flags); rps_lock(sd); …… qlen = skb_queue_len(&amp;sd-&gt;input_pkt_queue); if (qlen &lt;= netdev_max_backlog &amp;&amp; !skb_flow_limit(skb, qlen)) &#123; if (qlen) &#123;enqueue: /* 将报文添加到sd-&gt;input_pkt_queue队列的末尾，并更新队列尾部索引 */ __skb_queue_tail(&amp;sd-&gt;input_pkt_queue, skb); input_queue_tail_incr_save(sd, qtail); rps_unlock(sd); local_irq_restore(flags); return NET_RX_SUCCESS; &#125; /* * 程序执行到这里表明sd-&gt;input_pkt_queue已经满了，这个时候需要将sd-&gt;backlog加入到 * cpu私有数据的待轮询设备列表sd-&gt;poll_list中，并触发接收软中断NET_RX_SOFTIRQ， * 软中断处理函数net_rx_action()中会从调用poll回调函数从input_pkt_queue队列中取包。 * 这样input_pkt_queue队列中又有空闲位置用来存放报文了。 * rps/rfs注册的poll回调的实现具体可以参考process_backlog()。 */ if (!__test_and_set_bit(NAPI_STATE_SCHED, &amp;sd-&gt;backlog.state)) &#123; if (!rps_ipi_queued(sd)) ____napi_schedule(sd, &amp;sd-&gt;backlog); &#125; goto enqueue; &#125;drop: rps_unlock(sd); local_irq_restore(flags); …… return NET_RX_DROP;&#125; 在函数enqueue_to_backlog()中我们看到，如果cpu私有数据对象softnet_data的input_pkt_queue队列满了，那么就会将设备对象backlog(其实是struct napi_struct类型的实例)加入到cpu私有数据对象softnet_data的待轮询设备列表中，并触发一个NET_RX_SOFTIRQ的收包软中断。当中断子系统检测到有软中断发生，就会调用软中断的处理函数net_rx_action()，在函数net_rx_action()中则会遍历cpu私有数据对象softnet_data的待轮询的设备列表，当遍历到rps对应的backlog设备对象的时候，就会调用该设备对象注册的poll回调函数，即process_backlog()来处理input_pkt_queue队列。函数process_backlog()的实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041static int process_backlog(struct napi_struct *napi, int quota)&#123; struct softnet_data *sd = container_of(napi, struct softnet_data, backlog); bool again = true; int work = 0; …… napi-&gt;weight = weight_p; while (again) &#123; struct sk_buff *skb; /* 循环获取sd-&gt;process_queue队列中的报文，然后调用__netif_receive_skb()送至协议栈 */ while ((skb = __skb_dequeue(&amp;sd-&gt;process_queue))) &#123; rcu_read_lock(); __netif_receive_skb(skb); rcu_read_unlock(); input_queue_head_incr(sd); if (++work &gt;= quota) return work; &#125; local_irq_disable(); rps_lock(sd); if (skb_queue_empty(&amp;sd-&gt;input_pkt_queue)) &#123; napi-&gt;state = 0; again = false; &#125; else &#123; /* * 将sd-&gt;input_pkt_queue队列内容拼接到sd-&gt;process_queue队列中，从 * 上面的循环可以看出，process_backlog()并不是直接处理sd-&gt;input_pkt_queue队列的， * 而是通过sd-&gt;process_queue队列做中转，处理完process_queue队列报文之后，再将 * input_pkt_queue队列的内容拼接到process_queue队列中，下次又继续处理process_queue队列。 */ skb_queue_splice_tail_init(&amp;sd-&gt;input_pkt_queue, &amp;sd-&gt;process_queue); &#125; rps_unlock(sd); local_irq_enable(); &#125; return work;&#125; 从函数process_backlog()的实现我们可以看到处理input_pkt_queue队列其实就是从该队列中取出报文，然后调用__netif_receive_skb()上报文送至协议栈进行后续处理。所以对于本节开始提出的问题—rps处理流程中的报文是何时被送至协议栈的，现在就比较清晰了，对于没有rps的处理流程，如果采用的是NAPI收包方式，那么软中断处理函数net_rx_action()会调用网卡驱动注册的poll回调函数从网卡中获取到报文数据后就将报文数据上送至协议栈；而对于有rps的处理流程，软中断处理函数net_rx_action()会调用网卡驱动注册的poll回调函数从网卡中获取到报文数据后，暂时不直接送至协议栈，而是选择一个目标cpu，将报文放到该cpu私有数据对象softnet_data的input_pkt_queue队列中，待对列input_pkt_queue满了之后，就将该cpu对应的backlog设备对象加入到该cpu的待轮询设备列表中，并触发软中断，软中断处理函数轮询到backlog设备对象后，调用poll回调函数process_backlog()从input_pkt_queue队列中取出报文，再上送至协议栈。 到这里rps的原理及实现就介绍的差不多了。下面介绍下rfs的原理及实现。 rfs 从rps的原理来看，我们知道rps只是根据报文的hash值从分发处理报文的cpu列表中选取一个目标cpu，这样虽然负载均衡的效果很好，但是当用户态处理报文的cpu和内核处理报文软中断的cpu不同的时候，就会导致cpu的缓存不命中，影响性能。而rfs就是用来处理这种情况的，rfs的目标是通过指派处理报文的应用程序所在的cpu来在内核态处理报文，以此来增加cpu的缓存命中率。所以rfs相比于rps，主要差别就是在选取分发处理报文的目标cpu上，而rfs还需要依靠rps提供的机制进行报文的后续处理。 rfs实现指派处理报文的应用程序所在的cpu来在内核态处理报文这一目标主要是依靠两个流表来实现的，其中一个是设备流表，记录的是上次在内核态处理该流中报文的cpu；另外一个是全局的socket流表，记录的是流中的报文渴望被处理的目标cpu。 rfs的设备流表 设备流表定义在网卡的硬件接收队列中，如下： 1234567struct netdev_rx_queue &#123;#ifdef CONFIG_RPS …… struct rps_dev_flow_table __rcu *rps_flow_table;#endif ……&#125; ____cacheline_aligned_in_smp; 设备流表的类型为struct rps_dev_flow_table，其定义为： 12345struct rps_dev_flow_table &#123; unsigned int mask; struct rcu_head rcu; struct rps_dev_flow flows[0];&#125;; 设备流表中主要包括一个类型为struct rps_dev_flow的柔性数组，以及存放着柔性数组大小的成员mask。而struct rps_dev_flow类型的实例则主要包括存放着上次处理该流中报文的cpu以及所在cpu私有数据对象softnet_data的input_pkt_queue队列尾部索引的两个成员，其定义如下： 12345struct rps_dev_flow &#123; u16 cpu; /* 处理该流的cpu */ u16 filter; unsigned int last_qtail; /* sd-&gt;input_pkt_queue队列的尾部索引，即该队列长度 */&#125;; 上面说到设备流表中的mask成员是类型为struct rps_dev_flow的柔性数组的大小，也就是流表项的数目，这个主要是通过配置文件 /sys/class/net/(dev)/queues/rx-(n)/rps_flow_cnt进行指定的。当我们设置了配置文件，那么内核就会获取到数据，并初始化网卡硬件接收队列中的设备流表成员rps_flow_table，初始化过程可以参考函数store_rps_dev_flow_table_cnt()。 rfs的全局socket流表 rps_sock_flow_table是一个全局的数据流表，这个表中包含了数据流渴望被处理的CPU。这个CPU是当前处理流中报文的应用程序所在的CPU。全局socket流表会在调recvmsg，sendmsg (特别是inet_accept(), inet_recvmsg(), inet_sendmsg(), inet_sendpage() and tcp_splice_read())，被设置或者更新。全局socket流表rps_sock_flow_table的定义如下： 1234struct rps_sock_flow_table &#123; u32 mask; u32 ents[0] ____cacheline_aligned_in_smp;&#125;; struct rps_sock_flow_table类型的mask成员存放的就是ents这个柔性数组的大小，该值也是通过配置文件的方式指定的，相关的配置文件为 /proc/sys/net/core/rps_sock_flow_entries。 上面说到全局socket流表会在调用recvmsg()等函数时被更新，而在这些函数中是通过调用函数sock_rps_record_flow()来更新或者记录流表项信息的，而sock_rps_record_flow()中最终又是调用函数rps_record_sock_flow()来更新ents柔性数组的，该函数实现如下： 12345678910111213141516171819202122232425static inline void rps_record_sock_flow(struct rps_sock_flow_table *table, u32 hash)&#123; if (table &amp;&amp; hash) &#123; /* 用hash值对表的掩码求余，获取对应的表项索引 */ unsigned int index = hash &amp; table-&gt;mask; /* 保留hash值的高位，将用来存放cpu id的低位清零 */ u32 val = hash &amp; ~rps_cpu_mask; /* We only give a hint, preemption can change CPU under us */ /* * raw_smp_processor_id()获取当前cpu，经过上面和这两部分，val存放了 * 高位hash值和cpu id，为什么在高位还需要保留部分hash值而不直接存放cpu * id呢?原因是因为在get_rps_cpu()函数中，还会用val中存放的高位hash值 * 来校验skb中的hash值是否在rps_sock_flow_table中有对应的记录。详情可以 * 参考get_rps_cpu()。 */ val |= raw_smp_processor_id(); /* 记录hash值对应的cpu */ if (table-&gt;ents[index] != val) table-&gt;ents[index] = val; &#125;&#125; 在该函数中，table传递的就是全局socket流表rps_sock_flow_table的地址，hash则是流hash，也就是通过报文计算得到的hash值。从这个函数的实现我们可以看到，ents柔性数组中存放的不仅仅是当前处理流中报文的应用程序所在的cpu，其中还包括了流的hash值的高位数据，其内存布局如下： 为什么流表项中不直接存放cpu信息，还要存放hash值的高位部分呢？这个主要使用来做校验的，这部分可以在函数get_rps_cpu()中看到。介绍了rfs负载均衡策略中会使用到的两种流表之后，再来理解其负载均衡策略就比较清晰了。 rfs的负载均衡 上面说到，rfs相比于rps，主要差别就是在选取分发处理报文的目标cpu上。所以rfs负载均衡策略的实现也主要体现在函数get_rps_cpu()中，其实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485static int get_rps_cpu(struct net_device *dev, struct sk_buff *skb, struct rps_dev_flow **rflowp)&#123; const struct rps_sock_flow_table *sock_flow_table; struct netdev_rx_queue *rxqueue = dev-&gt;_rx; struct rps_dev_flow_table *flow_table; struct rps_map *map; int cpu = -1; u32 tcpu; u32 hash; …… /* 从接收队列中获取记录着上次处理报文所在流的cpu的流表rps_dev_flow_table */ flow_table = rcu_dereference(rxqueue-&gt;rps_flow_table); map = rcu_dereference(rxqueue-&gt;rps_map); if (!flow_table &amp;&amp; !map) goto done; skb_reset_network_header(skb); /* * 获取报文的hash值，如果硬件计算了，就用硬件计算的，否则进行软件计算， * 后续会用这个hash值去索引rps_dev_flow_table和rps_sock_flow_table两个流表 */ hash = skb_get_hash(skb); if (!hash) goto done; sock_flow_table = rcu_dereference(rps_sock_flow_table); if (flow_table &amp;&amp; sock_flow_table) &#123; struct rps_dev_flow *rflow; u32 next_cpu; u32 ident; /* First check into global flow table if there is a match */ ident = sock_flow_table-&gt;ents[hash &amp; sock_flow_table-&gt;mask]; /* * 从函数rps_record_sock_flow()中可以看到，ident由两部分组成，即 * 高位是hash值的一部分，低位是cpu id。所以这里用异或操作符(^)来判断 * ident高位部分和hash值是否一样，然后低位又和cpu id掩码按位取反的值 * 进行按位与操作，经过这样的计算，如果最后面表达式值为0，说明hash值 * 在rps_sock_flow_table中有记录，则用rfs进行后续处理，否则用rps。 */ if ((ident ^ hash) &amp; ~rps_cpu_mask) goto try_rps; /* * 在函数rps_record_sock_flow()中我们可以看到，rps_sock_flow_table全局 * 流表中ents数组中存放的并不是单纯的cpu id，而是hash值高位和cpu id的组合。 * 所以这里用cpu掩码来获取cpu id。全局流表rps_sock_flow_table中存放的是流 * 渴望被处理的cpu。 */ next_cpu = ident &amp; rps_cpu_mask; /* tcpu是上次处理这个流的cpu */ rflow = &amp;flow_table-&gt;flows[hash &amp; flow_table-&gt;mask]; tcpu = rflow-&gt;cpu; /* * 如果流渴望被处理的cpu和上次处理这个流的cpu不同，说明处理流的应用程序 * 被调度到了另外一个核next_cpu上，所以这里需要将tcpu设置为流渴望被 * 处理的cpu，这个流所在报文需要被送到next_cpu上。 * 对于最后一个条件，结合tcpu!=next_cpu，可以这样理解:处理流的应用程序 * 目前被调度到了另外一个cpu上，而上次处理该流中报文的cpu(即tcpu)中又没有 * 剩余没有处理的这条流中的报文，那么就可以将上次处理该流报文的cpu更新为 * 应用程序所在的cpu；如果tcpu中还有剩余的属于该流的报文没有处理完，那么 * 就会继续将本次处理的属于该流的新报文继续让tcpu来处理，这样可以保证同 * 属于一条流的报文可以被有序的处理，而不会乱序。 */ if (unlikely(tcpu != next_cpu) &amp;&amp; (tcpu &gt;= nr_cpu_ids || !cpu_online(tcpu) || ((int)(per_cpu(softnet_data, tcpu).input_queue_head - rflow-&gt;last_qtail)) &gt;= 0)) &#123; tcpu = next_cpu; rflow = set_rps_cpu(dev, skb, rflow, next_cpu); &#125; if (tcpu &lt; nr_cpu_ids &amp;&amp; cpu_online(tcpu)) &#123; *rflowp = rflow; cpu = tcpu; goto done; &#125; &#125; ……done: return cpu;&#125; 从其实现可以看到，rfs的负载均衡策略主要是通过判断报文的hash值(亦即流hash值)所对应的两个流表（设备流表和全局socket流表）中的记录的cpu是否相同，如果相同，那么该cpu就会被当作内核态处理该流中报文的cpu；如果不相同，那么在满足一定条件的情况需要将内核态处理该流中报文的cpu设置成用户态处理该流中报文的应用程序所在的cpu，如果条件不满足的话，那么还是会继续沿用上一次在内核态处理该流中的报文的cpu来进行本次报文的处理。切换用于在内核态处理流中报文的cpu所需要满足的任一条件如下： 1、 上一次在内核态处理该流中报文的cpu是离线的。 2、 上一次在内核态处理该流中报文的cpu是无效值（初始化会设置为nr_cpu_ids，该值无效）。 3、上一次在内核态处理该流中报文的cpu的私有数据对象softnet_data的input_pkt_queue队列的头部索引input_queue_head的值大于设备流表中记录的input_pkt_queue队列的尾部索引值，这说明上一次在内核态处理该流中报文的cpu的input_pkt_queue队列中已经没有属于该流的未上送给协议栈的报文了，此时如果切换在内核态处理属于该流的报文的cpu也不会导致属于该流的报文被乱序处理了。 到这里rfs的原理和实现就介绍完了。 参考：1、 rps：Receive Package Steering2、 rfs：Receive Flow Steering]]></content>
      <categories>
        <category>eBPF</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>eBPF</tag>
        <tag>协议栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于82599网卡的二层网络数据包发送]]></title>
    <url>%2F2017%2F05%2F01%2Fintel-82599-transmit-packet%2F</url>
    <content type="text"><![CDATA[这篇文档主要介绍了网络数据包在二层的发送流程。网络数据包在二层的发送主要包括了网络设备层和驱动层两个部分，所以下面将会从这两个方面讲述报文在二层的发送流程。 网络设备层在报文发送时的处理流程 当网络协议栈上层准备好了待发送的报文，即构造了一个管理着待发送报文数据的skb对象之后，便会调用网络设备层的主入口函数dev_queue_xmit()进行后续的发送处理。 当上层已经准备好的skb对象达到网络设备层之后，一般来说并不是直接交给网卡驱动的（在没有设置TCQ_F_CAN_BYPASS的情况下），而是会用类型为struct netdev_queue的发送队列先将skb对象缓存起来，接着依次处理发送队列中的skb对象，将其中的报文数据交给网卡发送出去。而在类型为struct netdev_queue的发送队列中，真正用来缓存skb对象的则是类型为struct Qdisc的实例，该类型的实例通常会实现一组出入队列的回调函数，来实现skb的缓存，重传和移除等操作。当发送队列中struct Qdisc的实例设置了TCQ_F_CAN_BYPASS标志的时候，会将上层下发的skb对象直接通过网卡驱动交给网卡进行发送。 下图是网络设备层的报文发送主要流程： 从上图中我们可以看到在网络设备层用struct Qdisc队列实例来缓存skb对象时，会调用函数__qdisc_run()来处理struct Qdisc队列实例中的skb对象，其实现如下： 1234567891011121314151617181920212223242526272829void __qdisc_run(struct Qdisc *q)&#123; int quota = weight_p; int packets; /* * 循环发送qdisc队列中的报文，直到达到了发送阈值，或者队列中的报文发送完毕， * 或者时间片到了，其他进程需要使用cpu */ while (qdisc_restart(q, &amp;packets)) &#123; /* * Ordered by possible occurrence: Postpone processing if * 1. we've exceeded packet quota * 2. another process needs the CPU; */ quota -= packets; /* * 如果quota &lt;= 0，说明qdisc队列中仍然有报文没有发送完，这个时候需要触发 * 软中断，在软中断处理函数中发送剩余报文 */ if (quota &lt;= 0 || need_resched()) &#123; __netif_schedule(q); break; &#125; &#125; qdisc_run_end(q);&#125; 从qdisc_run()函数的实现我们可以看到会有如下两种情况发生： 1) 一次性将struct Qdisc队列实例中所有的skb对象通过网卡驱动交给网卡发送出去。 2) 在某次处理struct Qdisc队列实例中的skb对象时，由于某些原因中途停止了，队列实例中可能还有skb对象没有处理完。 当struct Qdisc队列实例中还有skb对象没有处理完时，就会调用netif_schedule()函数触发一次发送软中断（NET_TX_SOFTIRQ），并将struct Qdisc队列实例加入到cpu私有数据对象softnet_data的output_queue链表成员中，在软中断中会遍历output_queue，继续处理其中的struct Qdisc队列实例剩余的skb对象。发送软中断处理函数实现如下： 123456789101112131415161718192021222324252627282930313233343536static __latent_entropy void net_tx_action(struct softirq_action *h)&#123; struct softnet_data *sd = this_cpu_ptr(&amp;softnet_data); …… /* * softnet_data-&gt;output_queue链表不为空，说明其中存有数据没有发送完毕的qdisc * 队列，那么这个时候需要调用qdisc_run()尝试将队列中的报文发送出去 */ if (sd-&gt;output_queue) &#123; struct Qdisc *head; local_irq_disable(); head = sd-&gt;output_queue; sd-&gt;output_queue = NULL; sd-&gt;output_queue_tailp = &amp;sd-&gt;output_queue; local_irq_enable(); while (head) &#123; struct Qdisc *q = head; spinlock_t *root_lock; head = head-&gt;next_sched; root_lock = qdisc_lock(q); spin_lock(root_lock); /* We need to make sure head-&gt;next_sched is read * before clearing __QDISC_STATE_SCHED */ smp_mb__before_atomic(); clear_bit(__QDISC_STATE_SCHED, &amp;q-&gt;state); qdisc_run(q); spin_unlock(root_lock); &#125; &#125;&#125; 在net_tx_action()函数的实现中可以看到，其间接又调用了__qdisc_run()函数，这说明只要struct Qdisc队列实例中有skb对象没有处理完，就会继续触发发送软中断直到所有的队列中所有的skb对象都被处理完。 ixgbe驱动中和数据发送相关的内容 在二层网络报文接收流程中，接收报文描述符承载了报文从网卡到主存的过程，与之相对应的，发送报文描述符则承载了报文从主存到网卡的过程。对于网卡驱动而言，当收到来自协议栈上层下发的网络报文时，网卡驱动会将存放着报文数据的地址写入到报文发送描述符中，并将填充了报文地址信息的描述符传递给网卡，而网卡则从报文发送描述符中存放的地址中读取报文数据。 报文发送描述符 对于82599网卡而言，其支持两种格式的报文发送描述符，即传统格式和高级格式。虽然有两种不同格式的报文发送描述符，但是两种格式的报文发送描述符所占用的内存大小是一样的（目前为16字节），只是对这块内存使用有所不同。对于两种不同格式的报文发送描述符，可以通过设置报文发送描述符中的TDESC.DEXT位进行区分，当该位设置为0的时候，表明使用的是传统格式；当该位设置为1的时候，表明使用的是高级格式。下面介绍高级格式的报文发送描述符。 相比于传统格式，高级格式的报文发送描述符可以用来支持更多的功能特性。高级格式的报文描述符由于需要支持更多的功能特性，所以分为了读格式和回写格式。 先来看下82599网卡中读格式的定义，如下图： 从图中可以看到，读格式的报文发送描述符中主要包含了报文数据所在的内存地址和一些报文元信息，如报文长度等。这里不再详述，详细可以参考82599网卡的datasheet。 再来看下82599网卡中回写格式的定义： 从图中可以看到，回写格式的报文发送描述符中有效的域很少，只有STA，而STA中有效位有只有DD位，网卡驱动可以通过该位是否被置位来判断报文发送描述符对应的报文数据是否已经被网卡处理过了。 在初始化阶段，网卡驱动会申请一定数量的报文发送描述符，并将这些内存进行dma一致性映射，获取对应的物理地址，并写入到网卡的寄存器中，这样网卡驱动和网卡就能同时操作这些报文发送描述符了。当网卡驱动收到协议栈下发的skb对象后，会将skb对象中存放的报文数据进行dma映射，获取对应的物理地址，并存放到报文发送描述符中对应的成员中，这样网卡就能从报文发送描述符中获取存放了报文数据的物理地址，然后从该地址中读取报文数据，并发送到网络中去。 ### 报文发送描述符环形队列 上面说到，报文发送描述符承载了报文从主存流入到网卡的过程，是网卡驱动和网卡都会操作的对象，那么自然而然会有以下几个疑问： 1）、报文发送描述符是以何种组织形式在网卡驱动和网卡之间进行传递的？ 2）、网卡驱动怎么通知网卡报文发送描述符可用的？ 在报文发送流程中，报文发送描述符是通过环形队列来管理的，当然这个环形队列是逻辑上的，队列中的描述符在内存上是连续的。网卡或者网卡驱动在进行操作的时候，如果发现已经到达了队列的末尾，那么下次操作又会从队列头部开始，从而实现环形的操作逻辑。报文发送描述符环形队列的结构体如下： 对于第一和第二个问题，其中也已经在上面的描述符环形队列图中有体现。在对问题进行回答之前先要了解下82599网卡中和报文发送描述符环形队列相关的几个寄存器。 1）、TDBA寄存器。这个寄存器存放了报文发送描述符环形队列的起始地址，也就是上图中Base指向的地址。 2）、TDLEN寄存器。这个寄存器存放了报文发送描述符环形队列的长度，也就是报文发送描述符环形队列所占用的字节数，对应上图中的Size。 3）、TDH寄存器。这个寄存器存放的是一个距离队列头部的偏移值，代表的是第一个填充了报文地址信息的描述符。当网卡处理完一个描述符对应的报文数据后，就会更新TDH寄存器的值，使之指向下一个填充了报文地址信息的描述符。也就是说这个寄存器的值是由网卡来更新的，该寄存器对应上图中的Head。 4）、TDT寄存器。这个寄存器存放的也是一个距离队列头部的偏移值，代表的是最后一个存放了报文地址信息的描述符的下一个描述符。当网卡驱动将一个skb对象中所有的数据分段对应的物理地址都填充到了对应的报文发送描述符中后，就会更新该寄存器的值，使之指向下一个即将被填充报文地址信息并给网卡使用的描述符，该寄存器对应上图中的Tail。 在了解了这几个寄存器的作用之后，对于本节一开始提出的两个问题就比较容易知晓了。对于第一个问题，报文描述符是以环形队列的方式来组织的；对于第二个问题，因为网卡驱动在填充完一个skb对象中数据分段的报文地址信息到报文发送描述符后，网卡驱动都会更新TDT寄存器的值，所以网卡可以根据TDT寄存器知道自己当前可用的描述符信息，简单来说TDH和TDT之间的描述符就是网卡可以使用的。 ### 数据分段和报文发送描述符关系 上面说到网卡驱动会将skb对象中存放了报文数据的内存进行dma映射，并将得到的物理地址存放到报文发送描述符中，而一个skb对象中可能存有多个数据分段，对于这种情况，网卡驱动则会将一个数据分段对应一个报文发送描述符，其对应关系如下图： ### 报文发送描述符的回收 当网卡完成报文发送之后，就会触发硬件中断。这里需要注意的是，新的数据包达到或者外发数据包的传输已经完成所触发的中断对应的中断号是同一个，所以在这个中断号对应的中断处理函数中需要考虑到是新的数据包达到所产生的中断，还是外发数据包的传输已经完成触发的中断。 另外，ixgbe驱动中因为使用了NAPI的收包方式，所以在中断处理函数中只是调用NAPI模块调度接口napi_schedule_irqoff()将设备加入到cpu私有数据中类型为struct softnet_data的对象的待轮询设备链表中，并触发软中断，而在软中断处理函数net_rx_action()中又只会调用设备注册的NAPI回调函数poll。所以无论是新的数据包达到，或者是外发数据包的传输已经完成所触发的中断，最终都会调用设备注册给NAPI接口的poll回调函数，因此报文发送描述符的回收也是在这个函数中完成的，在ixgbe驱动中，poll回调函数就是ixgbe_poll()。在ixgbe_poll()函数中又会调用ixgbe_clean_tx_irq()函数来完成报文发送描述符的回收，该函数实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107static bool ixgbe_clean_tx_irq(struct ixgbe_q_vector *q_vector, struct ixgbe_ring *tx_ring, int napi_budget)&#123; struct ixgbe_adapter *adapter = q_vector-&gt;adapter; struct ixgbe_tx_buffer *tx_buffer; union ixgbe_adv_tx_desc *tx_desc; unsigned int total_bytes = 0, total_packets = 0; unsigned int budget = q_vector-&gt;tx.work_limit; /* 获取第一个可以被网卡驱动处理的描述符索引 */ unsigned int i = tx_ring-&gt;next_to_clean; if (test_bit(__IXGBE_DOWN, &amp;adapter-&gt;state)) return true; tx_buffer = &amp;tx_ring-&gt;tx_buffer_info[i]; tx_desc = IXGBE_TX_DESC(tx_ring, i); i -= tx_ring-&gt;count; do &#123; /* * tx_buffer-&gt;next_to_watch保存的是环形队列中第一个没有存放某个报文数据 * 的报文发送描述符 */ union ixgbe_adv_tx_desc *eop_desc = tx_buffer-&gt;next_to_watch; /* if next_to_watch is not set then there is no work pending */ /* * 如果某个报文发送描述符对应的报文缓冲区的next_to_watch成员没有设置， * 说明这个缓冲区对象并不是某个报文对应的第一个缓冲区(当报文以共享方式 * 存放的时候，一个报文可能会对应多个缓冲区)。 */ if (!eop_desc) break; read_barrier_depends(); /* * 如果eop_desc描述符对应的dd为没有被设置，说明网卡还没有处理完属于该报文 * 对应的所有缓冲区，所以就暂时不处理对应的描述符了，而是要等到属于该报文 * 的所有描述符都被网卡处理完了之后才去处理属于该报文的所有描述符 */ if (!(eop_desc-&gt;wb.status &amp; cpu_to_le32(IXGBE_TXD_STAT_DD))) break; tx_buffer-&gt;next_to_watch = NULL; total_bytes += tx_buffer-&gt;bytecount; total_packets += tx_buffer-&gt;gso_segs; napi_consume_skb(tx_buffer-&gt;skb, napi_budget); /* 取消skb-&gt;data指向内存的dma映射，以让cpu可以使用该块内存 */ dma_unmap_single(tx_ring-&gt;dev, dma_unmap_addr(tx_buffer, dma), dma_unmap_len(tx_buffer, len), DMA_TO_DEVICE); tx_buffer-&gt;skb = NULL; dma_unmap_len_set(tx_buffer, len, 0); /* 取消skb中分片数据对应内存的dma映射，以让cpu可以使用该块内存 */ while (tx_desc != eop_desc) &#123; tx_buffer++; tx_desc++; i++; if (unlikely(!i)) &#123; i -= tx_ring-&gt;count; tx_buffer = tx_ring-&gt;tx_buffer_info; tx_desc = IXGBE_TX_DESC(tx_ring, 0); &#125; /* unmap any remaining paged data */ if (dma_unmap_len(tx_buffer, len)) &#123; dma_unmap_page(tx_ring-&gt;dev, dma_unmap_addr(tx_buffer, dma), dma_unmap_len(tx_buffer, len), DMA_TO_DEVICE); dma_unmap_len_set(tx_buffer, len, 0); &#125; &#125; /* move us one more past the eop_desc for start of next pkt */ tx_buffer++; tx_desc++; i++; if (unlikely(!i)) &#123; i -= tx_ring-&gt;count; tx_buffer = tx_ring-&gt;tx_buffer_info; tx_desc = IXGBE_TX_DESC(tx_ring, 0); &#125; /* issue prefetch for next Tx descriptor */ prefetch(tx_desc); /* update budget accounting */ budget--; &#125; while (likely(budget)); /* 更新环形队列中的next_to_clean */ i += tx_ring-&gt;count; tx_ring-&gt;next_to_clean = i; …… return !!budget;&#125; 到这里，报文在二层的发送流程就介绍完了。 参考：1、http://www.intel.com/content/www/us/en/embedded/products/networking/82599-10-gbe-controller-datasheet.html]]></content>
      <categories>
        <category>eBPF</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>eBPF</tag>
        <tag>网卡驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于82599网卡的二层网络数据包接收]]></title>
    <url>%2F2017%2F04%2F16%2Fintel-82599-receive-packet%2F</url>
    <content type="text"><![CDATA[本篇文档主要描述了网络数据包在二层的接收流程，主要包括以下三个部分： 1）、82599网卡和数据包接收相关的内容； 2）、ixgbe网卡驱动数据包接收相关的配置； 3）、ixgbe网卡驱动napi接口的处理。 82599网卡和数据包接收相关的内容 这一部分要介绍的是82599网卡中和数据包接收相关的内容。网络报文接收流程所涉及的内容很多，如报文过滤、mac层卸载、报文接收描述符、校验和卸载以及分离报文有效载荷和头部等，由于篇幅原因，这里只介绍了报文接收描述符相关的内容，其他内容会在后续描述中进行穿插。 说到网卡报文接收，就必须得说到报文接收描述符，因为报文接收描述符承载了报文从网卡流入到主存的过程。对于网卡硬件而言，当网卡收到网络报文的时候，会往报文接收描述符中指定的地址写入报文数据，而网卡驱动则会从报文接收描述符中指定的地址读取报文，并送往上层协议栈处理。 除了上面说到的存放报文的内存地址，报文接收描述符中还有用于存储报文信息的域。对于82599网卡而言，其支持两种格式的报文接收描述符，即传统格式和高级格式。虽然有两种不同格式的报文接收描述符，但是两种格式的报文接收描述符所占用的内存大小是一样的（目前为16字节），只是对这块内存使用有所不同。对于两种不同格式的报文接收描述符，可以在网卡驱动初始化的时候进行配置，通过设置网卡的SRRCTL寄存器的DRSCTYPE域进而选择使用某种格式的报文接收描述符。在初始化阶段，网卡驱动会申请报文描述符，并填充描述符中相关的域，然后告诉网卡该描述符可用，后续网卡接收到报文就可以用报文描述符来存储报文相关的信息，然后网卡将报文描述符回写给网卡驱动，网卡驱动从中获取所需要信息，并交由上层进行处理。 传统格式报文接收描述符 先来看下82599网卡中对传统格式报文接收描述符的定义，如下： 从上面的图中可以看到，报文接收描述符的低八个字节存放的是用于存放报文的内存起始地址，而高八个字节存放的是网卡对报文进行预处理得到的一些信息，如报文长度，VLAN Tag以及校验和信息等，这部分信息会在网卡回写报文描述符给驱动的时候存到描述符对应的域中。对于一些比较固定的功能，比如报文相关校验和计算，VLAN头的解析等功能都可以卸载到网卡，由网卡来操作，这样可以加速报文的处理。 ### 高级格式报文接收描述符 相比于传统格式，高级格式的报文接收描述符可以用来支持更多的功能特性，如分离报文有效负载和报文头等。高级格式的报文描述符由于需要支持更多的功能特性，所以分为了读格式和回写格式。 先来看下82599网卡中读格式的定义，如下图： 从图中可以看到，读格式的报文描述符中主要有四个部分，分别是报文缓冲区地址、A0位、头缓冲区地址和DD位。对于报文缓冲区地址和头缓冲区地址，顾名思义，存储的就是用来存放报文有效载荷和头部的缓冲区首地址。而对于DD位的作用，网卡驱动可以通过读取该位的值来判断该描述符对应的缓冲区中是否已经存放了网卡接收的报文。 再来看下82599网卡中回写格式的定义，如下图： 回写代表的就是网卡往描述符对应的缓冲区中存放了报文数据，并将报文相关的元信息写入到描述符对应的域中，并设置DD位，以告诉网卡驱动该描述已经存放了报文信息。回写格式中涉及到很多和报文相关的信息，如接收报文时所使用的RSS类型，报文长度和报文接收状态等信息。这里不一一介绍，详细可以参考82599的datasheet。 ### 报文接收描述符环形队列 上面说到，报文接收描述符承载了报文从网卡流入到主存的过程，是网卡驱动和网卡都会操作的对象，那么自然而然会有以下几个疑问： 1）、报文接收描述符是以何种组织形式在网卡驱动和网卡之间进行传递的？ 2）、网卡驱动怎么通知网卡报文接收描述符可用的？ 在报文接收流程中，报文接收描述符是通过环形队列来管理的，当然这个环形队列是逻辑上的，队列中的描述符在内存上是连续的。网卡或者网卡驱动在进行操作的时候，如果发现已经到达了队列的末尾，那么下次操作又会从队列头部开始，从而实现环形的操作逻辑。报文接收描述符环形队列的结构体如下： 对于第一和第二个问题，其中也已经在上面的描述符环形队列图中有体现。在对问题进行回答之前先要了解下82599网卡中和报文接收描述符环形队列相关的几个寄存器。 1）、RDBA寄存器。这个寄存器存放了报文接收描述符环形队列的起始地址，也就是上图中Base指向的地址。 2）、RDLEN寄存器。这个寄存器存放了报文接收描述符环形队列的长度，也就是接收描述符环形队列所占用的字节数，对应上图中的Size。 3）、RDH寄存器。这个寄存器存放的是一个距离队列头部的偏移值，代表的是第一个可以被网卡用来存放报文信息的描述符。当网卡完成了将一个报文信息存放到描述符后，就会更新RDH寄存器的值，使之指向下一个即将用来存放报文信息的描述符。也就是说这个寄存器的值是由网卡来更新的，该寄存器对应上图中的Head。 4）、RDT寄存器。这个寄存器存放的也是一个距离队列头部的偏移值，代表的是硬件可以用来存放报文信息的最后一个描述符的下一个描述符。当网卡驱动填充了报文描述中的报文缓冲区地址后就会更新该寄存器的值，使之指向下一个即将填充地址信息并给网卡使用的描述符，该寄存器对应上图中的Tail。 在了解了这几个寄存器的作用之后，对于本节一开始提出的两个问题就比较容易知晓了。对于第一个问题，报文描述符是以环形队列的方式来组织的；对于第二个问题，因为网卡驱动在提供可用报文接收描述符给网卡后都会更新RDT寄存器的值，所以网卡可以根据RDT寄存器知道自己当前可用的描述符信息，简单来说RDH和RDT之间的描述符就是网卡可以使用的。 ## ixgbe网卡驱动数据包接收相关的配置 第一部分已经讲了网卡对描述符的定义，以及网卡中用来操作描述符环形队列的几个相关的寄存器，对网卡是如何使用描述符有了一定的了解。这一部分我们一起来看下网卡驱动是如何使用描述符以及管理描述符环形队列的。 报文接收描述符以及描述符环形队列是网卡和网卡驱动都会操作的对象，所以网卡和网卡驱动对接收报文描述符的定义也必须保持一致。与网卡相对应的，网卡驱动从软件的角度定义了接收报文描述符，如下： 1234567891011121314151617181920212223242526272829union ixgbe_adv_rx_desc &#123; struct &#123; __le64 pkt_addr; /* Packet buffer address */ __le64 hdr_addr; /* Header buffer address */ &#125; read; struct &#123; struct &#123; union &#123; __le32 data; struct &#123; __le16 pkt_info; /* RSS, Pkt type */ __le16 hdr_info; /* Splithdr, hdrlen */ &#125; hs_rss; &#125; lo_dword; union &#123; __le32 rss; /* RSS Hash */ struct &#123; __le16 ip_id; /* IP id */ __le16 csum; /* Packet Checksum */ &#125; csum_ip; &#125; hi_dword; &#125; lower; struct &#123; __le32 status_error; /* ext status/error */ __le16 length; /* Packet length */ __le16 vlan; /* VLAN tag */ &#125; upper; &#125; wb; /* writeback */&#125;; 报文接收描述符环形队列是用做网络报文接收的，而在网卡中接收报文的最小单位是一个队列，即RX队列。所以一般来说就是一个RX队列对应一个报文接收描述符环形队列。 从ixgbe驱动的实现可以知道，ixgbe使用一个叫做中断向量的对象来管理队列，其定义如下： 12345678910111213141516171819202122232425struct ixgbe_q_vector &#123; struct ixgbe_adapter *adapter;#ifdef CONFIG_IXGBE_DCA int cpu; /* CPU for DCA */#endif u16 v_idx; /* index of q_vector within array, also used for * finding the bit in EICR and friends that * represents the vector for this ring */ u16 itr; /* Interrupt throttle rate written to EITR */ /* 分别以链表方式管理中断向量中的rx和tx队列 */ struct ixgbe_ring_container rx, tx; struct napi_struct napi; cpumask_t affinity_mask; int numa_node; struct rcu_head rcu; /* to avoid race with update stats on free */ char name[IFNAMSIZ + 9];#ifdef CONFIG_NET_RX_BUSY_POLL atomic_t state;#endif /* CONFIG_NET_RX_BUSY_POLL */ /* for dynamic allocation of rings associated with this q_vector */ struct ixgbe_ring ring[0] ____cacheline_internodealigned_in_smp;&#125;; 在上面的定义中，struct ixgbe_q_vector对象最后一个类型为struct ixgbe_ring的柔性数组成员就是由该中断向量所管理的队列，这里包括了RX队列和TX队列。报文接收流程只需要关注其中的RX队列即可。一般来说一个中断向量会关联一个硬件中断。当网卡往中断向量中的某个RX队列的描述符中写入报文信息时，就会触发对应的硬件中断，然后中断子系统就会调用我们注册的中断处理函数来处理这个中断，在ixgbe驱动中对应的就是ixgbe_intr()（在msi-x中断模式下对应的是ixgbe_msix_clean_rings()）。这里需要做一个说明，就是在legacy或者msi中断模式下，只会使用一个中断向量，对应的使用一个中断号；而在msi-x中断模式下，可能会有多个中断向量，对应的会有多个中断号，一般来说会把一个中断向量对应的中断号进行绑核处理，这样可以提高报文处理效率。而具体到某一个RX队列是如何同一个中断号进行关联的，这里还涉及到另外一个网卡寄存器，即Interrupt Vector Alloction（IVAR），这里不再详细介绍，可以参考ixgbe驱动的ixgbe_configure_msi_and_legacy()和ixgbe_configure_msix()函数，以及网卡中断部分的配置。 在ixgbe网卡驱动的实现中，我们可以看到驱动是以一个叫做struct ixgbe_ring的对象来管理报文描述符环形队列(不管是接收还是发送)，其定义如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859struct ixgbe_ring &#123; struct ixgbe_ring *next; /* pointer to next ring in q_vector */ struct ixgbe_q_vector *q_vector; /* backpointer to host q_vector */ struct net_device *netdev; /* netdev ring belongs to */ struct device *dev; /* device for DMA mapping */ struct ixgbe_fwd_adapter *l2_accel_priv; /* 环形队列缓冲区中的报文描述符数组 */ void *desc; /* descriptor ring memory */ /* 与报文描述符数组一一对应的报文缓冲区对象 */ union &#123; struct ixgbe_tx_buffer *tx_buffer_info; struct ixgbe_rx_buffer *rx_buffer_info; &#125;; unsigned long state; u8 __iomem *tail; /* 指向RDT寄存器对应的内核虚拟地址 */ /* 报文描述符数组对应的物理地址 */ dma_addr_t dma; /* phys. address of descriptor ring */ unsigned int size; /* length in bytes */ /* 环形队列缓冲区中的报文描述符个数 */ u16 count; /* amount of descriptors */ /* * 环形队列缓冲区关联的rx队列索引，这个索引是用来在adapter-&gt;rx数组索引环形队列缓冲区的 */ u8 queue_index; /* needed for multiqueue queue management */ u8 reg_idx; /* holds the special value that gets * the hardware register offset * associated with this ring, which is * different for DCB and RSS modes */ /* * next_to_use是环形队列缓冲区中将要提供给硬件使用的第一个报文描述符的索引，对应的就是RDT寄存器 * next_to_clean是环形队列缓冲区中驱动将要处理的第一个报文描述符的索引 */ u16 next_to_use; u16 next_to_clean; unsigned long last_rx_timestamp; union &#123; u16 next_to_alloc; struct &#123; u8 atr_sample_rate; u8 atr_count; &#125;; &#125;; u8 dcb_tc; struct ixgbe_queue_stats stats; struct u64_stats_sync syncp; union &#123; struct ixgbe_tx_queue_stats tx_stats; struct ixgbe_rx_queue_stats rx_stats; &#125;;&#125; ____cacheline_internodealigned_in_smp; struct ixgbe_ring对象中最重要的几个成员都已经做了注解，其中的desc成员就是报文描述符队列，从这里的实现也可以看出，报文描述符队列实际上是线性的，其逻辑上的环形操作是通过struct ixgbe_ring对象中的成员，如next_to_clean、next_to_alloc和next_to_use等来实现的。另外，struct ixgbe_ring对象中还有一个类型为dma_addr_t的dma成员，该成员就是desc成员对应的物理地址，有desc成员的内核虚拟地址进行一致性dma映射得到。这样ixgbe驱动可以通过desc来操作描述符环形队列，而网卡可以通过dma成员来操作描述符环形队列。 下面一起来看下ixgbe驱动是如何建立一个描述符环形队列管理对象的。其实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546int ixgbe_setup_rx_resources(struct ixgbe_ring *rx_ring)&#123; struct device *dev = rx_ring-&gt;dev; int orig_node = dev_to_node(dev); int ring_node = -1; int size; size = sizeof(struct ixgbe_rx_buffer) * rx_ring-&gt;count; if (rx_ring-&gt;q_vector) ring_node = rx_ring-&gt;q_vector-&gt;numa_node; rx_ring-&gt;rx_buffer_info = vzalloc_node(size, ring_node); if (!rx_ring-&gt;rx_buffer_info) rx_ring-&gt;rx_buffer_info = vzalloc(size); if (!rx_ring-&gt;rx_buffer_info) goto err; u64_stats_init(&amp;rx_ring-&gt;syncp); /* Round up to nearest 4K */ rx_ring-&gt;size = rx_ring-&gt;count * sizeof(union ixgbe_adv_rx_desc); rx_ring-&gt;size = ALIGN(rx_ring-&gt;size, 4096); set_dev_node(dev, ring_node); rx_ring-&gt;desc = dma_alloc_coherent(dev, rx_ring-&gt;size, &amp;rx_ring-&gt;dma, GFP_KERNEL); set_dev_node(dev, orig_node); if (!rx_ring-&gt;desc) rx_ring-&gt;desc = dma_alloc_coherent(dev, rx_ring-&gt;size, &amp;rx_ring-&gt;dma, GFP_KERNEL); if (!rx_ring-&gt;desc) goto err; rx_ring-&gt;next_to_clean = 0; rx_ring-&gt;next_to_use = 0; return 0;err: vfree(rx_ring-&gt;rx_buffer_info); rx_ring-&gt;rx_buffer_info = NULL; dev_err(dev, "Unable to allocate memory for the Rx descriptor ring\n"); return -ENOMEM;&#125; 函数ixgbe_setup_rx_resources()处理流程很清晰： 1）、根据之前配置好的环形队列中报文接收描述符个数申请报文描述符数组所需要的内存，以及对应的用来管理报文缓冲区地址信息的缓冲区对象，这个时候缓冲区对象中用来存放报文内容的地址仍然是无效的，因为还没有申请内存，在函数ixgbe_alloc_rx_buffers()处理完成之后，缓冲区对象中存放报文内容的地址就是有效的，可以提供给网卡用来存放报文数据。此外，对报文接收描述符数组内存进行一致性dma映射，获取对应的物理地址，网卡需要使用物理地址，而不是虚拟地址。 2）、初始化描述符环形队列操作所涉及到的索引成员，包括next_to_use和next_to_clean。 经过ixgbe_setup_rx_resources()函数的处理，就已经成功创建了一个描述符环形的管理对象。接下来就需要告诉网卡这个描述符环形队列的信息，这个就是函数ixgbe_configure_rx_ring()所要做的事情了，其实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061void ixgbe_configure_rx_ring(struct ixgbe_adapter *adapter, struct ixgbe_ring *ring)&#123; struct ixgbe_hw *hw = &amp;adapter-&gt;hw; /* 环形队列缓冲区中报文描述符数组对应的物理地址 */ u64 rdba = ring-&gt;dma; u32 rxdctl; u8 reg_idx = ring-&gt;reg_idx; /* disable queue to avoid issues while updating state */ rxdctl = IXGBE_READ_REG(hw, IXGBE_RXDCTL(reg_idx)); ixgbe_disable_rx_queue(adapter, ring); /* * 将报文描述符数组的首地址写入到RDBAH和RDBAL寄存器中，并将描述符数组的长度 * 写入到RDLEN寄存器中，这样网卡芯片就知道了报文描述符的信息，后续可以收到 * 合适的网络报文后，就会将报文存放到描述符里面的dma地址中，并递增内部的 * head寄存器值 */ IXGBE_WRITE_REG(hw, IXGBE_RDBAL(reg_idx), (rdba &amp; DMA_BIT_MASK(32))); IXGBE_WRITE_REG(hw, IXGBE_RDBAH(reg_idx), (rdba &gt;&gt; 32)); IXGBE_WRITE_REG(hw, IXGBE_RDLEN(reg_idx), ring-&gt;count * sizeof(union ixgbe_adv_rx_desc)); /* Force flushing of IXGBE_RDLEN to prevent MDD */ IXGBE_WRITE_FLUSH(hw); /* * 初始状态下，网卡芯片的head和tail指针都为0，表示网卡没有可用的报文描述符 * 等后面驱动申请了n个报文描述符中的dma地址后，就会将tail寄存器值设置为n， * 表示目前网卡可用的报文描述符数量为n个。这样，等网卡收到了合适的报文之后 * 就会存到报文描述符中的dma地址处。 */ IXGBE_WRITE_REG(hw, IXGBE_RDH(reg_idx), 0); IXGBE_WRITE_REG(hw, IXGBE_RDT(reg_idx), 0); ring-&gt;tail = adapter-&gt;io_addr + IXGBE_RDT(reg_idx); ixgbe_configure_srrctl(adapter, ring); ixgbe_configure_rscctl(adapter, ring); if (hw-&gt;mac.type == ixgbe_mac_82598EB) &#123; /* * enable cache line friendly hardware writes: * PTHRESH=32 descriptors (half the internal cache), * this also removes ugly rx_no_buffer_count increment * HTHRESH=4 descriptors (to minimize latency on fetch) * WTHRESH=8 burst writeback up to two cache lines */ rxdctl &amp;= ~0x3FFFFF; rxdctl |= 0x080420; &#125; /* enable receive descriptor ring */ rxdctl |= IXGBE_RXDCTL_ENABLE; IXGBE_WRITE_REG(hw, IXGBE_RXDCTL(reg_idx), rxdctl); ixgbe_rx_desc_queue_enable(adapter, ring); /* 申请报文描述符中用于存储报文数据的内存 */ ixgbe_alloc_rx_buffers(ring, ixgbe_desc_unused(ring));&#125; 从该函数的实现就可以看到，网卡驱动就是通过将接收报文描述符数组对应的物理地址写入到RDBA寄存器，并初始化RDH和RDT寄存器。通过写RDBA、RDH和RDT寄存器，网卡就知道了当前的描述符环形队列的信息。接着调用函数ixgbe_alloc_rx_buffers()申请用来存放报文数据的内存，并将对应的物理地址保存到接收描述符中，然后设置RDT寄存器，这样网卡就可以使用RDH和RDT之间的描述符进行接收报文处理了，ixgbe_alloc_rx_buffers()函数的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103void ixgbe_alloc_rx_buffers(struct ixgbe_ring *rx_ring, u16 cleaned_count)&#123; union ixgbe_adv_rx_desc *rx_desc; struct ixgbe_rx_buffer *bi; u16 i = rx_ring-&gt;next_to_use; /* nothing to do */ if (!cleaned_count) return; /* * 获取下一个将要提供给硬件使用的报文描述符(对应的索引为rx_ring-&gt;next_to_use)， * 以及报文描述符对应的缓冲区对象，缓冲区对象中保存了用于存放报文数据的内存地址信息， * 当然用于存放报文的内存对应的物理地址也会保存到报文描述符中。 */ rx_desc = IXGBE_RX_DESC(rx_ring, i); bi = &amp;rx_ring-&gt;rx_buffer_info[i]; /* * 这个地方执行这个计算的目的是什么呢?我们知道报文描述符队列在逻辑上是环形的( * 实际上是线性的，因为内存地址是线性分布的)，当我们操作这个队列到达末尾的时候， * 通过将索引重新指向队列开头来实现环形操作。所以呢，在计算之后，i表示的就是 * 目前位置距离队列末尾之间还没有提供给硬件使用的报文描述符个数的相反数，也就是 * 当前处理位置和队列末尾距离。 * 在下面的循环中，每处理一个报文描述符(申请用于存放报文数据的内存)都会将i递增， * 当i等于0的时候，说明达到了队列的末尾，下次处理就要从队列头开始了，从而实现 * 队列的环形操作。 */ i -= rx_ring-&gt;count; do &#123; /* * 申请用于存放报文数据的内存，并进行dma流式映射 */ if (!ixgbe_alloc_mapped_page(rx_ring, bi)) break; /* * Refresh the desc even if buffer_addrs didn't change * because each write-back erases this info. */ /* rx_desc-&gt;read.pkt_addr存放的地址就是用于存放报文的dma起始地址 */ rx_desc-&gt;read.pkt_addr = cpu_to_le64(bi-&gt;dma + bi-&gt;page_offset); /* rx_desc和bi递增，指向下一个描述符和对应的缓冲区对象 */ rx_desc++; bi++; i++; /* * 如果i == 0，说明操作环形队列缓冲区已经转了一圈了，这个时候就需要重新让 * rx_desc和bi分别指向描述符数组和缓冲区数组的起始位置，从头开始处理，当然 * 对应的i值也就要重新计算了，此时的值为队列中描述符个数的相反数。 */ if (unlikely(!i)) &#123; /* * 考虑下为什么描述符环形队列中已经被网卡使用过的描述符中存放报文内容的 * 内存需要重新申请并进行流式dma映射呢?我们知道，一个描述符中用来存放 * 报文的内存(实际上是一个页)，接收完报文后如果空间足够，有可能被其他描述符 * 重用，或者报文较大而产生分片，这个时候并不会从描述符中存放报文的内存中 * 将报文数据拷贝到skb-&gt;data中，而是将描述符中存放报文的页内存挂载到 * skb_shinfo(skb)-&gt;frags数组中，无论前面的哪种情况，本描述符中用于 * 存放报文数据的内存在本描述符用于接收报文之后都不能再被该描述符继续使用了， * 所需每次都需要重新申请内存，或者重用之前的报文描述符的页内存。这也是为什么 * 在函数ixgbe_fetch_rx_buffer()末尾会将rx_buffer-&gt;page置空的原因。 */ rx_desc = IXGBE_RX_DESC(rx_ring, 0); bi = rx_ring-&gt;rx_buffer_info; i -= rx_ring-&gt;count; &#125; /* clear the status bits for the next_to_use descriptor */ rx_desc-&gt;wb.upper.status_error = 0; cleaned_count--; &#125; while (cleaned_count); /* * i加上rx_ring-&gt;count之后指向的就是最后一个可用(对网卡芯片来说)的报文描述符的 * 下一个位置,，这个时候需要将这个索引值i写入到网卡芯片的tail寄存器中，让网卡 * 芯片知道目前可用的报文描述数量(tail - head) */ i += rx_ring-&gt;count; if (rx_ring-&gt;next_to_use != i) &#123; /* * 因为i指向的是最后一个可用报文描述符的下一个位置，这个位置也是下一次要 * 提供给网卡芯片使用的报文描述符的位置 */ rx_ring-&gt;next_to_use = i; /* update next to alloc since we have filled the ring */ rx_ring-&gt;next_to_alloc = i; /* Force memory writes to complete before letting h/w * know there are new descriptors to fetch. (Only * applicable for weak-ordered memory model archs, * such as IA-64). */ wmb(); /* 将i值写入到tail寄存器中 */ writel(i, rx_ring-&gt;tail); &#125;&#125; 补充说明：RDT寄存器由网卡驱动在提供报文接收描述符给网卡之后更新，而RDH寄存器由网卡在回写一个报文接收描述符给驱动之后更新。 ixgbe网卡驱动napi接口的处理 NAPI是Linux中综合了中断和轮询方式的网卡数据处理API。下面描述下ixgbe中是如何使用NAPI方式来进行收包处理的。 NAPI对象 在Linux中，NAPI接口提供了一个NAPI对象，这个是设备使用NAPI接口进行数据包处理的必要条件，先来看下其定义： 12345678910111213141516171819202122232425struct napi_struct &#123; /* The poll_list must only be managed by the entity which * changes the state of the NAPI_STATE_SCHED bit. This means * whoever atomically sets that bit can add this napi_struct * to the per-CPU poll_list, and whoever clears that bit * can remove from the list right before clearing the bit. */ struct list_head poll_list; unsigned long state; int weight; unsigned int gro_count; int (*poll)(struct napi_struct *, int);#ifdef CONFIG_NETPOLL spinlock_t poll_lock; int poll_owner;#endif struct net_device *dev; struct sk_buff *gro_list; struct sk_buff *skb; struct hrtimer timer; struct list_head dev_list; struct hlist_node napi_hash_node; unsigned int napi_id;&#125;; 一般来说，如果某个设备要使用NAPI接口进行数据包处理，那么该设备会在自己的设备对象中定义一个struct napi_struct类型的对象成员。在第二部分讲到过，ixgbe驱动中每个中断向量会关联一个中断号，从而在硬中断处理函数能获取到中断向量，而如果利用NAPI进行数据包处理的话，也就必须要获取到对应的struct napi_struct类型的对象，所以自然而然地ixgbe驱动将struct napi_struct类型的对象定义在了中断向量中。 下面对其中的部分重要成员进行简单的介绍： 1）、 poll_list。用于将本设备加入到cpu私有数据中类型为struct softnet_data的对象的待轮询设备链表中。 2）、state。设备的状态，有如下几种： 1234567enum &#123; NAPI_STATE_SCHED, /* Poll is scheduled */ NAPI_STATE_DISABLE, /* Disable pending */ NAPI_STATE_NPSVC, /* Netpoll - don't dequeue from poll_list */ NAPI_STATE_HASHED, /* In NAPI hash (busy polling possible) */ NAPI_STATE_NO_BUSY_POLL,/* Do not add in napi_hash, no busy polling */&#125;; 3）、weight。设备每次轮询所能处理的包的最大数量。 4）、poll。设备注册的轮询回调，在该回调中一般会遍历设备的所有rx队列，取出报文，送往上层处理。 NAPI初始化 从驱动实现我们知道，ixgbe驱动在中断向量中定义了一个类型为struct napi_struct的NAPI实例。在ixgbe驱动初始化的时候，会在创建中断向量的时候初始化其对应NAPI实例，实现如下： 12345678910111213141516171819202122232425262728293031323334static int ixgbe_alloc_q_vector(struct ixgbe_adapter *adapter, int v_count, int v_idx, int txr_count, int txr_idx, int rxr_count, int rxr_idx)&#123; struct ixgbe_q_vector *q_vector; struct ixgbe_ring *ring; int node = NUMA_NO_NODE; int cpu = -1; int ring_count, size; u8 tcs = netdev_get_num_tc(adapter-&gt;netdev); /* 计算这个中断向量所需要申请的环形队列缓冲区的总数量 */ ring_count = txr_count + rxr_count; /* 申请中断向量内存以及环形队列缓冲区对应的柔性数组内存。 */ size = sizeof(struct ixgbe_q_vector) + (sizeof(struct ixgbe_ring) * ring_count); …… /* allocate q_vector and rings */ /* numa架构下，在cpu所在的本地内存申请中断向量所需内存 */ q_vector = kzalloc_node(size, GFP_KERNEL, node); if (!q_vector) q_vector = kzalloc(size, GFP_KERNEL); if (!q_vector) return -ENOMEM; /* initialize NAPI */ /* 初始化napi收包方式 */ netif_napi_add(adapter-&gt;netdev, &amp;q_vector-&gt;napi, ixgbe_poll, 64); …… return 0;&#125; 从函数ixgbe_alloc_q_vector()调用netif_napi_add()初始化NAPI对象可以看到，ixgbe驱动注册的poll回调钩子是ixgbe_poll()，而每次轮询最大可处理的数据包为64个。 NAPI调度 在ixgbe驱动中因为使用了NAPI接口进行数据包处理，所以对应的上半部实现就变成了当硬中断触发后，在硬中断处理函数中调用NAPI的调度接口napi_schedule_irqoff()将设备加入到cpu私有数据中类型为struct softnet_data的对象的待轮询设备链表中，并触发软中断。以msi-x中断模式为例，其对应的具体实现如下： 1234567891011static irqreturn_t ixgbe_msix_clean_rings(int irq, void *data)&#123; struct ixgbe_q_vector *q_vector = data; /* EIAM disabled interrupts (on this vector) for us */ if (q_vector-&gt;rx.ring || q_vector-&gt;tx.ring) napi_schedule_irqoff(&amp;q_vector-&gt;napi); return IRQ_HANDLED;&#125; 而下半部的处理就是在网络子系统的软中断处理函数net_rx_action()中遍历cpu私有数据中类型为struct softnet_data的对象中的待轮询设备链表，依次调用每个设备注册的poll回调钩子进行报文接收处理，其对应的具体实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546static __latent_entropy void net_rx_action(struct softirq_action *h)&#123; struct softnet_data *sd = this_cpu_ptr(&amp;softnet_data); unsigned long time_limit = jiffies + 2; int budget = netdev_budget; LIST_HEAD(list); LIST_HEAD(repoll); local_irq_disable(); list_splice_init(&amp;sd-&gt;poll_list, &amp;list); local_irq_enable(); for (;;) &#123; struct napi_struct *n; if (list_empty(&amp;list)) &#123; if (!sd_has_rps_ipi_waiting(sd) &amp;&amp; list_empty(&amp;repoll)) return; break; &#125; n = list_first_entry(&amp;list, struct napi_struct, poll_list); budget -= napi_poll(n, &amp;repoll); /* If softirq window is exhausted then punt. * Allow this to run for 2 jiffies since which will allow * an average latency of 1.5/HZ. */ if (unlikely(budget &lt;= 0 || time_after_eq(jiffies, time_limit))) &#123; sd-&gt;time_squeeze++; break; &#125; &#125; __kfree_skb_flush(); local_irq_disable(); list_splice_tail_init(&amp;sd-&gt;poll_list, &amp;list); list_splice_tail(&amp;repoll, &amp;list); list_splice(&amp;list, &amp;sd-&gt;poll_list); if (!list_empty(&amp;sd-&gt;poll_list)) __raise_softirq_irqoff(NET_RX_SOFTIRQ); net_rps_action_and_irq_enable(sd);&#125; 上面说到过，在下半部的软中断处理函数中会调用设备注册的回调函数poll进行收包处理，而ixgbe驱动中对应的轮询回调函数就是ixgbe_poll()。在这个函数中会遍历NAPI对象关联的中断向量中的所有RX队列，将收到的每一个报文通过调用函数__netif_receive_skb()送往上层协议栈进行处理，具体处理细节可以参考驱动实现。 通过上面对ixgbe驱动中使用NAPI接口的描述，我们可以总结出NAPI接口的数据包接收流程如下： 注：上面的流程图中NAPI假设上层会关闭和打开的硬中断 参考：1、http://www.intel.com/content/www/us/en/embedded/products/networking/82599-10-gbe-controller-datasheet.html]]></content>
      <categories>
        <category>eBPF</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>eBPF</tag>
        <tag>网卡驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eBPF中的map]]></title>
    <url>%2F2017%2F03%2F26%2Febpf-map%2F</url>
    <content type="text"><![CDATA[map是一个通用的key-value存储结构,可以用来存储任意类型的数据。map在eBPF中扮演这十分重要的角色，这个文档将会介绍eBPF目前支持的map类型，以及某一类型map的实现细节。 在eBPF中可以利用map在eBPF程序调用之间保存状态信息，也可以利用map在用户态程序和内核之间共享数据等。正如文档开头所说，map是通用的key-value存储结构，因为在实现的时候是key和value都是当作二进制对象来存储的，所以可以用来存放任意类型的key和value数据。当然，这需要用户在map创建的时候指定key和value对应的类型大小，这样，内核在创建的时候就能根据key和value的大小来申请合适的内存用以存储数据。 内核提供了一个系统调用bpf()，以让用户态程序可以根据使用场景来创建合适的map。这个系统调用会返回一个关联了这个map对象的文件描述符，后续用户态程序可以用这个文件描述符来对相应的map对象进行一些操作，如查询、更新和删除，这部分的接口在tools/lib/bpf/bpf.h中定义了。关于这个bpf()系统调用以及map操作接口的详细信息，可以参考相关资料，其中bpf()系统调用相关的信息可以在man page中找到，而map操作相关的接口可以在tools/lib/bpf/bpf.h中看到具体的实现。 eBPF支持多种不同类型的map，所以用户在创建map的时候，需要根据应用场景指定一个合适的类型。目前版本支持的map类型定义在文件include/uapi/linux/bpf.h中，如下： 1234567891011enum bpf_map_type &#123; BPF_MAP_TYPE_UNSPEC, BPF_MAP_TYPE_HASH, BPF_MAP_TYPE_ARRAY, BPF_MAP_TYPE_PROG_ARRAY, BPF_MAP_TYPE_PERF_EVENT_ARRAY, BPF_MAP_TYPE_PERCPU_HASH, BPF_MAP_TYPE_PERCPU_ARRAY, BPF_MAP_TYPE_STACK_TRACE, BPF_MAP_TYPE_CGROUP_ARRAY,&#125;; 从类型定义的名字中我们大概可以知道这些map所对应的使用场景，如类型BPF_MAP_TYPE_PROG_ARRAY，通用用来存放一个关联了eBPF程序的文件描述符，这样就可以实现在多个eBPF程序之间跳转；而对于BPF_MAP_TYPE_HASH和BPF_MAP_TYPE_ARRAY而言，他们所存储的内容就没有特殊的用途，可以用来存放任意类型的key和value信息。 虽然目前eBPF支持多种map类型，但是从类型名字中可以看出，map类型可以分为hash和array两大类，也就对应了两种不同的实现方式。而每一类中不同map类型的实现又有共同的部分，所以懂得了每一大类map共有部分的实现，就对大类内其他类型的map实现有了整体的了解，然后再结合各自应用场景，就可以比较好地掌握适用于不同场景的map的实现。下面会详细介绍hash这一类map的基础实现，也就是BPF_MAP_TYPE_HASH类型的map在内核中的实现。 先来看下内核中map对象的定义： 12345678910111213struct bpf_map &#123; atomic_t refcnt; enum bpf_map_type map_type; u32 key_size; u32 value_size; u32 max_entries; u32 map_flags; u32 pages; struct user_struct *user; const struct bpf_map_ops *ops; struct work_struct work; atomic_t usercnt;&#125;; 在这个对象结构体中，key_size和value_size分别用于指定map存储的键值大小，而max_entries则指定这个map对象中可以存储的键值对数量，在实现map内部存储的时候会用到这些信息。 对于map对象中的ops成员，存储的是map对象所支持的操作，如查询、删除和更新等，其定义如下： 1234567891011struct bpf_map_ops &#123; struct bpf_map *(*map_alloc)(union bpf_attr *attr); void (*map_release)(struct bpf_map *map, struct file *map_file); void (*map_free)(struct bpf_map *map); int (*map_get_next_key)(struct bpf_map *map, void *key, void *next_key); void *(*map_lookup_elem)(struct bpf_map *map, void *key); int (*map_update_elem)(struct bpf_map *map, void *key, void *value, u64 flags); int (*map_delete_elem)(struct bpf_map *map, void *key); void *(*map_fd_get_ptr)(struct bpf_map *map, struct file *map_file, int fd); void (*map_fd_put_ptr)(void *ptr);&#125;; 并不是所有类型的map都需要实现全部的操作，因为有些操作是特定于map类型的。而这些操作是和内部存储结构实现密切相关的，同一种操作虽然对外提供的操作接口相同，但是对于不同的存储结构其实现方式也是不一样的。struct bpf_map对象对所有enum bpf_map_type中定义的map类型都适用。 BPF_MAP_TYPE_HASH类型的map，顾名思义，就是内部存储结构采用hash表实现的map。下面会讲述hash表的定义，以及基于hash表实现的map所提供的操作的实现。 接下来看下hash对象的定义，对象中每个字段的含义都进行了注释： 123456789101112131415struct bpf_htab &#123; struct bpf_map map; /* 记录了hash table的所对应的map信息*/ struct bucket *buckets; /* 指向hash表中的所有桶组成的数组*/ void *elems; /* 指向hash entries组成的数组首地址 */ /* 所有的hash entry都会挂载在percpu类型的链表上 */ struct pcpu_freelist freelist; /* * 当map type不是BPF_MAP_TYPE_PERCPU_HASH时，用来存储在每个 * cpu上申请的一个额外的hash entry */ void __percpu *extra_elems; atomic_t count; /* number of elements in this hashtable */ u32 n_buckets; /* number of hash buckets */ u32 elem_size; /* size of each element in bytes */&#125;; struct bpf_htab对象中的elems成员指向存放hash表中所有元素的内存。hash表中元素的定义如下： 12345678910111213struct htab_elem &#123; union &#123; struct hlist_node hash_node; struct bpf_htab *htab; struct pcpu_freelist_node fnode; &#125;; union &#123; struct rcu_head rcu; enum extra_elem_state state; &#125;; u32 hash; /* hash element对应的hash值 */ char key[0] __aligned(8); /* key + value组成的柔性数组 */&#125;; struct htab_elem对象中的hash成员即为由key值进行hash之后得到的，而key成员这个柔性数组存放的就是“key-value”信息，value值紧跟在key值之后。这个柔性数组的类型是char，这也正好应证了文档开头所说的在map内部key和value都是当作二进制数据来存储的，而这正也是map实现通用性存储的地方。 struct bpf_htab对象中的freelist这个成员是用来管理hash表中的所有的元素的，即所有的元素会均分到这个percpu类型的链表中，上面struct htab_elem定义中第一个联合体成员fnode正是使用freelist链表来管理元素的关键，这部分的执行流程可以参考文件kernel/bpf/hashtab.c中的函数prealloc_elems_and_freelist()。 在hash表的每个桶(bucket)中，使用链表来管理落在这个桶中的元素，所以bucket对象的定义如下： 1234struct bucket &#123; struct hlist_head head; /* 管理桶中所有元素的链表 */ raw_spinlock_t lock;&#125;; 在文档前面说过，map一个通用的key-value存储结构，所以需要支持一些操作，如插入数据、删除数据、更新数据以及查询数据等等。因此，实现一种map，除了定义其内部数据存储方式之外，还需要依据内部的数据存储方式实现其所支持的一些操作。对于struct bpf_map_ops对象中定义的操作，BPF_MAP_TYPE_HASH类型的map根据需要实现了其中的部分操作，如下： 12345678static const struct bpf_map_ops htab_ops = &#123; .map_alloc = htab_map_alloc, .map_free = htab_map_free, .map_get_next_key = htab_map_get_next_key, .map_lookup_elem = htab_map_lookup_elem, .map_update_elem = htab_map_update_elem, .map_delete_elem = htab_map_delete_elem,&#125;; 对于htab_ops中实现的部分操作，其具体实现可以参考文件kernel/bpf/hashtab.c，直接从代码里面看其实现，因为有上下文参考，所以更具有连贯性。]]></content>
      <categories>
        <category>eBPF</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>eBPF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DPDK中的无锁环形缓冲区]]></title>
    <url>%2F2016%2F11%2F20%2Fdpdk-ring-buffer%2F</url>
    <content type="text"><![CDATA[DPDK中实现了无锁环形缓冲区，支持单生产者或者多生产者入队列，单消费者或多消费者出队列。下面会记录dpdk中是如何管理所有使用的无锁环形缓冲区以及无锁环形缓冲区中支持的一些操作。 无锁环形缓冲区的组织 DPDK中定义的无锁环形缓冲区对象如下： 1234567891011121314151617181920212223242526272829303132333435363738struct rte_ring &#123; char name[RTE_MEMZONE_NAMESIZE]; /**&lt; Name of the ring. */ int flags; /**&lt; Flags supplied at creation. */ const struct rte_memzone *memzone; /**&lt; Memzone, if any, containing the rte_ring */ /** Ring producer status. */ struct prod &#123; uint32_t watermark; /**&lt; Maximum items before EDQUOT. */ uint32_t sp_enqueue; /**&lt; True, if single producer. */ uint32_t size; /**&lt; Size of ring. */ uint32_t mask; /**&lt; Mask (size-1) of ring. */ volatile uint32_t head; /**&lt; Producer head. */ volatile uint32_t tail; /**&lt; Producer tail. */ &#125; prod __rte_cache_aligned; /** Ring consumer status. */ struct cons &#123; uint32_t sc_dequeue; /**&lt; True, if single consumer. */ uint32_t size; /**&lt; Size of the ring. */ uint32_t mask; /**&lt; Mask (size-1) of ring. */ volatile uint32_t head; /**&lt; Consumer head. */ volatile uint32_t tail; /**&lt; Consumer tail. */#ifdef RTE_RING_SPLIT_PROD_CONS &#125; cons __rte_cache_aligned;#else &#125; cons;#endif#ifdef RTE_LIBRTE_RING_DEBUG struct rte_ring_debug_stats stats[RTE_MAX_LCORE];#endif void * ring[0] __rte_cache_aligned; /**&lt; Memory space of ring starts here. * not volatile so need to be careful * about compiler re-ordering */&#125;; 从上面的定义可以看出，无锁环形缓冲区对象中定义了一个生产者对象和一个消费者对象，对应的也就是缓冲区的写对象和读对象。另外，也可以看出无锁环形缓冲区在内存中的组织形式是前面是无锁环形缓冲区对象本身，然后紧接着就是实际用于存储内容的环形队列，在某一时刻其内存布局如下图所示： 无锁环形缓冲区是一种通用的数据结构，所以可能会在多个地方使用，在dpdk中就会有多种情况会使用，比如内存池。所以随之而来的一个疑问就是dpdk是如何管理其所使用的所有的无锁环形缓冲区的呢？从它的源码实现(rte_ring.c/rte_ring.h)中我们可以找到答案，与此相关的部分源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455TAILQ_HEAD(rte_ring_list, rte_tailq_entry);static struct rte_tailq_elem rte_ring_tailq = &#123; .name = RTE_TAILQ_RING_NAME,&#125;;EAL_REGISTER_TAILQ(rte_ring_tailq)struct rte_ring *rte_ring_create(const char *name, unsigned count, int socket_id, unsigned flags)&#123; char mz_name[RTE_MEMZONE_NAMESIZE]; struct rte_ring *r; struct rte_tailq_entry *te; const struct rte_memzone *mz; ssize_t ring_size; int mz_flags = 0; struct rte_ring_list* ring_list = NULL; int ret; ring_list = RTE_TAILQ_CAST(rte_ring_tailq.head, rte_ring_list); /* get the size of memory occupied by ring */ ring_size = rte_ring_get_memsize(count); …… te = rte_zmalloc("RING_TAILQ_ENTRY", sizeof(*te), 0); if (te == NULL) &#123; RTE_LOG(ERR, RING, "Cannot reserve memory for tailq\n"); rte_errno = ENOMEM; return NULL; &#125; rte_rwlock_write_lock(RTE_EAL_TAILQ_RWLOCK); mz = rte_memzone_reserve(mz_name, ring_size, socket_id, mz_flags); if (mz != NULL) &#123; r = mz-&gt;addr; /* no need to check return value here, we already checked * the arguments above */ rte_ring_init(r, name, count, flags); /* 低维尾队列的entry存放着rte_ring的管理对象地址 */ te-&gt;data = (void *) r; r-&gt;memzone = mz; /* 将存放着环形缓冲区对象的尾队列entry插入到低维尾队列的末端 */ TAILQ_INSERT_TAIL(ring_list, te, next); &#125; else &#123; r = NULL; RTE_LOG(ERR, RING, "Cannot reserve memory\n"); rte_free(te); &#125; rte_rwlock_write_unlock(RTE_EAL_TAILQ_RWLOCK); return r;&#125; 从上面的源码我们可以知道，dpdk是用尾队列来管理其所使用的所有无锁环形缓冲区的，也就是说一个尾队列中的元素就是一个无锁环形缓冲区对象。那用来管理所有无锁环形缓冲区的尾队列，dpdk又是如何管理的呢？在函数rte_ring_create()中可以看到管理着无锁环形缓冲区的尾队列头部是存放在一个类型为struct rte_tailq_elem的全局变量rte_ring_tailq的head成员中，其中struct rte_tailq_elem定义如下： 12345struct rte_tailq_elem &#123; struct rte_tailq_head *head; TAILQ_ENTRY(rte_tailq_elem) next; const char name[RTE_TAILQ_NAMESIZE];&#125;; 从struct rte_tailq_elem的定义可以看到，管理着无锁环形缓冲区尾队列的头部是另外一个尾队列的一个元素，类似的管理方式还用在了dpdk的内存池等数据结构中，所以在dpdk中采用了两级尾队列来管理所使用的数据结构。其实这样说也不完整，因为除了用二级尾队列来管理所使用的数据结构之外，dpdk还用了一个全局共享内存中的列表数组rte_config.mem_config-&gt;tailq_head[RTE_MAX_TAILQ]来分别存储这个二级尾队列中低维尾队列存放的元素，即管理某一种特定数据结构的尾队列头部。 无锁环形缓冲区的操作 记录完了dpdk中对无锁环形缓冲区的管理方式，接下来对dpdk中实现的无锁环形缓冲区所支持的操作做一个记录。从一开始说过无锁环形缓冲区支持单生产者或者多生产者入队列，单消费者或多消费者出队列等操作。其实从另外一个角度还可以说dpdk中的无锁环形缓冲区中支持两种出入队列的模式，即出入队列元素数目固定模式和尽力而为模式，出入队列元素数目固定模式就是说只有进入队列的元素数目达到指定数目才算操着成功，否则失败；而出入队列元素数目尽力而为模式就是说对于指定的数目，如果当时队列状态并不能满足，则以当时队列状态为准，尽可能满足指定的数目，举个例子，如果参数指定需要入队列3个元素，但队列中只剩下2个空闲空间，那么就将其中2个元素入队列，出队列情况同理。 下面以两个核同时往队列各写入一个元素来介绍无锁环形缓冲区支持的多生产者入队列功能，其他的操作方式都可以从这里推演出来。在代码中多生产者入队列相关的函数为__rte_ring_mp_do_enqueue()，下面的流程也是根据这个函数整理出来的。 1） 初始状态下生产者的头和尾指向了同一个位置。如下图所示： 2） 第一步，在两个核上，将r->prod.head和r->prod.tail分别拷贝到本地的临时变量prod_head和prod_tail中，然后将本地临时变量prod_next指向队列的下一个空闲位置。检查队列中是否有足够的空间，如果没有，则返回失败（如果是写入多个元素，没有足够剩余空间的话，则需要看指定的模式，如果是尽力而为模式，则尽可能往队列中写入元素；否则返回失败）。如下图所示： 3） 第二步，使用CAS操作指令将本地变量prod_next的值赋值给r->prod.head，即两者指向同一个位置。CAS操作指令有如下特性： a) 如果r->prod.head不等于本地变量prod_head，则CAS操作失败，代码重新从第一步开始执行。 b) 如果r->prod.head等于本地变量prod_head，则CAS操作成功，代码继续往后执行。 在这里我们假定在核1上操作成功，那么对于核2该操作就会失败，核2会从第一步重新执行。如下图所示： 4） 第三步，核2上的CAS操作成功，核1上成功往环形缓冲区中写入了一个元素（obj4），接着核2也成功往环形缓冲区中写入了一个元素（obj5）。如下图所示： 5） 第四步，在第三步中两个核都已经成功往环形缓冲区写入了一个元素，现在两个核都需要更新r->prod.tail。这里又有一个条件，就是只有r->prod.tail等于本地变量prod_head的核才能去更新r->prod.tail的值。从图中可以看到，目前只有在核1上才能满足这个条件，核2不满足，因此更新r->prod.tail的操作只在核1上进行，核2需要等待核1完成。如下图所示： 6） 第五步，在第四步中，一旦核1完成了更新r->prod.tail的操作，那么核2也能满足更新r->prod.tail的条件，核2此时也会去更新r->prod.tail。如下图所示： ## 无锁环形缓冲区的回绕 DPDK中的无锁环形缓冲区还有另一个特性，那就是充分利用了unsigned类型的回绕特点，这样对于缓冲区中已用空间和剩余空间的计算就得到了极大的简化，也使得生产者头和尾、消费者头和尾的下标值不局限在0和size(ring) – 1之间，只要在0和2^32 - 1范围之内即可。这一点可以参考dpdk的开发者文档，其具体实现也包含在了上节介绍的操作流程当中，感兴趣的可以去看下。 参考文章: 1、http://dpdk.org/doc/guides/prog_guide/]]></content>
      <categories>
        <category>DPDK</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
        <tag>DPDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[客户端非阻塞socket建链流程]]></title>
    <url>%2F2016%2F10%2F23%2Fclient-nonblocking-connect%2F</url>
    <content type="text"><![CDATA[TCP协议是面向连接的、可靠的、基于字节流的传输层协议。那使用tcp协议进行通信的两端是如何进行通信的？使用tcp协议进行通信的两端是通过套接字（scoket）来建立连接的。套接字socket主要有两种类型，阻塞和非阻塞。通常为了防止进程阻塞以及避免cpu被长时间占用，客户端和服务端一般都会采用非阻塞socket进行通信，其中Nginx就是一个典型的例子。下面我们就以Nginx的upstream机制所涉及的与后端服务器建链的流程来总结下使用非阻塞socket的客户端建链流程。 先来看下Nginx的upstream机制所涉及的与后端服务器建链的流程，其相关函数为ngx_event_connect_peer，该函数的执行流程如下： 在建链流程图中调用connect尝试与后端服务器建链那一步，如果connect返回0表明Nginx与后端服务器已经建立了连接，但是如果connect返回-1并且错误码是EINPROGRESS表明后端服务器由于某些原因暂时没有完成连接的建立，后续建立连接后会通知Nginx，此时Nginx的做法是，将连接对应的读写事件加入到了epoll中监控，并将写事件加入到定时器中，因为Nginx与后端服务器建立连接是有时间限制的，所以后续如果超时执行了写事件回调函数，则表示建链因超时而失败了。如果不是因为超时而是连接上有写事件发生调用事件回调函数，则会以SO_ERROR为参数调用setsockopt函数，然后根据错误码来判断Nginx是否与后端服务器建立了连接，这部分见在调用ngx_event_connect_peer函数的ngx_stream_proxy_connect函数中设置的读写事件回调函数ngx_stream_proxy_connect_handler。 从上面Nginx的处理流程中我们可以看到非阻塞socket客户端建链的一般步骤： 1、 调用socket接口获取一个socket描述符。 2、 根据实际情况设置socket的一些属性，如接收缓冲区和发送缓冲区的大小等等。 3、 将socket设置为非阻塞socket。通过调用哪个接口将socket设置为非阻塞的呢？可以以FIONBIO为参数调用ioctl函数将socket设置为非阻塞的，具体ioctl函数的描述和使用可以参见[Linux manual page](http://www.man7.org/linux/man-pages/man2/ioctl.2.html)。 4、 将socket描述符对应的EPOLLIN和EPOLLOUT事件加入到epoll的监控机制中，这样当socket描述符有事件发生时能够及时通知应用程序进行相应事件的处理。 5、 调用connect系统调用与后端服务器建立连接。如果connect函数返回0则表示应用程序与后端服务器建链成功，应用程序就可以执行下一步动作了；如果connect返回的是-1，那么就需要根据系统返回的错误码进一步分析导致调用connect出错的原因。在Linux下，如果错误码是EINPROGRESS表明与后端服务器只是暂时没有完成建链操作，并不代表失败，这个时候就需要设置socket描述符的EPOLLIN和EPOLLOU事件对应的处理函数。待后续socket写事件发生，epoll就会通知应用程序该socket目前可写，然后调用写事件对应的处理函数，判断建链是否成功，如果建链成功，则应用程序就可以执行下一步操作了，如果建链失败，则返回。除了EINPROGRESS之外的错误码都表示Nginx和后端服务器建链失败了。 上面的第五步中对于connect的调用为什么需要做如此细化的处理呢？这个就要看非阻塞socket的特性了，因为对于非阻塞的socket，connect系统调用会立即返回，如果返回0，表明建链应用程序与远端服务器建链成功，如果返回-1且错误码是EINPROGRESS，则会在后台进行三次握手的处理，后续可以通过select、epoll等I/O多路复用机制监控socket描述符的写事件来判断建链成功与否，在connect()函数的[Linux manual page](http://man7.org/linux/man-pages/man2/connect.2.html)中也有类似的描述，如下： > EINPROGRESS > The socket is nonblocking and the connection cannot be completed immediately. > It is possible to select or poll for completion by selecting the socket for writing. > After select indicates writability, use getsockopt to read the SO_ERROR option at level > SOL_SOCKET to determine whether connect() completed successfully (SO_ERROR is zero) > or unsuccessfully (SO_ERROR is one of the usual error codes listed here, explaining > the reason for the failure).]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ngx_stream_limit_conn_module模块解析]]></title>
    <url>%2F2016%2F10%2F16%2Fngx-stream-limit-conn-module%2F</url>
    <content type="text"><![CDATA[ngx_stream_limit_conn_module模块在stream子系统中是用来限制某个ip的并发连接数的。在stream子系统中，虽然没有像http子系统那样在代码中明确地定义处理阶段，但是其处理流程也是按照一定的阶段来划分的，stream处理阶段包括：Post-accept、Pre-access、Access、SSL、Preread、Content、Log。 按照上面的阶段划分，ngx_stream_limit_conn_module模块就是处在Pre-access阶段的，下面便结合代码来分析下该模块的实现。 一、 配置命令解析 ngx_stream_limit_conn_module模块包括三条配置指令，分别是：limit_conn_zone、limit_conn和limit_conn_log_level。其中limit_conn_log_level主要是用来设置日志级别的，这里暂时不讨论。下面的配置文件就是使用ngx_stream_limit_conn_module模块实现限制客户端并发连接数的例子，配置文件如下： 12345678stream &#123; limit_conn_zone $binary_remote_addr zone=addr:10m; ... server &#123; ... limit_conn addr 5; &#125;&#125; Nginx在代码中对limit_conn_zone和limit_conn指令的定义如下： 123456789101112131415161718static ngx_command_t ngx_stream_limit_conn_commands[] = &#123; &#123; ngx_string("limit_conn_zone"), NGX_STREAM_MAIN_CONF|NGX_CONF_TAKE2, ngx_stream_limit_conn_zone, 0, 0, NULL &#125;, &#123; ngx_string("limit_conn"), NGX_STREAM_MAIN_CONF|NGX_STREAM_SRV_CONF|NGX_CONF_TAKE2, ngx_stream_limit_conn, NGX_STREAM_SRV_CONF_OFFSET, 0, NULL &#125;, …… ngx_null_command&#125;; 先来看下limit_conn_zone命令，该命令主要是用来申请一块用于存储客户端ip并发连接数的共享内存。Nginx是如何解析和组织limit_conn_zone指令参数的？其解析函数ngx_stream_limit_conn_zone()的执行流程如下： 从上面的流程中可以看出，一条limit_conn_zone命令在Nginx内部对应一块共享内存，而一块共享内存就会对应一个模块上下文，那这个模块上下文是用来做什么的呢？先来看下它的定义： 123typedef struct &#123; ngx_rbtree_t *rbtree;&#125; ngx_stream_limit_conn_ctx_t; 从上面的定义中可以看到，这个模块上下文其实就是定义了一颗红黑树，从后面的功能实现我们会发现Nginx就是利用这棵红黑树来组织不同客户端ip的并发连接信息的，一个客户端ip对应一个红黑树节点，红黑树的节点的内存都是从limit_conn_zone命令定义的共享内存中获取的。为什么要使用共享内存呢？因为一个客户端的多个并发连接请求不一定会被同一个worker子进程处理，所以需要用共享内存来存储同一个ip的并发连接信息已使所有的子进程都可见。 再来看下limit_conn命令，该命令主要是用来设置限制客户端ip并发连接数所使用的共享内存和并发连接数大小。那么Nginx又是如何解析和组织limit_conn_zone指令参数的，其解析函数ngx_stream_limit_conn()执行流程如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546static char *ngx_stream_limit_conn(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)&#123; ngx_shm_zone_t *shm_zone; ngx_stream_limit_conn_conf_t *lccf = conf; ngx_stream_limit_conn_limit_t *limit, *limits; ngx_str_t *value; ngx_int_t n; ngx_uint_t i; value = cf-&gt;args-&gt;elts; /* 申请一块shm共享内存 */ shm_zone = ngx_shared_memory_add(cf, &amp;value[1], 0, &amp;ngx_stream_limit_conn_module); …… limits = lccf-&gt;limits.elts; …… /* * 判断是否已经有重复的共享内存，也就是说不同的limit_conn命令 * 不能指定同一个共享内存 */ for (i = 0; i &lt; lccf-&gt;limits.nelts; i++) &#123; if (shm_zone == limits[i].shm_zone) &#123; return "is duplicate"; &#125; &#125; /* * 获取limit_conn第二个参数，因为是限制的ip的连接数， * 所以将其转换为数字 */ n = ngx_atoi(value[2].data, value[2].len); …… limit = ngx_array_push(&amp;lccf-&gt;limits); if (limit == NULL) &#123; return NGX_CONF_ERROR; &#125; /* 保存解析结果 */ limit-&gt;conn = n; limit-&gt;shm_zone = shm_zone; return NGX_CONF_OK;&#125; 从上面的执行流程中可以看到，在解析limit_conn命令的时候也会调用ngx_shared_memory_add函数告诉Nginx内核需要使用一块共享内存，而在解析limit_conn_zone命令的时候也会调用ngx_shared_memory_add函数告诉Nginx需要使用一块共享内存。可以注意到，这两个地方调用这个函数的时候所使用的共享内存名字和参数tag都是一样的，换句话说他们都是指向同一块共享内存，只是在解析limit_conn命令调用ngx_shared_memory_add函数时指定的共享内存大小是0，而解析limit_conn_zone命令调用ngx_shared_memory_add函数时指定的共享内存大小是一个确切的值（配置文件中配置），这么处理也是符合常理的，因为limit_conn也确实不知道共享内存大小，所以便指定共享内存大小为0。另外，Nginx并没有规定这两条命令出现的先后顺序，所以此时解析配置文件的时候会有如下两种情况： 1、 配置文件中先配置了limit_conn_zone命令。这种情况下，当解析到limit_conn_zone指令时，因为配置文件中之前没有配置过同名字和tag的共享内存，所以此时会新增一个共享内存节点，挂载到全局唯一的ngx_cycle_t对象的shared_memory成员中。然后解析到limit_conn指令的时候则会通过匹配名字和tag直接返回刚刚新增的那块共享内存。 2、 配置文件中先配置了limit_conn命令。这种情况下，当解析到limit_conn指令时，也会因为配置文件中没有配置过同名字和tag的共享内存，所以也会新增一个共享内存节点到ngx_cycle_t对象的shared_memory中，但是这个时候该节点指定的共享内存大小是0。然后解析到limit_conn_zone指令时，由于名字和tag一样会索引到刚刚创建的那块共享内存，并且会发现共享内存大小为0，此时则会把limit_conn_zone指令指定的大小替换0，并返回这块共享内存。 这样就可以不用规定这两条命令出现的先后顺序，增加了配置文件的自由性。 从上面的流程图可以看出，对于每一条limit_conn指令，Nginx都会用如下的结构体进行抽象： 1234typedef struct &#123; ngx_shm_zone_t *shm_zone; // limit_conn第一个参数指定的共享内存 ngx_uint_t conn; // 一个ip同时可以发起的最大连接数&#125; ngx_stream_limit_conn_limit_t; limit_conn指令的解析结果会存储在这个对象中，并挂载到ngx_stream_limit_conn_module模块的server级别配置项中。另外，还有一点需要注意的就是，Nginx允许一个server块内出现多条limit_conn指令，当然前提是这些指令使用不同的共享内存，这个时候就会以动态数组的方式来组织多条limit_conn指令的配置信息，这一点可以参见ngx_stream_limit_conn_conf_t的定义。 如果出现了多条limit_conn指令，则所有的limit_conn指令都会生效，并且会以并发连接数最小的那个配置来决定客户端的连接是否会被断开。举个例子，如果有如下配置:server { limit_conn zone0 2; limit_conn zone1 3; }。对于一个客户端来说，如果并发连接数大于2的时候，那么会由于zone0配置的限制而导致第三个开始的后续连接都会被断开。这点从后面的功能实现部分可以看出。 二、 阶段介入 如果配置文件的stream块中配置了limit_conn和limit_conn_zone指令，在Nginx完成配置文件解析并提供stream四层反向代理服务后，客户端向stream块中的server发送请求时Nginx就会限制客户端的并发连接数了，那么Nginx是通过什么方式来介入到四层方向代理流程中的呢？在stream框架重要组成部分–ngx_stream_core_module模块的stream main级别配置项结构体中可以发现Nginx的处理方式，该配置项结构体如下： 123456789101112131415161718192021typedef struct &#123; /* * servers动态数组存放的是代表出现在stream块内的server块的 * 配置项结构体，在ngx_stream_core_server函数中会将生成的 * ngx_stream_core_module模块的srv级别配置项结构体添加到 * 这个动态数组中。 */ ngx_array_t servers; /* ngx_stream_core_srv_conf_t */ /* * 存放的是stream块内所有server块内出现的listen指令的参数， * 一个listen对应其中的一个元素 */ ngx_array_t listen; /* ngx_stream_listen_t */ /* stream limit conn模块注册的处理函数 */ ngx_stream_access_pt limit_conn_handler; /* stream access模块注册的处理函数 */ ngx_stream_access_pt access_handler; &#125; ngx_stream_core_main_conf_t; 从这个配置项结构体中可以发现Nginx会将ngx_stream_limit_conn_module模块处理函数挂载到limit_conn_handler字段中。那Nginx是什么时候把ngx_stream_limit_conn_module模块的处理函数注册到这里的呢，又是什么时候会调用这个函数呢？ 先来看下Nginx什么时候会把ngx_stream_limit_conn_module模块的处理函数注册到ngx_stream_core_main_conf_t对象的limit_conn_handler中。来看下ngx_stream_limit_conn_module模块实现的NGX_STREAM_MODULE模块类型的接口ngx_stream_module_t： 123456789static ngx_stream_module_t ngx_stream_limit_conn_module_ctx = &#123; ngx_stream_limit_conn_init, /* postconfiguration */ NULL, /* create main configuration */ NULL, /* init main configuration */ ngx_stream_limit_conn_create_conf, /* create server configuration */ ngx_stream_limit_conn_merge_conf, /* merge server configuration */&#125;; 从上面这个结构体中我们可以看到，ngx_stream_limit_conn_module模块注册了postconfiguration字段的回调函数，而这个回调函数正是在解析完配置文件之后被调用的，该模块正是在这个回调函数中将其对应的处理函数挂载到了ngx_stream_core_main_conf_t对象的limit_con_handler中，这从函数ngx_stream_limit_conn_init就可以看到，函数实现如下： 1234567891011static ngx_int_tngx_stream_limit_conn_init(ngx_conf_t *cf)&#123; ngx_stream_core_main_conf_t *cmcf; cmcf = ngx_stream_conf_get_module_main_conf(cf, ngx_stream_core_module); cmcf-&gt;limit_conn_handler = ngx_stream_limit_conn_handler; return NGX_OK;&#125; 再来看下Nginx又是什么时候会调用ngx_stream_limit_conn_module模块的处理函数来实现限制客户端的并发连接数的。从一开始的时候有说该模块是介于Pre-access阶段的，根据这个阶段的定位，这个时候Nginx已经和客户端建立了连接，但是还没有提供服务，所以是在ngx_stream_init_connection()函数中（该函数会注册给ngx_listening_t监听对象的handler）调用的。limit_conn_handler调用如下： 123456789101112131415161718voidngx_stream_init_connection(ngx_connection_t *c)&#123; …… /* * 如果注册了ngx_stream_limit_conn_module模块的处理方法， * 则会对来自同一个ip的并发连接数进行限制 */ if (cmcf-&gt;limit_conn_handler) &#123; rc = cmcf-&gt;limit_conn_handler(s); if (rc != NGX_DECLINED) &#123; ngx_stream_close_connection(c); return; &#125; &#125; ……&#125; 从这里可以看出如果limit_conn_handler返回的不是NGX_DECLINED，则会结束与客户端之间的连接。 三、 功能实现 通过阶段介入小节的介绍，我们已经知道ngx_stream_limit_conn_module模块是如何介入到stream子系统的处理流程中的。那么该模块又是如何根据解析到的配置信息来实现限制客户端的并发连接数的呢？这个就是ngx_stream_limit_conn_handler函数实现的了，其执行流程如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394static ngx_int_tngx_stream_limit_conn_handler(ngx_stream_session_t *s)&#123; size_t n; uint32_t hash; ngx_str_t key; ngx_uint_t i; ngx_slab_pool_t *shpool; ngx_rbtree_node_t *node; ngx_pool_cleanup_t *cln; struct sockaddr_in *sin; ngx_stream_limit_conn_ctx_t *ctx; ngx_stream_limit_conn_node_t *lc; ngx_stream_limit_conn_conf_t *lccf; ngx_stream_limit_conn_limit_t *limits; /* 判断客户端和nginx之间的连接协议族 */ switch (s-&gt;connection-&gt;sockaddr-&gt;sa_family) &#123; case AF_INET: sin = (struct sockaddr_in *) s-&gt;connection-&gt;sockaddr; /* 获取客户端ip地址 */ key.len = sizeof(in_addr_t); key.data = (u_char *) &amp;sin-&gt;sin_addr; break; …… &#125; /* 用crc32计算ip地址对应的hash值 */ hash = ngx_crc32_short(key.data, key.len); lccf = ngx_stream_get_module_srv_conf(s, ngx_stream_limit_conn_module); limits = lccf-&gt;limits.elts; for (i = 0; i &lt; lccf-&gt;limits.nelts; i++) &#123; ctx = limits[i].shm_zone-&gt;data; …… /* 用ip地址hash值到红黑树中查找对应的节点 */ node = ngx_stream_limit_conn_lookup(ctx-&gt;rbtree, &amp;key, hash); /* 如果node == NULL，说明该ip地址还没有发起过连接 */ if (node == NULL) &#123; n = offsetof(ngx_rbtree_node_t, color) + offsetof(ngx_stream_limit_conn_node_t, data) + key.len; /* * 从共享内存中分配用于存储红黑树节点的内存。一个红黑树节 * 点用来存储同一个ip对应的多个连接之间共有的状态信息。 */ node = ngx_slab_alloc_locked(shpool, n); /* * 申请内存失败，说明达到了共享内存支持的最大ip个数， * 返回NGX_ABORT，主流程会结束Nginx与这个客户端连接 */ if (node == NULL) &#123; …… return NGX_ABORT; &#125; lc = (ngx_stream_limit_conn_node_t *) &amp;node-&gt;color; node-&gt;key = hash; // 红黑树节点的key值为客户端ip的hash值 lc-&gt;len = (u_char) key.len; // 记录客户端ip地址长度 lc-&gt;conn = 1; // 该ip地址首次连接，conn置为1 ngx_memcpy(lc-&gt;data, key.data, key.len); // 记录客户端ip地址 ngx_rbtree_insert(ctx-&gt;rbtree, node); //将当前节点加入到红黑树中 &#125; else &#123; lc = (ngx_stream_limit_conn_node_t *) &amp;node-&gt;color; /* * 判断ip地址已经发起的连接总数是否达到了限制的阈值， * 如果达到了，返回NGX_ABORT，断开此次连接 */ if ((ngx_uint_t) lc-&gt;conn &gt;= limits[i].conn) &#123; …… return NGX_ABORT; &#125; /* * 程序执行到这里表明当前ip已经发起的连接数还没有 * 达到限制的阈值，递增已连接数，然后做后续处理 */ lc-&gt;conn++; &#125; …… &#125; /* 返回NGX_DECLINED，则主流程接着往下处理 */ return NGX_DECLINED;&#125; 从上面的流程中，可以清楚地看到其功能逻辑： 1、 如果配置文件中没有配置limit_conn_zone和limit_conn命令，则不会对客户端ip进行并发连接数的限制。这个从ngx_stream_init_connection()中可以看到。 2、 遍历配置文件中配置的所有limit_conn指令的规则，对于每一条规则，如果某个客户端ip第一次和Nginx建立连接，则会从共享内存中申请内存，存放已经建立的连接数（此时为1）以及客户端地址信息；如果某个客户端ip之前已经和Nginx建立过连接，则会判断已经建立的连接数是否达到了配置的阈值，如果达到了，则返回NGX_ABORT，在主流程ngx_stream_init_connection()函数中就会断开与客户端的连接。如果没有达到配置阈值，则会递增已建立连接数，并返回NGX_DECLINED，主流程则会继续往后续阶段执行。只要有一条规则不符合条件，则会导致Nginx断开与客户端的连接。]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ngx_stream_access_module模块解析]]></title>
    <url>%2F2016%2F10%2F15%2Fngx-stream-access-module%2F</url>
    <content type="text"><![CDATA[ngx_stream_access_module模块在stream子系统中是用来实现对某个ip访问控制功能。在stream子系统中，虽然没有像http子系统那样在代码中明确地定义处理阶段，但是其处理流程也是按照一定的阶段来划分的，stream处理阶段包括：Post-accept、Pre-access、Access、SSL、Preread、Content、Log。 按照上面的阶段划分，ngx_stream_access_module模块就是处在Access阶段的，下面便结合代码来分析下该模块的实现。 一、 配置命令解析 ngx_stream_access_module模块目前支持两条配置指令，allow和deny。两个命令的使用方法是一样的，如下： Syntax: allow/deny address | CIDR | unix: | all; Default: — Context: stream, server 从上面的用法可以看出，allow/deny分别是用来允许/限制某个地址的访问，地址的格式可以是普通的ip地址、CIDR形式的地址以及Unix-domain sockets。如果是all的话，则表示允许/限制所有地址的访问。如果在配置文件中配置了allow或者deny命令，当配置文件解析模块解析到这两个命令的时候，Nginx内部是如何来封装这两个命令的配置参数以便来实现其功能的呢？先来看下Nginx对这两个命令的定义： 123456789101112131415161718static ngx_command_t ngx_stream_access_commands[] = &#123; &#123; ngx_string("allow"), NGX_STREAM_MAIN_CONF|NGX_STREAM_SRV_CONF|NGX_CONF_TAKE1, ngx_stream_access_rule, NGX_STREAM_SRV_CONF_OFFSET, 0, NULL &#125;, &#123; ngx_string("deny"), NGX_STREAM_MAIN_CONF|NGX_STREAM_SRV_CONF|NGX_CONF_TAKE1, ngx_stream_access_rule, NGX_STREAM_SRV_CONF_OFFSET, 0, NULL &#125;, ngx_null_command&#125;; 从这里可以看出，当配置文件中出现allow/deny命令的时候，都是调用ngx_stream_access_rule函数来解析其配置信息的。所以现在来看下这个函数的实现，为了简化分析，这里只看ipv4的处理（ipv6和unix-domain socket的实现是类似的）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748static char *ngx_stream_access_rule(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)&#123; ngx_stream_access_srv_conf_t *ascf = conf; ngx_int_t rc; ngx_uint_t all; ngx_str_t *value; ngx_cidr_t cidr; ngx_stream_access_rule_t *rule; …… ngx_memzero(&amp;cidr, sizeof(ngx_cidr_t)); /* 获取配置文件的配置参数 */ value = cf-&gt;args-&gt;elts; /* 判断allow参数是不是"all"，如果是的话则all为1 */ all = (value[1].len == 3 &amp;&amp; ngx_strcmp(value[1].data, "all") == 0); if (!all) &#123; …… rc = ngx_ptocidr(&amp;value[1], &amp;cidr); // 计算ip地址和掩码 &#125; /* ipv4 */ if (cidr.family == AF_INET || all) &#123; /* 创建存储可访问和禁止访问规则的动态数组 */ if (ascf-&gt;rules == NULL) &#123; ascf-&gt;rules = ngx_array_create(cf-&gt;pool, 4, sizeof(ngx_stream_access_rule_t)); if (ascf-&gt;rules == NULL) &#123; return NGX_CONF_ERROR; &#125; &#125; /* 从动态数组中申请一个元素 */ rule = ngx_array_push(ascf-&gt;rules); if (rule == NULL) &#123; return NGX_CONF_ERROR; &#125; /* 将allow或deny中配置的ip信息记录下来 */ rule-&gt;mask = cidr.u.in.mask; rule-&gt;addr = cidr.u.in.addr; rule-&gt;deny = (value[0].data[0] == 'd') ? 1 : 0; &#125; …… return NGX_CONF_OK;&#125; 在这里的实现可以看出，Nginx内部将allow/deny命令抽象成了一条规则，其定义如下： 123456typedef struct &#123; in_addr_t mask; // 掩码地址 in_addr_t addr; // ip地址 /* 因为这个结构体是allow和deny共用，所以需要一个标志位来区分 */ ngx_uint_t deny; /* unsigned deny:1; */&#125; ngx_stream_access_rule_t; 所以解析allow/deny命令其实就是填充规则对象，并将规则对象挂载到了ngx_stream_access_module模块在server级别的配置项结构体里面，对于同一个server下面的allow/deny命令，则是以一个动态数组进行组织的。 在具体解析allow/deny命令的配置参数时，首先判断配置参数是不是all，如果是的话，就不进行ip地址的解析转换，而是直接填充上面的规则。由于函数一开始就将ngx_cidr_t类型的对象cidr清零，所以如果参数是all的话，对应的规则中的addr和mask都是0，而用来区分是allow还是deny命令的标志则视具体情况而定。如果配置参数不是all的话，那么就需要对配置参数进行转换，即把配置参数中的ip地址或者cidr形式的地址转换成对应的网络序值并存储在ngx_cidr_t对象中，然后从ngx_stream_access_module模块在server级别的配置项结构体的rule动态数组中申请一个元素，并将配置参数转换结果填充到规则中去。所以，如果一个server块中配置了多条allow/deny指令的话，等到server块解析完毕之后，那么这些命令参数都解析并存储到了ngx_stream_access_module模块在server级别的配置项结构体的rule动态数组中。 二、 访问控制介入 如果配置文件中某个server块内配置了allow/deny指令，在Nginx完成配置文件解析并提供stream四层反向代理服务后，客户端向Nginx中的该server发送请求时Nginx就会进行访问控制了，那么Nginx是通过什么方式来介入到四层方向代理流程中的呢？在stream框架重要组成部分–ngx_stream_core_module模块的stream main级别配置项结构体中可以发现Nginx的处理方式，该配置项结构体如下： 123456789101112131415161718192021typedef struct &#123; /* * servers动态数组存放的是代表出现在stream块内的server块的 * 配置项结构体，在ngx_stream_core_server函数中会将生成的 * ngx_stream_core_module模块的srv级别配置项结构体添加到 * 这个动态数组中。 */ ngx_array_t servers; /* ngx_stream_core_srv_conf_t */ /* * 存放的是stream块内所有server块内出现的listen指令的参数， * 一个listen对应其中的一个元素 */ ngx_array_t listen; /* ngx_stream_listen_t */ /* stream limit conn模块注册的处理函数 */ ngx_stream_access_pt limit_conn_handler; /* stream access模块注册的处理函数 */ ngx_stream_access_pt access_handler; &#125; ngx_stream_core_main_conf_t; 这个配置项结构体中可以发现Nginx会将ngx_stream_access_module模块的处理函数挂载到这里。那Nginx是什么时候把ngx_stream_access_module模块的处理函数注册到这里的呢，又是什么时候会调用这个函数呢？ 先来看下Nginx什么时候会把ngx_stream_access_module模块的处理函数注册到ngx_stream_core_main_conf_t对象的access_handler中的。ngx_stream_access_module模块实现的NGX_STREAM_MODULE模块类型的接口ngx_stream_module_t如下： 123456789static ngx_stream_module_t ngx_stream_access_module_ctx = &#123; ngx_stream_access_init, /* postconfiguration */ NULL, /* create main configuration */ NULL, /* init main configuration */ ngx_stream_access_create_srv_conf, /* create server configuration */ ngx_stream_access_merge_srv_conf /* merge server configuration */&#125;; 从上面这个结构体中我们可以看到，ngx_stream_access_module模块注册了postconfiguration字段的回调函数，而这个回调函数正是在解析完配置文件之后被调用的，该模块正是在这个回调函数中将其对应的处理函数挂载到了ngx_stream_core_main_conf_t对象的access_handler中，这从函数ngx_stream_access_init就可以看到，函数实现如下： 1234567891011static ngx_int_tngx_stream_access_init(ngx_conf_t *cf)&#123; ngx_stream_core_main_conf_t *cmcf; cmcf = ngx_stream_conf_get_module_main_conf(cf, ngx_stream_core_module); /* 注册access_handler */ cmcf-&gt;access_handler = ngx_stream_access_handler; return NGX_OK;&#125; 再来看下Nginx又是什么时候会调用ngx_stream_access_module模块的处理函数来实现访问控制功能的。因为要对某个ip地址实现访问控制，所以Nginx必须得先和这个地址建立连接，但又必须要在提供具体的服务之前，否则访问控制就没有意义。所以根据访问控制的定位，ngx_stream_core_main_conf_t结构体中的access_handler会回调函数会在ngx_stream_init_connection函数中被调用。而函数ngx_stream_init_connection则是在Nginx与客户端建立连接之后被调用，此时还没有提供任何服务，这与访问控制的思路是一致的。access_handler回调函数被调用的地方如下： 123456789101112131415161718voidngx_stream_init_connection(ngx_connection_t *c)&#123; …… /* * 如果配置了access模块命令，则进行准入判断，如果禁止访问， * 则结束请求 */ if (cmcf-&gt;access_handler) &#123; rc = cmcf-&gt;access_handler(s); if (rc != NGX_OK &amp;&amp; rc != NGX_DECLINED) &#123; ngx_stream_close_connection(c); return; &#125; &#125; ……&#125; 从这里可以看到access_handler如果返回的不是NGX_OK或者NGX_DECLINED的话，那么就会结束客户端请求及连接，也就是禁止该请求的的访问。 三、 访问控制实现 通过访问控制介入小节的分析，我们已经知道ngx_stream_access_module模块如何介入到Nginx的stream子系统的处理流程中。当客户端发来请求之后，Nginx又是如何依据从配置文件中解析得到的规则来进行访问控制的呢？仍以ipv4为例，结合ngx_stream_access_module模块处理函数ngx_stream_access_handler来看下： 12345678910111213141516171819202122static ngx_int_tngx_stream_access_handler(ngx_stream_session_t *s)&#123; struct sockaddr_in *sin; ngx_stream_access_srv_conf_t *ascf; ascf = ngx_stream_get_module_srv_conf(s, ngx_stream_access_module); /* 判断nginx与客户端之间的连接的协议族 */ switch (s-&gt;connection-&gt;sockaddr-&gt;sa_family) &#123; case AF_INET: /* ascf-&gt;rules不为空说明在配置文件中配置了access规则 */ if (ascf-&gt;rules) &#123; /* 获取客户端地址信息 */ sin = (struct sockaddr_in *) s-&gt;connection-&gt;sockaddr; return ngx_stream_access_inet(s, ascf, sin-&gt;sin_addr.s_addr); &#125; break; …… &#125; return NGX_DECLINED;&#125; ngx_stream_access_handler函数中调用的ngx_stream_access_inet函数实现如下： 1234567891011121314151617181920212223static ngx_int_tngx_stream_access_inet(ngx_stream_session_t *s, ngx_stream_access_srv_conf_t *ascf, in_addr_t addr)&#123; ngx_uint_t i; ngx_stream_access_rule_t *rule; /* 遍历解析配置文件时得到的规则动态数组，看addr是否在规则之中 */ rule = ascf-&gt;rules-&gt;elts; for (i = 0; i &lt; ascf-&gt;rules-&gt;nelts; i++) &#123; /* 如果在规则数组中找到了对应的ip地址，则需要进一步处理 */ if ((addr &amp; rule[i].mask) == rule[i].addr) &#123; return ngx_stream_access_found(s, rule[i].deny); &#125; &#125; /* * 如果客户端ip地址不在规则数组中，则返回NGX_DECLINED， * 主流程往下继续执行 */ return NGX_DECLINED;&#125; Ngx_stream_access_inet函数中调用的ngx_stream_access_found()函数实现如下： 123456789101112static ngx_int_tngx_stream_access_found(ngx_stream_session_t *s, ngx_uint_t deny)&#123; /* 如果是deny的话，则返回NGX_ABORT，表明当前ip不允许访问 */ if (deny) &#123; …… return NGX_ABORT; &#125; /* 如果是allow，则返回NGX_OK，表明当前ip允许访问 */ return NGX_OK;&#125; 从上面三个函数的实现我们可以非常清楚地看到ngx_stream_access_module模块实现的访问控制功能，其判断逻辑如下： 1、如果配置文件中没有配置访问控制规则，即没有配置allow/deny命令，则不对客户端请求进行访问控制。这个从ngx_stream_init_connection()中可以看到。 2、如果触发此次请求的客户端ip地址不在规则之内，则不对该ip地址进行访问控制判断，并返回NGX_DECLINED触发ngx_stream_init_connection()中的主流程继续往后续阶段处理。 3、如果触发此次请求的客户端ip地址在规则之内，并且该规则对应的是allow指令的话，则允许该ip地址进行访问，返回NGX_OK触发主流程往后续阶段处理，如果规则对应的是deny指令的话，则不允许该规则访问，返回NGX_ABORT，触发主流程结束请求的后续处理。 以上便是Nginx访问控制功能的实现机制。]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异步文件I/O]]></title>
    <url>%2F2016%2F09%2F03%2Faio-epoll-eventfd%2F</url>
    <content type="text"><![CDATA[Nginx中的文件异步I/O采用的不是glibc库提供的基于多线程实现的异步I/O,而是由Linux内核实现的。Linux内核提供了5个系统调用来完成文件操作的异步I/O功能，列举如下： 方法名 参数含义 执行意义 int io_setup(unsigned nr_events, aio_context_t *ctxp) nr_events表示需要可以处理的事件的最小个数，ctxp是文件异步I/O的上下文描述符指针 初始化文件异步I/O的上下文，返回0表示成功。 int io_destory(aio_context_t ctx) ctx是文件异步I/O的上下文描述符 销毁文件异步I/O的上下文，返回0表示成功。 int io_submit(aio_context_t ctx, long nr, struct iocb *cbp[]) ctx是文件异步I/O的上下文描述符，nr是一次提交的事件个数，cbp是提交的事件数组中的首个元素地址 提交文件异步I/O操作。返回值表示成功提交的事件个数 int io_cancel(aio_context_t ctx, struct iocb *iocb, struct io_event *result) ctx表示文件异步I/O的上下文描述符，iocb是要取消的异步I/O操作，而result表示这个操作的执行结果 取消之前使用io_submit提交的一个文件异步I/O操作。返回0表示成功。 int io_getevents(aio_context_t ctx, long min_nr,long_nr, struct io_event *events, struct timespec *timeout) ctx表示文件异步I/O的上下文描述符，获取的已完成事件个数范围是[min_nr,nr]，events是执行完成的事件数组，timeout是超时时间，也就是获取min_nr个时间前的等待时间。 从已完成的文件异步I/O操作队列中读取操作。 文件异步I/O中有一个核心结构体–struct iocb，其定义如下： 12345678910111213141516171819202122struct iocb &#123; /*存储业务指针，与io_getevents方法返回的io_event结构体data成员一致*/ u_int64_t aio_data; u_int32_t PADDED(aio_key, aio_reserved1); u_int16_t aio_lio_opcode; //操作码 int16_t aio_reqprio; //请求的优先级 u_int32_t aio_fildes; //异步I/O操作的文件描述符 u_int64_t aio_buf; //读/写对应的用户态缓冲区 u_int64_t aio_nbytes; //读/写操作的字节长度 int64_t aio_offset; //读/写操作对应的文件中的偏移量 u_int64_t aio_reserved2; /* * 设置为IOCB_FLAG_RESFD,表示当有异步I/O请求完成时让内核 * 使用evenfd进行通知，这是与epoll配合使用的关键 */ u_int32_t aio_flags; /* * 当aio_flags为IOCB_FLAG_RESFD，用于事件 * 通知的eventfd句柄 */ u_int32_t aio_resfd;&#125; 上述的struct iocb结构体中，aio_flags和aio_read两个结构体成员正是Nginx中将文件异步I/O、eventfd以及epoll机制结合起来一起使用的关键，三者的结合也使得Nginx中的文件异步I/O同网络事件的处理一样高效。另外有一点需要说明的就是Nginx中目前只使用了异步I/O中的读操作，即struct iocb结构体中的成员aio_lio_opcode的值为IO_CMD_PREAD，因为文件的异步I/O不支持缓存操作，而正常写文件的操作往往是写入内存中就返回，而如果使用异步I/O方式写入的话反而会使得速度下降。 struct iocb用在提交和取消异步I/O事件中，而通过io_getevents获取已经完成的I/O事件时则用到的是另一个十分重要的结构体—struct io_event，其定义如下： 1234567struct io_event &#123; uint64_t data; //与提交事件时iocb结构体的aio_data成员一致 uint64_t obj; //指向提交事件时对应的iocb结构体 /*异步I/O操作结果，res大于等于0表示成功，小于0失败*/ int64_t res; int64_t res2; //保留字段&#125; 在Nginx中，主要用到的字段就是data和res。其中data中保存的是文件异步I/O事件对象，res就是保存异步I/O的结果。 在简单了解了Linux内核提供的异步I/O系统调用及其在Nginx中涉及到的相关知识后，再来讲述下eventfd系统调用，因为这是Nginx将异步I/O事件集成到epoll中的一个桥梁。为什么这么说呢？通过上面对异步I/O结构体struct iocb结构体的分析，我们知道，当成员aio_flags设置为IOCB_FLAG_RESFD时，表明使用eventfd句柄来进行异步I/O事件的完成通知。正是这个eventfd，让其可以使用epoll机制来对其进行监控，从而间接对异步I/O事件完成进行监控，保证了事件驱动模块对网络事件和文件异步I/O事件的处理接口保持一致。 eventfd系统调用原型如下： 1int eventfd(unsigned int initval, int flags); eventfd系统调用常用与进程之间通信或者用于内核与应用程序之间通信。在Nginx中正是利用了内核会在异步I/O事件完成时通过eventfd通知Nginx来完成对异步I/O事件的间接监控。 在简单介绍完Linux内核提供的异步I/O接口以及eventfd系统调用后，接下来开始分析文件异步I/O事件是如何在ngx_epoll_module中实现的。下面是涉及到的主要全局变量，可以分为两部分： 1)、系统调用相关： int ngx_eventfd = -1; 这个是用于通知异步I/O事件完成的描述符，在Nginx中它会赋值给struct iocb结构体中的aio_resfd成员，也是epoll监控的描述符。 aio_context_t ngx_aio_ctx = 0; 这个就是异步I/O接口会使用到的异步I/O上下文，并且需要经过io_setup初始化后才能使用。 2)、与网络事件处理兼容相关。 static ngx_event_t ngx_eventfd_event; 这个就是eventfd描述符对应的读事件对象。因为文件异步I/O事件完成后，内核通知应用程序eventfd有可读事件(EPOLLIN)发生。然后应用程序就会调用读事件回调函数进行处理。 static ngx_connection_t ngx_eventfd_conn; 这个就是eventfd描述符对应的连接对象。 为什么要称它们是与网络事件处理兼容呢？回想下Nginx在处理网络事件的时候会为socket获取一个连接对象，然后设置连接对象ngx_connection_t的fd成员为socket描述符，接着设置连接的读事件和写事件，并设置对应的事件回调函数，最后将读/写事件（或整个连接）加入到epoll中监控。对应地处理文件异步I/O事件时，首先是让I/O事件的完成通知用eventfd来完成，然后设置eventfd的读事件及其处理函数，再用一个连接对象来保存eventfd和读事件，并将eventfd加入到epoll监控。这样就保证了Nginx内核可以像处理网络事件一样处理文件异步I/O事件。但是Nginx内核处理文件异步I/O事件又有其特别的地方。因为当epoll中监控到eventfd有读事件完成时，只可以说明Linux内核通知Nginx有文件异步I/O事件完成了，此时Nginx还并不知道有哪些或有几个异步I/O事件完成了，可以这么理解，eventfd仅仅是Linux内核用来通知Nginx有异步I/O事件完成了。那Nginx又是如何获取完成的异步I/O事件的呢，这就是eventfd描述符关联的读事件回调函数所需要完成的工作了，这个后面进行详细说明。 现假设有一个模块需要读取磁盘中的文件，那么如果Nginx启动了文件异步I/O处理的话，那么这个读盘的操作会被Nginx作为一个异步I/O事件来处理。 因为要将这个读盘事件以异步I/O方式来处理，那么首先就需要初始化一个异步I/O上下文，在Nginx中代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758static voidngx_epoll_aio_init(ngx_cycle_t *cycle, ngx_epoll_conf_t *epcf)&#123; int n; struct epoll_event ee;#if (NGX_HAVE_SYS_EVENTFD_H) ngx_eventfd = eventfd(0, 0); //调用eventfd()系统调用创建efd描述符#else ngx_eventfd = syscall(SYS_eventfd, 0);#endif …… n = 1; /*设置ngx_eventfd为非阻塞*/ if (ioctl(ngx_eventfd, FIONBIO, &amp;n) == -1) &#123; …… &#125; /* *初始化文件异步io上下文，aio_requests表示至少可以处理的异步文件io事件 *个数 */ if (io_setup(epcf-&gt;aio_requests, &amp;ngx_aio_ctx) == -1) &#123; …… &#125; /*设置异步io完成时通知的事件*/ /* ngx_event_t-&gt;data成员通常就是事件对应的连接对象*/ ngx_eventfd_event.data = &amp;ngx_eventfd_conn; ngx_eventfd_event.handler = ngx_epoll_eventfd_handler; ngx_eventfd_event.log = cycle-&gt;log; ngx_eventfd_event.active = 1; //下面会加入到epoll中监控 ngx_eventfd_conn.fd = ngx_eventfd; ngx_eventfd_conn.read = &amp;ngx_eventfd_event; /*文件异步io对应的连接对象读事件为ngx_eventfd_event*/ ngx_eventfd_conn.log = cycle-&gt;log; ee.events = EPOLLIN|EPOLLET; //监控eventfd读事件并设置为ET模式 /* * ngx_eventfd被监控到有读事件发生时，会利用ee.data.ptr获取 * 对应的连接对象,详见ngx_epoll_process_events() */ ee.data.ptr = &amp;ngx_eventfd_conn; /* * 将异步文件io的通知的描述符加入到epoll监控中，因为 *在ngx_file_aio_read( )函数中将struct iocb结构体的aio_flags * 成员赋值为IOCB_FLAG_RESFD，他会告诉内核当异步io请求处理完成时使用 *eventfd描述符通知应用程序，这使得异步io、eventfd和epoll可以结合起来 *使用。另外，将将struct iocb结构体的aio_resfd设置为ngx_eventfd， *那么当有异步io事件完成时，epoll就会收到ngx_eventfd描述符的读 *事件，然后*ngx_epoll_process_events()中会调用其读事件回调函数，即 * ngx_epoll_eventfd_handler处理内核的通知。 */ if (epoll_ctl(ep, EPOLL_CTL_ADD, ngx_eventfd, &amp;ee) != -1) &#123; return; &#125; ……&#125; 通过调用ngx_epoll_aio_init方法，Nginx就将异步I/O以eventfd为桥梁与epoll结合起来了。 初始化完异步I/O上下文后，模块就可以提交文件异步I/O事件了。在此之前需要再了解下Nginx封装的一个异步I/O事件的对象，如下： 123456789101112struct ngx_event_aio_s &#123; void *data; /*由业务模块实现，用于在异步I/O事件完成后进行业务相关的处理*/ ngx_event_handler_pt handler; ngx_file_t *file; //文件异步I/O涉及的文件对象 …… ngx_fd_t fd; //异步I/O将要操作的文件描述符 …… /*aiocb就是struct iocb类型的，异步I/O事件控制块*/ ngx_aiocb_t aiocb; ngx_event_t event; //异步I/O对应的事件对象&#125;; 那Nginx中是如何处理异步I/O事件的提交的呢？其代码实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364ssize_tngx_file_aio_read(ngx_file_t *file, u_char *buf, size_t size, off_t offset, ngx_pool_t *pool)&#123; ngx_err_t err; struct iocb *piocb[1]; ngx_event_t *ev; ngx_event_aio_t *aio; …… /* * ngx_event_aio_t封装的异步io对象，如果file-&gt;aio为空， * 需要初始化* file-&gt;aio */ if (file-&gt;aio == NULL &amp;&amp; ngx_file_aio_init(file, pool) != NGX_OK) &#123; return NGX_ERROR; &#125; aio = file-&gt;aio; ev = &amp;aio-&gt;event; …… /*提交异步事件之前要初始化结构体struct iocb*/ ngx_memzero(&amp;aio-&gt;aiocb, sizeof(struct iocb)); /* * 将struct iocb的aio_data成员赋值为异步io的事件对象，下面提交异步 * I/O事件之后，等该事件完成，在通过io_getevents()获取到事件后， * 对应的struct io_event结构体中的data成员就会指向这个事件。 * struct iocb的aio_data成员和struct io_event的data成员指向的是 * 同一个东西 */ aio-&gt;aiocb.aio_data = (uint64_t) (uintptr_t) ev; aio-&gt;aiocb.aio_lio_opcode = IOCB_CMD_PREAD; aio-&gt;aiocb.aio_fildes = file-&gt;fd; aio-&gt;aiocb.aio_buf = (uint64_t) (uintptr_t) buf; aio-&gt;aiocb.aio_nbytes = size; aio-&gt;aiocb.aio_offset = offset; /* * 设置IOCB_FLAG_RESFD当内核有异步io请求处理完时 * 通过eventfd通知应用程序 */ aio-&gt;aiocb.aio_flags = IOCB_FLAG_RESFD; aio-&gt;aiocb.aio_resfd = ngx_eventfd; //这个就是eventfd描述符 /* * 将异步I/O对应的事件处理函数设置为ngx_file_aio_event_handler。 *当io_getevents()函数中获取到该异步io事件时，会调用该回调函数， * 在Nginx中并不是直接调用，而是先将其加入到ngx_posted_event队列， * 等遍历完所有完成的异步io事件后，再依次调用所有事件的回调函数 */ ev-&gt;handler = ngx_file_aio_event_handler; piocb[0] = &amp;aio-&gt;aiocb; /* * 将该异步io请求加入到异步io上下文中，等待io完成，内核会通过eventfd * 通知应用程序 */ if (io_submit(ngx_aio_ctx, 1, piocb) == 1) &#123; ev-&gt;active = 1; ev-&gt;ready = 0; ev-&gt;complete = 0; return NGX_AGAIN; &#125; ……&#125; 在模块提交了文件异步I/O事件后，在事件完成之后，Linux就会触发eventfd的读事件来告诉Nginx异步I/O事件完成了，我们知道，当epoll监控到eventfd有事件发生时，在ngx_epoll_process_events()函数中会通过epoll_wait取出该事件，然后通过struct epoll_event结构体中的data.ptr成员获取eventfd对应的连接对象(在上面有介绍)，并调用连接对象中的读事件处理函数ngx_epoll_eventfd_handler()，而Nginx正是通过这个读事件处理函数来获取真正完成的文件异步I/O事件，这个读事件处理函数正是在ngx_epoll_aio_init()函数中进行注册的。该函数的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263static voidngx_epoll_eventfd_handler(ngx_event_t *ev)&#123; int n, events; long i; uint64_t ready; ngx_err_t err; ngx_event_t *e; ngx_event_aio_t *aio; struct io_event event[64]; //一次性最多处理64个异步io事件 struct timespec ts; /* * 通过read获取已经完成的事件数，并设置到ready中，注意这里的ready * 可以大于64 */ n = read(ngx_eventfd, &amp;ready, 8); …… ts.tv_sec = 0; ts.tv_nsec = 0; while (ready) &#123; /* *从已完成的异步io队列中读取已完成的事件，返回值代表获取的事件个数 */ events = io_getevents(ngx_aio_ctx, 1, 64, event, &amp;ts); …… if (events &gt; 0) &#123; ready -= events; //计算剩余已完成的异步io事件 for (i = 0; i &lt; events; i++) &#123; /* * data成员指向这个异步io事件对应着的实际事件， * 这个与struct iocb 结构体中的aio_data成员是一致的。 * struct iocb 控制块中的aio_data成员被赋予对应io事件 * 对象是在函数ngx_file_aio_read()中实现的。 */ e = (ngx_event_t *) (uintptr_t) event[i].data; …… /* * 异步io事件ngx_event_t-&gt;data成员指向的 * 就是ngx_event_aio_t对象，这个在 * ngx_file_aio_init()函数中可以看到 */ aio = e-&gt;data; /*res成员代表的是异步io事件执行的结果*/ aio-&gt;res = event[i].res; /* * 将异步io事件加入到ngx_posted_events普通读写 * 事件队列中 */ ngx_post_event(e, &amp;ngx_posted_events); &#125; continue; &#125; if (events == 0) &#123; return; &#125; /* events == -1 */ ngx_log_error(NGX_LOG_ALERT, ev-&gt;log, ngx_errno, "io_getevents() failed"); return; &#125;&#125; 一般来说业务模块在文件异步I/O事件完成后都需要在进行一些和业务相关的处理，那Nginx又是怎么实现的呢？当然是通过注册回调函数的方法来实现的。在通过ngx_epoll_eventfd_handler()回调函数获取到已经完成的文件异步I/O事件并加入到ngx_posted_events队列中后，执行ngx_posted_events队列中的事件时就会回调异步I/O事件完成的回调函数ngx_file_aio_event_handler(在ngx_file_aio_read注册)，然后在该函数中调用业务模块(提交文件异步I/O事件的模块)实现的回调方法，这个方法一般都是在业务模块提交异步I/O事件前注册到上面介绍的ngx_event_aio_t的handler成员中。其中异步I/O事件完成后的回调函数实现如下： 12345678910111213141516171819202122static voidngx_file_aio_event_handler(ngx_event_t *ev)&#123; ngx_event_aio_t *aio; /* * 获取事件对应的data对象，即ngx_event_aio_t，这个在 * ngx_file_aio_init()函数中初* 始化的 */ aio = ev-&gt;data; …… /* * 这个回调是由真正的业务模块实现的，举个例子如果是http cache模块， * 则会在ngx_http_file_cache_aio_read()函数中调用完 * ngx_file_aio_read()后设置为ngx_http_cache_aio_event_handler() * 进行业务逻辑的处理，为什么要在调用完 * ngx_file_aio_read()之后再设置呢，因为可能业务模块一开始并没有为 * ngx_file_t对* 象设置ngx_event_aio_t对象，而是在 * ngx_file_aio_read()中调用ngx_file_aio_init()进行初始化的。 */ aio-&gt;handler(ev);&#125; 到这里Nginx中设计到的文件异步I/O、eventfd和epoll的配合使用就介绍完了。 参考文章: 1、《深入理解Nginx 模块开发与架构解析》]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx信号控制]]></title>
    <url>%2F2016%2F08%2F03%2Fsignal-control%2F</url>
    <content type="text"><![CDATA[在生产环境中，Nginx一般采用的是一个master进程，多个worker进程的模型，其中master进程不需要处理网络事件，它不负责业务的执行，只是通过管理worker进程来实现重启服务、平滑升级、更换日志文件、配置文件实时生效等功能，而worker进程则是用来提供服务，如静态文件服务、反向代理等功能。那么Nginx中是如何实现master的重启服务、平滑升级、更换日志文件以及配置文件实时生效的呢？另外，master进程是如何将重启服务、平滑升级、更换日志文件以及配置文件实时生效等告知worker进程的呢？答案是信号。下面将结合代码来看看Nginx是如何实现上述功能的。 因为Nginx是利用信号来实现平滑升级、更换日志文件、配置文件实时生效、重启服务等功能的，所以在Nginx的启动过程中会向操作系统内核注册所使用到的信号，其代码实现如下： 12345678910int ngx_cdeclmain(int argc, char *const *argv)&#123; …… /*初始化信号*/ if (ngx_init_signals(cycle-&gt;log) != NGX_OK) &#123; return 1; &#125; ……&#125; ngx_init_signals()实现如下： 123456789101112131415161718ngx_int_tngx_init_signals(ngx_log_t *log)&#123; ngx_signal_t *sig; struct sigaction sa; //Linux内核使用的信号 /*遍历signals数组，向内核注册所有nginx支持的信号*/ for (sig = signals; sig-&gt;signo != 0; sig++) &#123; ngx_memzero(&amp;sa, sizeof(struct sigaction)); sa.sa_handler = sig-&gt;handler; //设置信号发生时的处理函数 sigemptyset(&amp;sa.sa_mask); //向内核注册信号的回调方法 if (sigaction(sig-&gt;signo, &amp;sa, NULL) == -1) &#123; …… &#125; &#125; return NGX_OK;&#125; 从ngx_init_signals()函数的实现中我们可以看到，Nginx通过系统调用sigaction()将所支持的信号注册到操作系统内核，当内核捕捉到对应的信号时，就会调用回调函数进行信号处理。从代码中可以看到，其实Nginx支持的所有信号对应的处理函数都是同一个，即ngx_signal_hanlder()。在这个函数中，会根据锁发生的信号，将Nginx中对应于该信号的一个全局变量置位，然后在master进程的处理循环中将会依据这个全局变量进行相应的动作，这部分的具体实现稍后介绍。 一般来说当环境中已经有Nginx进程（包括master进程、worker进程）在运行了，才有所谓的平滑升级、更换日志文件或者配置文件实时生效等管理功能。那么Nginx是如何通过命令行控制这些功能的呢？Nginx的做法是启动一个新的Nginx进程，来向环境中已经存在的master进程发送相应的控制信号，从而实现让已有服务执行相应的动作。那么新的Nginx进程是如何向环境中已有的master进程发送信号的呢？其代码实现如下： 12345678910int ngx_cdeclmain(int argc, char *const *argv)&#123; …… /*"nginx -s xxx"*/ if (ngx_signal) &#123; return ngx_signal_process(cycle, ngx_signal); &#125; ……&#125; 从代码实现上可以看到新的Nginx进程执行完这个之后就返回退出了，因为这个新启动的进程就是用来发送信号的。那新的进程是如何向已有master进程发送信号的呢？答案是通过kill系统调用。总的来说也是分两步：首先是获取已运行master进程存放在nginx.pid文件中的pid，也就是ngx_signal_process()函数的功能，其实现如下： 12345678910111213141516171819202122232425262728293031323334353637ngx_int_tngx_signal_process(ngx_cycle_t *cycle, char *sig)&#123; ssize_t n; ngx_pid_t pid; ngx_file_t file; ngx_core_conf_t *ccf; u_char buf[NGX_INT64_LEN + 2]; /*获取核心模块存储配置项结构体指针*/ ccf = (ngx_core_conf_t *) ngx_get_conf(cycle-&gt;conf_ctx, ngx_core_module); ngx_memzero(&amp;file, sizeof(ngx_file_t)); file.name = ccf-&gt;pid; //ccf-&gt;pid为nginx.pid文件 file.log = cycle-&gt;log; /*以可读方式打开nginx.pid文件*/ file.fd = ngx_open_file(file.name.data, NGX_FILE_RDONLY, NGX_FILE_OPEN, NGX_FILE_DEFAULT_ACCESS); …… /*读文件*/ n = ngx_read_file(&amp;file, buf, NGX_INT64_LEN + 2, 0); if (ngx_close_file(file.fd) == NGX_FILE_ERROR) &#123; …… &#125; if (n == NGX_ERROR) &#123; return 1; &#125; /*去掉结尾的控制字符*/ while (n-- &amp;&amp; (buf[n] == CR || buf[n] == LF)) &#123; /* void */ &#125; /*将字符串转换为数字，获取master进程pid*/ pid = ngx_atoi(buf, ++n); …… /*封装kill系统调用的信号发送函数*/ return ngx_os_signal_process(cycle, sig, pid);&#125; 其次，在获取到已运行master进程的pid后，调用kill命令将新的Nginx进程携带的信号发送给已运行master进程，这也正是ngx_os_signal_process()函数的功能，其实现如下： 1234567891011121314151617181920ngx_int_tngx_os_signal_process(ngx_cycle_t *cycle, char *name, ngx_pid_t pid)&#123; ngx_signal_t *sig; /* * 遍历nginx内核支持的信号，找到与name相同的信号，通过kill系统 * 调用向master进程发送信号 */ for (sig = signals; sig-&gt;signo != 0; sig++) &#123; if (ngx_strcmp(name, sig-&gt;name) == 0) &#123; if (kill(pid, sig-&gt;signo) != -1) &#123; return 0; &#125; ngx_log_error(NGX_LOG_ALERT, cycle-&gt;log, ngx_errno, "kill(%P, %d) failed", pid, sig-&gt;signo); &#125; &#125; return 1;&#125; 这个时候新的Nginx进程的任务就完成了，然后就返回退出了，那么接下来已运行的master进程是如何处理的呢？这个就涉及到master进程的工作循环了。我们知道master进程是不对外提供服务的，而是专门用来管理worker进程的，那Nginx中是如何实现的呢？前面提到了，Nginx中是通过新启动一个进程来给master进程发送信号的，所以master进程的工作循环中就是在等待信号的到来。信号到来之后，会触发信号处理函数，然后将信号对应的标志位置位，然后在master进程中就检测相应的标志位来进行相应的处理。在介绍master对于具体信号是如何处理之前看下master进程对哪些信号感兴趣，列举如下： 信号 信号对应的全局变量 意义 QUIT ngx_quit 优雅地关闭整个服务 TERM(INT) ngx_terminage 强制关闭整个服务 USR1 ngx_reopen 重新打开所有文件 WINCH ngx_noaccept 所有子进程不再接收新的连接 USR2 ngx_change_binary 平滑升级 HUP ngx_reconfigure 重新读取配置文件 CHLD ngx_reap 子进程退出，监控所有子进程 上面表格中的CHLD信号并不是有新的Nginx进程发送的，而是操作系统内核在检测到有子进程退出时会向父进程，即master发送一个CHLD信号，然后master进程在对此做进一步分析，亦即ngx_reap_children()的功能。在ngx_master_process_cycle()中我们可以看到，master首先将其感兴趣的信号加入到了阻塞自己的信号集合中（通过SIG_BLOCK调用sigprocmask()），在完成这些动作之后会调用sigsuspend()将自己挂起，等待信号集合中的信号发生，将自己唤醒，然后依据信号发生后置位的全局变量（见上表）做相应的处理，其处理流程图如下： 从master进程的工作循环中我们可以看到，当master收到相应的信号，并做完自己处理流程后，会通过ngx_signal_worker_processes()向worker进程发送相应的信号。举个例子，比如在收到了QUIT信号之后，master会向所有的worker子进程也发送QUIT信号，以通知worker要优雅地退出（所谓的优雅，其实就是处理完现有连接，不再接受新的连接），并关闭监听的socket句柄。那么worker进程会对那些信号感兴趣，以及在收到master发送的相应信号后是如何处理的呢？这个就是worker工作循环的内容了。在worker进程中，收到了master发送的信号后，也会将在介绍worker工作循环之前，先来看下worker对那些信号感兴趣以及会将那些对应的全局变量置位，列举如下： 信号 信号对应的全局变量 意义 QUIT ngx_quit 优雅地关闭服务，退出子进程 TERM(INT) ngx_terminage 强制关闭服务，退出子进程 USR1 ngx_reopen 重新打开所有文件 除了上面介绍的三个信号，在worker进程的工作循环中还可以看到另一个全局变量ngx_exciting。这个标志位只有一个地方会设置它，也就是在收到QUIT信号后。ngx_quit只有在首次设置为1时，才会将ngx_exciting置位。为什么呢？因为当worker进程收到QUIT信号后，它会知道自己需要优雅地关闭进程，即处理完现有连接并且不再接受新的连接，所以ngx_exciting表示的是一种正在退出的状态，即还有连接没有处理完。下面就是worker进程的工作流程： 这里只是简要的说明了master进程和worker进程的信号处理流程，对于详细的处理流程，比如平滑升级、配置文件实时生效等，还是要通过阅读代码才能更好的理清细节。 参考文章: 1、《深入理解Nginx 模块开发与架构解析》]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[slab共享内存使用及其实现原理]]></title>
    <url>%2F2016%2F08%2F02%2Fslab-shared-memory%2F</url>
    <content type="text"><![CDATA[在生产环境中，Nginx一般采用的是一个master进程，多个worker进程的模型，这样设计有一个好处，就是服务更加健壮，但是另一方面，如果一个请求分布在不同的进程上，当进程间需要互相配合才能完成请求的处理时，进程间通信的困难就凸显出来，虽然已经有许多进程间通信的方法，但是那些都只适合简单语意的场景，如果进程间需要交互复杂对象，如树、图等，则需要新的进程间通信方式，那就是今天要讲述的slab共享内存。 在讲述如何使用slab共享内存之前，先介绍几个和slab相关密切的结构体： 1、 ngx_cycle_t中的shared_memory成员： 12345struct ngx_cycle_s &#123; …… ngx_list_t shared_memory; ……&#125;; 我们知道，在Nginx中，无论是master还是worker进程，ngx_cycle_t结构体对于一个进程来说是唯一的。在读取或者初始化配置文件时，如果某个Nginx模块需要使用共享内存，则通过调用ngx_shared_memory_add函数向上面的全局共享内存链表shared_memory中添加一个共享内存的节点信息（并未真正申请内存并做初始化），并返回这个节点供模块后续操作。 2、ngx_shm_zone_t结构体 这个结构体就是ngx_shared_memory_add函数返回的节点地址，其定义如下： 12345678struct ngx_shm_zone_s &#123; void *data; // 作为init方法的参数，用于传递数据 ngx_shm_t shm; //描述共享内存的结构体 /*在真正创建好slab共享内存池后调用这个方法*/ ngx_shm_zone_init_pt init; void *tag; //对应于ngx_shared_memory_add的tag参数 ngx_uint_t noreuse; /* unsigned noreuse:1; */&#125;; 这个结构体中各个成员的意义和用法在注释中已经说明，其中需要强调的是tag成员，这个成员是用来防止两个毫不相关的Nginx模块定义的共享内存恰好具有同样的名字，从而导致数据管理的混乱。通常，tag参数均会传入本Nginx模块的结构体的地址。 3、ngx_slab_pool_t结构体 这个结构体就是slab共享内存池的管理结构体，用于描述这块共享内存池的信息，其定义如下： 12345678910111213141516typedef struct &#123; ngx_shmtx_sh_t lock; size_t min_size; //一页中最小内存块(chunk)大小 size_t min_shift; //一页中最小内存块对应的偏移 ngx_slab_page_t *pages; //slab内存池中所有页的描述 ngx_slab_page_t *last; //指向最后一个可用页 ngx_slab_page_t free; //内存池中空闲页组成链表头部 u_char *start; //实际页起始地址 u_char *end; //实际页结束地址 ngx_shmtx_t mutex; //slab内存池互斥锁 u_char *log_ctx; u_char zero; unsigned log_nomem:1; void *data; void *addr; //指向内存池起始地址&#125; ngx_slab_pool_t; 内存池管理结构中成员的具体作用在下面将会介绍的slab共享内存的内存布局时会有体现。 4、ngx_slab_page_t结构体 slab共享内存有一个很重要的思想就是分页机制，这和操作系统的分页是类似的。例如将一块slab共享内存以4KB作为一页的大小分成许多页，然后每个页就用ngx_slab_page_t结构体进行管理，其定义如下： 12345struct ngx_slab_page_s &#123; uintptr_t slab; //多用途，描述页相关信息，bitmap和内存块大小 ngx_slab_page_t *next; //指向双向链表中的下一个页 uintptr_t prev; //指向双向链表前一个页，低2位用于存放内存块类型&#125;; 这个结构体中的slab和prev都是有多用途的，它们的意义如下： 参数值 参数含义 执行意义 执行意义 小块内存 NGX_SLAB_SMALL 表示该页中存放的等长内存块的大小对应的偏移量 指向双向链表的前一个元素，低2位为11，以NGX_SLAB_SMALL表示小块内存 指向双向链表中的下一个元素（链表中页等分的内存块大小相等） 中等内存 NGX_SLAB_EXACT 作为bitmap表示该页中内存块的使用情况，bitmap中某个位为1表明对应内存块已使用 指向双向链表的前一个元素，低2为10，NGX_SLAB_EXACT表示中等大小内存 指向双向链表中的下一个元素（链表中页等分的内存块大小相等） 大块内存NGX_SLAB_BIG 高16位作为bitmap表示该页中内存块使用情况，低4位用来表示内存块大小对应的位移 指向双向链表的前一个元素，低2位为01，NGX_SLAB_BIG表示大块内存 指向双向链表中的下一个元素（链表中页等分的内存块大小相等） 超大块内存 NGX_SLAB_PAGE 超大块内存会使用一页或者多页,这批页面中的第一页， slab的前三位会被设置为NGX_SLAB_PAGE_START。其余为表示紧随其后的相邻的同批页面；其余页的slab会被设置为SLAB_PAGE_BUSY 指向双向链表的前一个元素，低2位为01，NGX_SLAB_BIG表示大块内存 指向双向链表中的下一个元素（链表中页等分的内存块大小相等） 上面就是slab涉及到的几个结构体及其相关成员的意义和用法，如果一个Nginx模块需要使用要共享内存，则一般遵循下面的使用步骤： 1、在读取和初始化配置文件时，调用ngx_shared_memory_add向全局共享内存链表ngx_cycle_t-&gt;shared_memory中添加一个共享内存节点，此时并没有真正分配内存及初始化，告诉Nginx内核模块需要使用共享内存，等Nginx内核读取完配置文件，初始化服务的时候才会去申请共享内存，并做初始化，此部分详见ngx_init_cycle。这个函数一般是由模块调用的。 2、 Nginx内核读取完配置文件后，初始化服务的时候会在ngx_init_cycle中调用ngx_shm_alloc和ngx_slab_init遍历全局slab共享内存链表shared_memory，依次申请和初始化每一块slab共享内存。ngx_shm_alloc函数用于申请共享内存，其具体通过系统调用mmap实现；ngx_slab_init函数用于初始化ngx_shm_alloc申请的用作slab的共享内存。 3、 在模块向Nginx内核申请了共享内存节点以及Nginx内核完成共享内存申请和初始化之后，模块就可以根据自身功能需要去申请和释放共享内存了，其涉及的主要函数如下： 1） 申请函数：ngx_slab_alloc、ngx_slab_alloc_locked和ngx_slab_alloc_pages 2） 释放函数：ngx_slab_free、ngx_slab_free_locked和ngx_slab_free_pages 介绍完模块使用slab共享内存的一般步骤之后，来介绍下slab共享内存的实现原理，这里只是对内存布局，申请和释放过程简单做了总结，想要完全知道实现原理，还是得去看源码。 首先来看下slab共享内存的内存布局。slab内存布局如下图所示，需要补充说明的是图中的结构体成员为了画图的方便对其定义顺序进行了必要的调整，并且有部分成员没有列举。 slab内存布局体现在ngx_slab_init函数对slab共享内存的初始化中，下面对内存布局进行简单的介绍： 1、 ngx_slab_pool_t。这个是整块slab共享内存池的管理结构，其中描述的就是这块slab共享内存池的信息，如实际用于分配的页的起始地址、页描述数组首地址等。 2、slots数组。在slab共享内存池中有三种状态的页，分别是空闲页、半满页和全满页。ngx_slab_pool_t-&gt;free便指向的就是空闲页组成的链表，半满页就是指其中的内存块未完全被分配完的页，这些页会以其中内存块的大小对应的位移作为下标存放自slots数组对应的元素中，并且互相之间也是以链表连接；然后对于所有内存块都已经使用完的全满页，其会脱离半满页链表。综上，slots数组中每个元素存放的是对应页中内存块大小的半满页链表。 3、 pages数组。pages数组是slab内存池中用于管理实际用于分配的页的管理结构组成的数组。 4、 因为slab内存池中是通过地址对齐的方式将每个用于实际分配的页和其对应的管理结构关联起来的，因此每一页的首地址都必须是以4k大小对齐的。图中部分便是由于地址对齐而浪费的内存空间。 5、 实际用于分配的页。其是通过地址对齐方式同对应的页管理结构关联，即pages数组中的元素。 6、 同第4部分中一样也是浪费的内存空间。 再看下slab共享内存的申请。slab共享内存的申请涉及到的接口函数有以下几个：ngx_slab_alloc、ngx_slab_alloc_locked和ngx_slab_alloc_pages。我们以申请一块大小为13 bytes的内存块为例。 1、 初始状态下所有页都是空闲页，需要注意的是空闲页之间并不是通过链表连接的，其布局如下： 2、 当前Nginx内核支持以下几种规格内存块大小的页：8 bytes、16 bytes、32 bytes、64 bytes、128 bytes、…、2048 bytes，这说明slots数组中有9个元素。如果我们要申请13 bytes大小的内存块，由于其介于[8, 16]bytes之间，因此要申请其中存放的内存块是16 bytes的页，此时内存池中布局如下： 3、 我们的目的是为了申请一块大小为13 bytes的内存块，因为之前对应的slots[1]中是空链表，所以会从实际页面中申请一个页，并将其在free空闲链表中对应的页管理结构挂在slots[1]中，并且该页中的内存块大小为16 bytes。那此时页管理结构中每个成员的值如下： 成员 意义描述 值 uintptr_t slab 表示该页中内存块大小 0x000000004 ngx_slab_page_t *next 表示同处slots[1]的下一个链表元素，如果没有，为NULL NULL uintptr_t pre 低两位为11，表示小块内存，其余位为slots[1]地址 &amp;slots[1] 4、 因为13 bytes的内存块在slab中属于是NGX_SLAB_SMALL类型的内存，其在一页中可以划分的内存块数量大于 8 * sizeof(uintptr_t)个，因此一个slab的位数不足以表示其中内存块的使用情况，此时其页管理结构中的ngx_slab_page_t-&gt;slab用于表示内存块大小对应的偏移，那么它的bitmap用什么来表示呢？答案是用页中开头的内存块依据需要作为bitmap来使用。还是以13 bytes为例(以32位为例)，计算如下：虽然我们要申请的是13 bytes，但是slab分配的内存会是16 bytes。内存块为16 bytes的页中内存块数量 = 1 &lt;&lt; (ngx_pagesize_shift - 4) = 256个， 所需bitmap个数 = 1 &lt;&lt; (ngx_pagesize_shift - 4) / (8 * sizeof(uintprt_t)) = 8个 所有bitmap占用内存块个数 = 8 * sizeof(uintptr_t) / 16 = 2 个 此时该页内存布局和bitmap值如下： bitmap[0]的第三位为111，表示低两位对应的内存块用于bitmap，第三位对应的是此次申请的内存块，以上三块均在使用中，故均为1。 最后讲述下slab共享内存的释放，释放的具体实现较为复杂，其中包含了全满页释放内存块后入半满页链表，半满页释放内存块后如果其中内存块均为使用则需要如空闲链表，在最开始处说过所有未使用的空闲页之间不是以链表相连接的，但是如果是重入空闲链表，则重入的页和剩余未使用的页之间使用链表相连接的。 还是以刚刚申请的内存块为例。如果使用共享内存的模块释放了申请的共享内存，那么由于此时其中的所有内存块均未使用（用于bitmap的两个内存块除外）需要将该页重新加入空闲页组成的链表中，此时slab内存池状态如下： 到这里，slab共享内存池的使用及实现原理就介绍完了，对于slab共享内存池的实现原理，在Nginx源码中的实现比这里介绍的更为复杂，这里只是对一种一种情况进行的简要分析，如果要更好的理解实现原理，还是需要参考相关资料并从源码入手才能真正掌握。 参考文章: 1、《深入理解Nginx 模块开发与架构解析》]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实例分析set命令及相关脚本引擎]]></title>
    <url>%2F2016%2F08%2F02%2Fset-script-engine%2F</url>
    <content type="text"><![CDATA[Nginx的ngx_http_rewrite_module模块使用了Nginx的脚本引擎，提供了外部变量的功能，主要是通过set命令进行定义。在Nginx中，什么是外部变量，什么是内部变量呢？简单来说，内部变量就是在Nginx的代码中定义的，外部变量就是在nginx.conf配置文件中定义的，并且在文件中确定了变量的赋值。其实Nginx中对变量的定义的语义和C语言中的对变量的声明的语义是比较接近的，因为这个时候并没有为这个变量分配用于存储变量值的内存，只是说明了有这么个变量存在。 外部变量在Nginx启动的时候通过脚本引擎被编译为了C代码，但是它们是在请求处理过程中才被执行和生效的。因此，外部变量一般是通过如下步骤进行设计的： 1、 Nginx启动时将配置文件中的set脚本式配置编译为相关的数据和执行方法。 2、 接收到http请求时，在NGX_HTTP_SERVER_REWRITE_PHASE和 NGX_HTTP_REWRITE_PHASE阶段中查找匹配了的location下是否有待执行的脚本，如果有则依次执行。 下面先来看下变量及脚本引擎涉及到的一些相关的数据结构，理解这些数据结构的定义有助于代码的分析。 先来看下变量涉及到的相关数据结构。我们知道，变量通常由变量名和变量值组成，因此变量值和变量名都需要一个数据结构他对其进行描述。对于同一个变量名，随着场景的不同而具有不同的值，即变量值随着请求不同而不同（当然并不排除不同请求的部分或全部变量具有相同的值）。但是，对于不同的请求，变量名是一样的，只是变量值不同而已，因此，在Nginx内部全局只有一个用来保存所有变量名的地方，即ngx_http_core_main_conf_t结构体中。而对于变量值来说，因为其对于每个请求而不尽相同，因此存储变量值的地方应该在请求的结构中。对于上述描述中涉及到的结构体或相关成员分别介绍如下： 1、 变量定义结构体 ngx_http_variable_t 12345678910struct ngx_http_variable_s &#123; ngx_str_t name; /*变量名，但不包括前置的$符号*/ /*如果需要变量在最初赋值的时候进行变量值设置，需实现该方法*/ ngx_http_set_variable_pt set_handler; /*每次获取变量值时会调用该方法*/ ngx_http_get_variable_pt get_handler; uintptr_t data; /*作为get_handler或者set_handler方法的参数*/ ngx_uint_t flags; /*变量的特性，如值可变、索引化、不缓存、不hash*/ ngx_uint_t index; /*变量值在请求的缓存数组中的索引值*/&#125;; 在上面这个结构体中，我们可以看到有两个用于变量解析的方法，分别为set_handler和get_handler。其中的data成员即作为这两个解析方法的其中一个参数，用于传递必要的信息。如果一个变量被使用模块进行了索引化，则index成员描述的意思就是该变量在索引数组中的下标，通过该下标可以直接获得变量名结构体。另外，flags成员描述的是这个变量相关的一些属性，如值可变、索引化、不缓存和不hash等，其属性定义如下： 1234#define NGX_HTTP_VAR_CHANGEALBE 1#define NGX_HTTP_VAR_NOCACHAEABLE 2#define NGX_HTTP_VAR_INDEXED 4#define NGX_HTTP_VAR_NOHASE 8 2、 变量值结构体 ngx_http_variable_value_t 12345678typedef struct &#123; unsigned len:28; /*变量值必须是在一段连续内存中存放，len表示长度*/ unsigned valid:1; /*valid为1，表示该变量是被解析过的，且数据可用*/ unsigned no_cacheable:1; /*为1表示该变量不可以被缓存*/ unsigned not_found:1;/*表示该变量被解析过，但没有解析到相应的值*/ unsigned escape:1; u_char *data; /*指向变量值所在内存起始地址，与len成员配合使用*/&#125; ngx_variable_value_t; 上面结构体中的data成员和len成员共同描述了变量值，其余成员主要是描述了变量值的一些属性。 3、 存储变量名的数据结构在上文中我们说过，变量名存储在全局唯一的ngx_http_core_main_conf_t结构体对象中，具体说是存放在ngx_http_core_main_conf_t中的如下三个成员中： 12345678typedef struct &#123; …… ngx_hash_t variables_hash; /*存储变量名的hash表*/ ngx_array_t variables; /*存储索引过的变量的数组*/ /*用于构造variables_hash散列表*/ ngx_hash_keys_arrays_t *variables_keys; ……&#125; ngx_http_core_main_conf_t; 对于上面的三个结构体成员，分别介绍如下： 1) variable_keys 当Nginx解析配置正常结束时，所有的变量都被集中在variables_keys中。 2) variables_hash variables_keys中的变量，除了显示说明变量不能进行hash的变量，其余变量都会进行hash，以加快访问速度，variables_hash就是最终构造出来的变量hash表。 3) variables variables数组中存储的是被使用模块索引化了的变量。其中，被索引的变量必须是定义过的，其合法性校验就是利用其是否在variables_keys数组中来判断的。 4、 存储变量值的数据结构 1234struct ngx_http_request_s &#123; /*variables存储的是索引化的变量值*/ ngx_http_variable_value_t *variables;&#125; 变量值如果可以被缓存，那么它一定只能存放在每一个http请求内，因为我们知道变量值的生命周期和一个请求是一样的。所以变量值就存放在ngx_http_request_s内的variables数组内。可以简单认为，Nginx在解析配置文件过程中遇到的所有变量都会加入到r-&gt;variables数组中。当http请求刚到达Nginx的时候，就会创建存放变量值的variables数组。这里的r-&gt;variables数组和存放变量名的数组cmcf-&gt;variables是一一对应的，即两者对应元素一起构成键值对。 看完变量相关的数据结构后，再一起看下set命令涉及到的脚本引擎相关的数据结构。 1、脚本引擎上下文 12345678910typedef struct &#123; u_char *ip; //指向待执行的脚本指令。 u_char *pos; //指向下面的buf，即最后变量的值存放的地方 ngx_http_variable_value_t *sp; //变量值构成的栈 ngx_str_t buf; //解析一个复杂值参数变量时，最后解析的结果存放在此 unsigned skip:1; //置1表示不需要拷贝数据，直接跳过数据拷贝步骤 ngx_int_t status; //脚本引擎执行状态 ngx_http_request_t *request; //指向脚本引擎所属的http请求 ……&#125; ngx_http_script_engine_t; 同一段脚本被编译进Nginx中，在不同的请求里执行的效果是完全不同的（因为变量值的生命周期和请求是一样的），所以每个请求都必须有其自己的脚本执行上下文。其中ip指向的是待执行的脚本指令，sp是解析变量值的时候临时存放变量值的地方，待解析变量名的时候会从sp中获取对应变量值，存放在r-&gt;variables数组中。buf成员是用来解析复杂值参数变量的时候用来存放变量值的地方，pos成员指向其中。 3、 编译变量名的结构体 1234typedef struct &#123; ngx_http_script_code_pt code; //code指向获取变量名的脚本指令方法 uintptr_t index; //r-variables数组中的索引号&#125; ngx_http_script_var_code_t; &#160; &#160; &#160; &#160;4、 编译变量值的结构体 1234567typedef struct &#123; ngx_http_script_code_pt code; //code指向获取变量值的脚本指令 /*外部变量值如果为整数，则转为整数后赋值给value，否则value为0*/ uintptr_t value; uintptr_t text_len; //外部变量值(set的第二个参数)的长度 uintptr_t text_data; //外部变量值的起始地址&#125; ngx_http_script_value_code_t; 5、 编译复杂变量值的结构体 1234typedef struct &#123; ngx_http_script_code_pt code;//code指向编译复杂变量值的脚本指令方法 ngx_array_t *lengths; //lengths存放的是复杂变量值中内嵌变量的值长度&#125; ngx_http_script_complex_value_code_t; 6、 存放编译所有编译脚本的结构体 我们知道ngx_http_script_engine_t是随着http请求到来时才创建的，所以它无法保存Nginx启动时就编译出来的脚本。保存编译出来的脚本这个工作实际上是由ngx_http_rewrite_loc_conf_t结构体来承担的，如下所示： 12345typedef struct &#123; ngx_array_t *codes; //保存着所属location下的所有编译后脚本 ngx_uint_t stack_size; //变量值栈sp的大小 ……&#125; ngx_http_rewrite_loc_conf_t; ngx_http_rewrite_loc_conf_t其实就是rewrite模块在location级别下面的配置结构体。如果匹配的location下面没有脚本式配置，则codes成员是空的，否则codes成员就会放置承载着解析后的脚本指令的结构体。到这里，set命令涉及到的相关数据结构就简单介绍完了，下面将结合具体的例子对代码中的处理流程进行简单分析。下文将以一条具有代表性的set配置命令进行代码层面上的分析，配置如下： 123location /image/ &#123; set $key “$&#123;value&#125;test”&#125; 当解析到set命令相关的配置项时，就会调用set命令实现的回调函数：ngx_http_rewrite_set。在这个函数中就会将set命令及其参数进行编译成脚本。待请求匹配到对应的location并执行到NGX_HTTP_SERVER_REWRITE_PHASE或者NGX_HTTP_REWRITE_PHASE阶段的时候，就会执行编译成的脚本。函数ngx_http_rewrite_set的执行流程如下： 1、 检查set命令配置的合法性。具体做法就是检查set命令的第一个参数是否以”$“符号开头。在Nginx配置文件中，变量都是以”$“字符开头的，还有可选的用于将变量名括起来的大括号–”{}”。 2、 向Nginx内核添加这个外部变量。具体做法就是调用ngx_http_add_variable()这个函数，将变量名字加入到全局唯一的ngx_http_core_mian_conf_t结构体的variables_keys成员中。我们知道，ngx_http_core_mian_conf_t结构体的variables_keys成员会收集Nginx中出现的所有变量，不管是内部还是外部变量。另外，外部变量是允许赋值的，因此在调用ngx_http_add_variable()这个函数时，其变量属性必须是值可变的NGX_HTTP_VAR_CHANGEABLE。 3、 将外部变量索引化。具体做法就是调用ngx_http_get_variable_index()函数将变量加入到将变量名字加入到全局唯一的ngx_http_core_mian_conf_t结构体的variables成员中。因为从逻辑上来讲，既然定义了外部变量，那么一般来说都是会使用的，因此将其索引化，加快访问速度。 4、 对于不是5类特殊变量的外部变量，需要设置变量的get_handler回调函数和索引化下标。通常来说，内部变量的get_handler回调是必须要实现的，因为只有读取到这个变量的时候才会调用get_handler去获取内部变量值。但是外部变量是每次set都会立即赋值的，获取变量值的时候是直接从请求的r-&gt;variables数组中获取的，此时的get_handler用处不大，但是为什么又要设置外部变量的get_handler回调呢？因为可能有些模块会在set脚本执行前就使用到这个外部变量，此时外部变量值是不存在的，即缓存在r-&gt;variables中的值是空的，此时会调用get_handler求值。故外部变量的get_handler也不能为NULL，将其设置为ngx_http_variable_null_value，即设置为空字符串值。 5、 解析变量值。因为变量值可以是纯字符串的简单值参数，也可以是包含其他变量的复杂值参数。这两种值参数的编译过程是不同的，需要区别对待。本例中的便是属于复杂值参数类型的处理。ngx_http_rewrite_value函数的处理流程如下： 1） 判断值参数是复杂值参数还是简单值参数。具体做法就是遍历变量值参数字符串，看其中是否出现了”$“字符并统计其个数。 2） 如果值参数中”$“个数为0，表明是简单值参数。那么在lcf-&gt;codes数组中申请存放编译简单值参数的指令性结构体ngx_http_script_value_code_t所需的内存，并设置结构体的值。 3）如果值参数中”$“个数不为0，，表明是复杂值参数。这个也是上述实例将会走的处理流程。首先向lcf-&gt;codes数组中申请用于存放编译复杂值参数的指令性结构体ngx_http_script_complex_value_code_t所需内存，并设置相应的成员。然后根据复杂值参数中具体内容进行分类编译，具体就是按照值参数内容对内含变量和普通字符串进行按序编译。此部分流程可以结合下面的实例布局图进行理解。 6、 编译变量名。此时有两种情况，一、编译一个想通过set命令对其值进行更改内部变量；二、编译一个外部变量。 1） 编译一个想通过set命令对其值进行更改内部变量。如果一个内部变量，在Nginx的处理过程中希望可以通过set命令改变其值，则在实现该内部变量的时候，会实现set_handler回调方法，当执行到set命令的时候，解析变量值的时候会调用set_handler回调将配置文件中设置的值赋给内部变量。set_handler方法是有内部变量定义过的，因此其不需要索引下标也可以找到对应的变量。 2） 编译一个外部变量。向lcf-&gt;codes数组中申请用于存放编译变量名的指令性结构体所需要的空间。设置编译指令性结构体方法和索引化下标。 下面是上述配置项相关数据结构间的内存布局示意图： 在上面的内存布局图中我们可以得到一下几个信息： 1、如果配置文件中出现了一个带有复杂变量值参数的set命令，则结构体ngx_http_script_complex_value_code_t意味着编译该条set命令的开始。 2、 在结构体ngx_http_script_complex_value_code_t中有两部分，第一部分是lengths数组，其中保存的是复杂值参数各个部分的值的长度，其中包含的编译值长度的结构体是在编译对应部分值的结构体时进行赋值，比如本例中的”${value}test”可以分为两部分，一个是内含变量”${value}”，一个是普通字符串”test”，则lengths数组中有两个元素，分别存储着用于获取内含变量”${value}”对应的值的长度，以及普通字符串”test”的长度的编译结构体。第二部分是函数指针ngx_http_script_code_pt，此时对应的实现为ngx_http_script_complex_value_code，这个函数的主要作用就是遍历lengths数组，计算实例中set赋值的变量的值的总长度，然后用这个长度为e-&gt;buf申请内存，保存解析后的变量值，并将e-&gt;pos指向e-&gt;buf.data，e-&gt;sp等于e-&gt;buf。 3、 在结构体ngx_http_script_complex_value_code_t之后，就是用于编译变量值的结构体，如上面所说，实例中的复杂值参数可以分为两部分，并按照先后顺序对这两部分进行分别编译，首先是编译获取内含变量”${value}”值的结构体–ngx_http_script_var_code_t，在这个结构体中包含了两部分，第一部分是ngx_http_script_code_pt，此时实现是ngx_http_script_copy_var_code，该函数用于结合第二部分的索引化下标获取变量值，并将此部分存储到e-&gt;buf中，如内存布局图中的2（1）所示；然后是编译普通字符串“test”的部分，对于普通字符串，脚本引擎的处理比较特殊，在编译普通字符串的结构体（ngx_http_script_copy_code_t）之后紧跟着普通字符串值本身，然后ngx_http_script_copy_code_t结构体中的ngx_http_script_code_pt的实现ngx_http_script_copy_code会将紧跟在后面的普通字符串值拷贝到e-&gt;buf中，与内含变量值组成目标变量的值，如内存布局图中的标号3（1）和3（2）所示。 4、编译完变量值之后是编译变量名，编译变量名涉及到的结构体是ngx_http_script_var_code_t，其中的ngx_http_script_code_pt对应的实现是ngx_http_script_set_var_code，这个函数会将e-&gt;sp（此时也为e-&gt;buf）中解析后的变量值拷贝r-&gt;variables数组中对应的下标处。到此编译便完成了。 在Nginx解析完配置项之后，上面实例中涉及到的set命令也已经编译成脚本了，存放在ngx_http_rewrite_loc_conf_t结构体中的codes数组中。那何时会执行编译好的脚本呢？当请求匹配了上面的”/ image”的location并执行到NGX_HTTP_SERVER_REWRITE_PHASE或者NGX_HTTP_REWRITE_PHASE阶段时，就会执行存放在codes数组中的脚本。具体对应到代码就是ngx_http_rewrite_handler()函数的处理，其执行流程如下： 参考文章: 1、《深入理解Nginx 模块开发与架构解析》]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
</search>
